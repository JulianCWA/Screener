{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02b3ab1-266e-4f09-882b-dc5b7d8a367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperated Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789e5650-9c6e-491f-b017-b6a0cb864177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAB Sheet Script V2.4 = FINAL!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d5df3d-314a-4342-94c4-25c30462c788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 16:04:40,454 - INFO - Processing C:\\Users\\JulianHeron\\OneDrive - Credentialed Wealth Advisors\\Documents\\AQR_Factor_Checks\\Betting_Against_Beta_Equity_Factors_Monthly.xlsx (sheet BAB Factors)\n",
      "2025-04-11 16:04:41,253 - INFO - Read 1129 rows with columns: ['DATE', 'AUS', 'AUT', 'BEL', 'CAN', 'CHE', 'DEU', 'DNK', 'ESP', 'FIN', 'FRA', 'GBR', 'GRC', 'HKG', 'IRL', 'ISR', 'ITA', 'JPN', 'NLD', 'NOR', 'NZL', 'PRT', 'SGP', 'SWE', 'USA', 'Global', 'Global Ex USA', 'Europe', 'North America', 'Pacific']\n",
      "2025-04-11 16:04:41,261 - INFO - Selected columns: ['DATE', 'USA', 'Global', 'Global Ex USA']\n",
      "2025-04-11 16:04:41,976 - DEBUG - After date cleaning, 1129 rows remain\n",
      "2025-04-11 16:04:41,989 - DEBUG - Rows after melting and dropping NA: 2039\n",
      "2025-04-11 16:04:42,300 - DEBUG - Raw existing data sample (first 5 rows): [{'factor': 'BAB', 'date': datetime.date(1930, 12, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 1, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 2, 28), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 3, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 4, 30), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}]\n",
      "2025-04-11 16:04:43,391 - DEBUG - Existing keys sample (first 5): [('HML_Devil', datetime.date(2022, 10, 31), \"The Devil's in HML's Details\", 'USA'), ('SMB', datetime.date(1979, 6, 30), 'Fama-French Factors', 'USA'), ('BAB', datetime.date(1991, 10, 31), 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'Global'), ('BAB', datetime.date(1998, 4, 30), 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'USA'), ('SMB', datetime.date(1992, 8, 31), 'Fama-French Factors', 'Global')]\n",
      "2025-04-11 16:04:43,393 - DEBUG - Total existing keys: 16190\n",
      "2025-04-11 16:04:43,413 - INFO - No new rows to load into factor_returns from C:\\Users\\JulianHeron\\OneDrive - Credentialed Wealth Advisors\\Documents\\AQR_Factor_Checks\\Betting_Against_Beta_Equity_Factors_Monthly.xlsx (sheet BAB Factors)\n",
      "2025-04-11 16:04:43,416 - DEBUG - Input date range: 1930-12-31 00:00:00 to 2024-12-31 00:00:00\n",
      "2025-04-11 16:04:43,418 - DEBUG - Database date range: 1926-07-31 to 2025-01-31\n",
      "2025-04-11 16:04:43,425 - INFO - Processing C:\\Users\\JulianHeron\\OneDrive - Credentialed Wealth Advisors\\Documents\\AQR_Factor_Checks\\Betting_Against_Beta_Equity_Factors_Monthly.xlsx (sheet MKT)\n",
      "2025-04-11 16:04:43,835 - INFO - Read 1182 rows with columns: ['DATE', 'AUS', 'AUT', 'BEL', 'CAN', 'CHE', 'DEU', 'DNK', 'ESP', 'FIN', 'FRA', 'GBR', 'GRC', 'HKG', 'IRL', 'ISR', 'ITA', 'JPN', 'NLD', 'NOR', 'NZL', 'PRT', 'SGP', 'SWE', 'USA', 'Global', 'Global Ex USA', 'Europe', 'North America', 'Pacific']\n",
      "2025-04-11 16:04:43,837 - INFO - Selected columns: ['DATE', 'USA', 'Global', 'Global Ex USA']\n",
      "2025-04-11 16:04:44,499 - DEBUG - After date cleaning, 1182 rows remain\n",
      "2025-04-11 16:04:44,507 - DEBUG - Rows after melting and dropping NA: 2166\n",
      "2025-04-11 16:04:44,713 - DEBUG - Raw existing data sample (first 5 rows): [{'factor': 'BAB', 'date': datetime.date(1930, 12, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 1, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 2, 28), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 3, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 4, 30), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}]\n",
      "2025-04-11 16:04:45,746 - DEBUG - Existing keys sample (first 5): [('HML_Devil', datetime.date(2022, 10, 31), \"The Devil's in HML's Details\", 'USA'), ('SMB', datetime.date(1979, 6, 30), 'Fama-French Factors', 'USA'), ('BAB', datetime.date(1991, 10, 31), 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'Global'), ('BAB', datetime.date(1998, 4, 30), 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'USA'), ('SMB', datetime.date(1992, 8, 31), 'Fama-French Factors', 'Global')]\n",
      "2025-04-11 16:04:45,747 - DEBUG - Total existing keys: 16190\n",
      "2025-04-11 16:04:45,761 - INFO - No new rows to load into factor_returns from C:\\Users\\JulianHeron\\OneDrive - Credentialed Wealth Advisors\\Documents\\AQR_Factor_Checks\\Betting_Against_Beta_Equity_Factors_Monthly.xlsx (sheet MKT)\n",
      "2025-04-11 16:04:45,763 - DEBUG - Input date range: 1926-07-31 00:00:00 to 2024-12-31 00:00:00\n",
      "2025-04-11 16:04:45,766 - DEBUG - Database date range: 1926-07-31 to 2025-01-31\n",
      "2025-04-11 16:04:45,772 - INFO - Processing C:\\Users\\JulianHeron\\OneDrive - Credentialed Wealth Advisors\\Documents\\AQR_Factor_Checks\\Betting_Against_Beta_Equity_Factors_Monthly.xlsx (sheet SMB)\n",
      "2025-04-11 16:04:46,189 - INFO - Read 1181 rows with columns: ['DATE', 'AUS', 'AUT', 'BEL', 'CAN', 'CHE', 'DEU', 'DNK', 'ESP', 'FIN', 'FRA', 'GBR', 'GRC', 'HKG', 'IRL', 'ISR', 'ITA', 'JPN', 'NLD', 'NOR', 'NZL', 'PRT', 'SGP', 'SWE', 'USA', 'Global', 'Global Ex USA', 'Europe', 'North America', 'Pacific']\n",
      "2025-04-11 16:04:46,191 - INFO - Selected columns: ['DATE', 'USA', 'Global', 'Global Ex USA']\n",
      "2025-04-11 16:04:46,929 - DEBUG - After date cleaning, 1181 rows remain\n",
      "2025-04-11 16:04:46,942 - DEBUG - Rows after melting and dropping NA: 2151\n",
      "2025-04-11 16:04:47,124 - DEBUG - Raw existing data sample (first 5 rows): [{'factor': 'BAB', 'date': datetime.date(1930, 12, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 1, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 2, 28), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 3, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 4, 30), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}]\n",
      "2025-04-11 16:04:48,175 - DEBUG - Existing keys sample (first 5): [('HML_Devil', datetime.date(2022, 10, 31), \"The Devil's in HML's Details\", 'USA'), ('SMB', datetime.date(1979, 6, 30), 'Fama-French Factors', 'USA'), ('BAB', datetime.date(1991, 10, 31), 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'Global'), ('BAB', datetime.date(1998, 4, 30), 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'USA'), ('SMB', datetime.date(1992, 8, 31), 'Fama-French Factors', 'Global')]\n",
      "2025-04-11 16:04:48,176 - DEBUG - Total existing keys: 16190\n",
      "2025-04-11 16:04:48,195 - INFO - No new rows to load into factor_returns from C:\\Users\\JulianHeron\\OneDrive - Credentialed Wealth Advisors\\Documents\\AQR_Factor_Checks\\Betting_Against_Beta_Equity_Factors_Monthly.xlsx (sheet SMB)\n",
      "2025-04-11 16:04:48,197 - DEBUG - Input date range: 1926-07-31 00:00:00 to 2024-11-30 00:00:00\n",
      "2025-04-11 16:04:48,199 - DEBUG - Database date range: 1926-07-31 to 2025-01-31\n",
      "2025-04-11 16:04:48,203 - INFO - Processing C:\\Users\\JulianHeron\\OneDrive - Credentialed Wealth Advisors\\Documents\\AQR_Factor_Checks\\Betting_Against_Beta_Equity_Factors_Monthly.xlsx (sheet HML FF)\n",
      "2025-04-11 16:04:48,580 - INFO - Read 1181 rows with columns: ['DATE', 'AUS', 'AUT', 'BEL', 'CAN', 'CHE', 'DEU', 'DNK', 'ESP', 'FIN', 'FRA', 'GBR', 'GRC', 'HKG', 'IRL', 'ISR', 'ITA', 'JPN', 'NLD', 'NOR', 'NZL', 'PRT', 'SGP', 'SWE', 'USA', 'Global', 'Global Ex USA', 'Europe', 'North America', 'Pacific']\n",
      "2025-04-11 16:04:48,582 - INFO - Selected columns: ['DATE', 'USA', 'Global', 'Global Ex USA']\n",
      "2025-04-11 16:04:49,257 - DEBUG - After date cleaning, 1181 rows remain\n",
      "2025-04-11 16:04:49,265 - DEBUG - Rows after melting and dropping NA: 2151\n",
      "2025-04-11 16:04:49,376 - DEBUG - Raw existing data sample (first 5 rows): [{'factor': 'BAB', 'date': datetime.date(1930, 12, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 1, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 2, 28), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 3, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 4, 30), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}]\n",
      "2025-04-11 16:04:50,688 - DEBUG - Existing keys sample (first 5): [('HML_Devil', datetime.date(2022, 10, 31), \"The Devil's in HML's Details\", 'USA'), ('SMB', datetime.date(1979, 6, 30), 'Fama-French Factors', 'USA'), ('BAB', datetime.date(1991, 10, 31), 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'Global'), ('BAB', datetime.date(1998, 4, 30), 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'USA'), ('SMB', datetime.date(1992, 8, 31), 'Fama-French Factors', 'Global')]\n",
      "2025-04-11 16:04:50,690 - DEBUG - Total existing keys: 16190\n",
      "2025-04-11 16:04:50,723 - INFO - No new rows to load into factor_returns from C:\\Users\\JulianHeron\\OneDrive - Credentialed Wealth Advisors\\Documents\\AQR_Factor_Checks\\Betting_Against_Beta_Equity_Factors_Monthly.xlsx (sheet HML FF)\n",
      "2025-04-11 16:04:50,725 - DEBUG - Input date range: 1926-07-31 00:00:00 to 2024-11-30 00:00:00\n",
      "2025-04-11 16:04:50,731 - DEBUG - Database date range: 1926-07-31 to 2025-01-31\n",
      "2025-04-11 16:04:50,741 - INFO - Processing C:\\Users\\JulianHeron\\OneDrive - Credentialed Wealth Advisors\\Documents\\AQR_Factor_Checks\\Betting_Against_Beta_Equity_Factors_Monthly.xlsx (sheet HML Devil)\n",
      "2025-04-11 16:04:51,367 - INFO - Read 1182 rows with columns: ['DATE', 'AUS', 'AUT', 'BEL', 'CAN', 'CHE', 'DEU', 'DNK', 'ESP', 'FIN', 'FRA', 'GBR', 'GRC', 'HKG', 'IRL', 'ISR', 'ITA', 'JPN', 'NLD', 'NOR', 'NZL', 'PRT', 'SGP', 'SWE', 'USA', 'Global', 'Global Ex USA', 'Europe', 'North America', 'Pacific']\n",
      "2025-04-11 16:04:51,369 - INFO - Selected columns: ['DATE', 'USA', 'Global', 'Global Ex USA']\n",
      "2025-04-11 16:04:52,103 - DEBUG - After date cleaning, 1182 rows remain\n",
      "2025-04-11 16:04:52,112 - DEBUG - Rows after melting and dropping NA: 2154\n",
      "2025-04-11 16:04:52,217 - DEBUG - Raw existing data sample (first 5 rows): [{'factor': 'BAB', 'date': datetime.date(1930, 12, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 1, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 2, 28), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 3, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 4, 30), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}]\n",
      "2025-04-11 16:04:53,362 - DEBUG - Existing keys sample (first 5): [('HML_Devil', datetime.date(2022, 10, 31), \"The Devil's in HML's Details\", 'USA'), ('SMB', datetime.date(1979, 6, 30), 'Fama-French Factors', 'USA'), ('BAB', datetime.date(1991, 10, 31), 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'Global'), ('BAB', datetime.date(1998, 4, 30), 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'USA'), ('SMB', datetime.date(1992, 8, 31), 'Fama-French Factors', 'Global')]\n",
      "2025-04-11 16:04:53,363 - DEBUG - Total existing keys: 16190\n",
      "2025-04-11 16:04:53,379 - INFO - No new rows to load into factor_returns from C:\\Users\\JulianHeron\\OneDrive - Credentialed Wealth Advisors\\Documents\\AQR_Factor_Checks\\Betting_Against_Beta_Equity_Factors_Monthly.xlsx (sheet HML Devil)\n",
      "2025-04-11 16:04:53,380 - DEBUG - Input date range: 1926-07-31 00:00:00 to 2024-12-31 00:00:00\n",
      "2025-04-11 16:04:53,384 - DEBUG - Database date range: 1926-07-31 to 2025-01-31\n",
      "2025-04-11 16:04:53,390 - INFO - Processing C:\\Users\\JulianHeron\\OneDrive - Credentialed Wealth Advisors\\Documents\\AQR_Factor_Checks\\Betting_Against_Beta_Equity_Factors_Monthly.xlsx (sheet UMD)\n",
      "2025-04-11 16:04:53,808 - INFO - Read 1176 rows with columns: ['DATE', 'AUS', 'AUT', 'BEL', 'CAN', 'CHE', 'DEU', 'DNK', 'ESP', 'FIN', 'FRA', 'GBR', 'GRC', 'HKG', 'IRL', 'ISR', 'ITA', 'JPN', 'NLD', 'NOR', 'NZL', 'PRT', 'SGP', 'SWE', 'USA', 'Global', 'Global Ex USA', 'Europe', 'North America', 'Pacific']\n",
      "2025-04-11 16:04:53,810 - INFO - Selected columns: ['DATE', 'USA', 'Global', 'Global Ex USA']\n",
      "2025-04-11 16:04:54,514 - DEBUG - After date cleaning, 1176 rows remain\n",
      "2025-04-11 16:04:54,523 - DEBUG - Rows after melting and dropping NA: 2136\n",
      "2025-04-11 16:04:54,734 - DEBUG - Raw existing data sample (first 5 rows): [{'factor': 'BAB', 'date': datetime.date(1930, 12, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 1, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 2, 28), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 3, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 4, 30), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}]\n",
      "2025-04-11 16:04:55,945 - DEBUG - Existing keys sample (first 5): [('HML_Devil', datetime.date(2022, 10, 31), \"The Devil's in HML's Details\", 'USA'), ('SMB', datetime.date(1979, 6, 30), 'Fama-French Factors', 'USA'), ('BAB', datetime.date(1991, 10, 31), 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'Global'), ('BAB', datetime.date(1998, 4, 30), 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'USA'), ('SMB', datetime.date(1992, 8, 31), 'Fama-French Factors', 'Global')]\n",
      "2025-04-11 16:04:55,946 - DEBUG - Total existing keys: 16190\n",
      "2025-04-11 16:04:55,963 - INFO - No new rows to load into factor_returns from C:\\Users\\JulianHeron\\OneDrive - Credentialed Wealth Advisors\\Documents\\AQR_Factor_Checks\\Betting_Against_Beta_Equity_Factors_Monthly.xlsx (sheet UMD)\n",
      "2025-04-11 16:04:55,965 - DEBUG - Input date range: 1927-01-31 00:00:00 to 2024-12-31 00:00:00\n",
      "2025-04-11 16:04:55,968 - DEBUG - Database date range: 1926-07-31 to 2025-01-31\n",
      "2025-04-11 16:04:55,976 - INFO - Processing C:\\Users\\JulianHeron\\OneDrive - Credentialed Wealth Advisors\\Documents\\AQR_Factor_Checks\\Betting_Against_Beta_Equity_Factors_Monthly.xlsx (sheet ME(t-1))\n",
      "2025-04-11 16:04:56,443 - INFO - Read 1182 rows with columns: ['DATE', 'AUT', 'HKG', 'ESP', 'GBR', 'ITA', 'DEU', 'DNK', 'NZL', 'NLD', 'USA', 'PRT', 'BEL', 'ISR', 'GRC', 'NOR', 'SGP', 'CHE', 'IRL', 'CAN', 'FIN', 'JPN', 'SWE', 'FRA', 'AUS', 'Global Ex USA', 'Global', 'Europe', 'North America', 'Pacific']\n",
      "2025-04-11 16:04:56,445 - INFO - Selected columns: ['DATE', 'USA', 'Global', 'Global Ex USA']\n",
      "2025-04-11 16:04:57,236 - DEBUG - After date cleaning, 1182 rows remain\n",
      "2025-04-11 16:04:57,250 - DEBUG - Rows after melting and dropping NA: 2210\n",
      "2025-04-11 16:04:57,450 - DEBUG - Raw existing data sample (first 5 rows): [{'factor': 'BAB', 'date': datetime.date(1930, 12, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 1, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 2, 28), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 3, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 4, 30), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}]\n",
      "2025-04-11 16:04:58,705 - DEBUG - Existing keys sample (first 5): [('HML_Devil', datetime.date(2022, 10, 31), \"The Devil's in HML's Details\", 'USA'), ('SMB', datetime.date(1979, 6, 30), 'Fama-French Factors', 'USA'), ('BAB', datetime.date(1991, 10, 31), 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'Global'), ('BAB', datetime.date(1998, 4, 30), 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'USA'), ('SMB', datetime.date(1992, 8, 31), 'Fama-French Factors', 'Global')]\n",
      "2025-04-11 16:04:58,706 - DEBUG - Total existing keys: 16190\n",
      "2025-04-11 16:04:58,726 - INFO - No new rows to load into factor_returns from C:\\Users\\JulianHeron\\OneDrive - Credentialed Wealth Advisors\\Documents\\AQR_Factor_Checks\\Betting_Against_Beta_Equity_Factors_Monthly.xlsx (sheet ME(t-1))\n",
      "2025-04-11 16:04:58,728 - DEBUG - Input date range: 1926-07-31 00:00:00 to 2024-12-31 00:00:00\n",
      "2025-04-11 16:04:58,732 - DEBUG - Database date range: 1926-07-31 to 2025-01-31\n",
      "2025-04-11 16:04:58,739 - INFO - Processing C:\\Users\\JulianHeron\\OneDrive - Credentialed Wealth Advisors\\Documents\\AQR_Factor_Checks\\Betting_Against_Beta_Equity_Factors_Monthly.xlsx (sheet RF)\n",
      "2025-04-11 16:04:59,258 - INFO - Read 1183 rows with columns: ['DATE', 'Risk Free Rate']\n",
      "2025-04-11 16:04:59,260 - INFO - Selected columns: ['DATE', 'Risk Free Rate']\n",
      "2025-04-11 16:05:00,089 - DEBUG - After date cleaning, 1183 rows remain\n",
      "2025-04-11 16:05:00,099 - DEBUG - Rows after melting and dropping NA: 1183\n",
      "2025-04-11 16:05:00,270 - DEBUG - Raw existing data sample (first 5 rows): [{'factor': 'BAB', 'date': datetime.date(1930, 12, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 1, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 2, 28), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 3, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 4, 30), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}]\n",
      "2025-04-11 16:05:01,573 - DEBUG - Existing keys sample (first 5): [('HML_Devil', datetime.date(2022, 10, 31), \"The Devil's in HML's Details\", 'USA'), ('SMB', datetime.date(1979, 6, 30), 'Fama-French Factors', 'USA'), ('BAB', datetime.date(1991, 10, 31), 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'Global'), ('BAB', datetime.date(1998, 4, 30), 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'USA'), ('SMB', datetime.date(1992, 8, 31), 'Fama-French Factors', 'Global')]\n",
      "2025-04-11 16:05:01,574 - DEBUG - Total existing keys: 16190\n",
      "2025-04-11 16:05:01,590 - INFO - No new rows to load into factor_returns from C:\\Users\\JulianHeron\\OneDrive - Credentialed Wealth Advisors\\Documents\\AQR_Factor_Checks\\Betting_Against_Beta_Equity_Factors_Monthly.xlsx (sheet RF)\n",
      "2025-04-11 16:05:01,592 - DEBUG - Input date range: 1926-07-31 00:00:00 to 2025-01-31 00:00:00\n",
      "2025-04-11 16:05:01,595 - DEBUG - Database date range: 1926-07-31 to 2025-01-31\n",
      "2025-04-11 16:05:01,603 - INFO - Factor data processing complete!\n"
     ]
    }
   ],
   "source": [
    "# Version 2.4 FINAL\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.types import VARCHAR, DATE, DECIMAL\n",
    "import logging\n",
    "from dateutil.parser import parse\n",
    "import re\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(), logging.FileHandler('load_factor_data.log')]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Database connection\n",
    "engine = create_engine(\n",
    "    \"mssql+pyodbc://JULIANS_LAPTOP\\\\SQLEXPRESS/CWA_Fund_Database\"\n",
    "    \"?driver=ODBC+Driver+18+for+SQL+Server&trusted_connection=yes&TrustServerCertificate=yes\"\n",
    ")\n",
    "\n",
    "# Configuration for all sheets (unchanged)\n",
    "config = {\n",
    "    \"path\": r\"C:\\Users\\JulianHeron\\OneDrive - Credentialed Wealth Advisors\\Documents\\AQR_Factor_Checks\\Betting_Against_Beta_Equity_Factors_Monthly.xlsx\",\n",
    "    \"sheets\": {\n",
    "        \"BAB\": {\n",
    "            \"sheet\": \"BAB Factors\",\n",
    "            \"start_row\": 19,\n",
    "            \"paper\": \"Betting Against Beta (Frazzini and Pedersen, 2014)\",\n",
    "            \"factor\": \"BAB\",\n",
    "            \"asset_class\": \"Equity\",\n",
    "            \"headers\": [\"DATE\", \"AUS\", \"AUT\", \"BEL\", \"CAN\", \"CHE\", \"DEU\", \"DNK\", \"ESP\", \"FIN\", \"FRA\", \"GBR\", \n",
    "                        \"GRC\", \"HKG\", \"IRL\", \"ISR\", \"ITA\", \"JPN\", \"NLD\", \"NOR\", \"NZL\", \"PRT\", \"SGP\", \"SWE\", \n",
    "                        \"USA\", \"Global\", \"Global Ex USA\", \"Europe\", \"North America\", \"Pacific\"],\n",
    "            \"columns\": [\"DATE\", \"USA\", \"Global\", \"Global Ex USA\"]\n",
    "        },\n",
    "        \"MKT\": {\n",
    "            \"sheet\": \"MKT\",\n",
    "            \"start_row\": 19,\n",
    "            \"paper\": \"Fama-French Factors\",\n",
    "            \"factor\": \"MKT\",\n",
    "            \"asset_class\": \"Equity\",\n",
    "            \"headers\": [\"DATE\", \"AUS\", \"AUT\", \"BEL\", \"CAN\", \"CHE\", \"DEU\", \"DNK\", \"ESP\", \"FIN\", \"FRA\", \"GBR\", \n",
    "                        \"GRC\", \"HKG\", \"IRL\", \"ISR\", \"ITA\", \"JPN\", \"NLD\", \"NOR\", \"NZL\", \"PRT\", \"SGP\", \"SWE\", \n",
    "                        \"USA\", \"Global\", \"Global Ex USA\", \"Europe\", \"North America\", \"Pacific\"],\n",
    "            \"columns\": [\"DATE\", \"USA\", \"Global\", \"Global Ex USA\"]\n",
    "        },\n",
    "        \"SMB\": {\n",
    "            \"sheet\": \"SMB\",\n",
    "            \"start_row\": 19,\n",
    "            \"paper\": \"Fama-French Factors\",\n",
    "            \"factor\": \"SMB\",\n",
    "            \"asset_class\": \"Equity\",\n",
    "            \"headers\": [\"DATE\", \"AUS\", \"AUT\", \"BEL\", \"CAN\", \"CHE\", \"DEU\", \"DNK\", \"ESP\", \"FIN\", \"FRA\", \"GBR\", \n",
    "                        \"GRC\", \"HKG\", \"IRL\", \"ISR\", \"ITA\", \"JPN\", \"NLD\", \"NOR\", \"NZL\", \"PRT\", \"SGP\", \"SWE\", \n",
    "                        \"USA\", \"Global\", \"Global Ex USA\", \"Europe\", \"North America\", \"Pacific\"],\n",
    "            \"columns\": [\"DATE\", \"USA\", \"Global\", \"Global Ex USA\"]\n",
    "        },\n",
    "        \"HML_FF\": {\n",
    "            \"sheet\": \"HML FF\",\n",
    "            \"start_row\": 19,\n",
    "            \"paper\": \"Fama-French Factors\",\n",
    "            \"factor\": \"HML_FF\",\n",
    "            \"asset_class\": \"Equity\",\n",
    "            \"headers\": [\"DATE\", \"AUS\", \"AUT\", \"BEL\", \"CAN\", \"CHE\", \"DEU\", \"DNK\", \"ESP\", \"FIN\", \"FRA\", \"GBR\", \n",
    "                        \"GRC\", \"HKG\", \"IRL\", \"ISR\", \"ITA\", \"JPN\", \"NLD\", \"NOR\", \"NZL\", \"PRT\", \"SGP\", \"SWE\", \n",
    "                        \"USA\", \"Global\", \"Global Ex USA\", \"Europe\", \"North America\", \"Pacific\"],\n",
    "            \"columns\": [\"DATE\", \"USA\", \"Global\", \"Global Ex USA\"]\n",
    "        },\n",
    "        \"HML_Devil\": {\n",
    "            \"sheet\": \"HML Devil\",\n",
    "            \"start_row\": 19,\n",
    "            \"paper\": \"The Devil's in HML's Details\",\n",
    "            \"factor\": \"HML_Devil\",\n",
    "            \"asset_class\": \"Equity\",\n",
    "            \"headers\": [\"DATE\", \"AUS\", \"AUT\", \"BEL\", \"CAN\", \"CHE\", \"DEU\", \"DNK\", \"ESP\", \"FIN\", \"FRA\", \"GBR\", \n",
    "                        \"GRC\", \"HKG\", \"IRL\", \"ISR\", \"ITA\", \"JPN\", \"NLD\", \"NOR\", \"NZL\", \"PRT\", \"SGP\", \"SWE\", \n",
    "                        \"USA\", \"Global\", \"Global Ex USA\", \"Europe\", \"North America\", \"Pacific\"],\n",
    "            \"columns\": [\"DATE\", \"USA\", \"Global\", \"Global Ex USA\"]\n",
    "        },\n",
    "        \"UMD\": {\n",
    "            \"sheet\": \"UMD\",\n",
    "            \"start_row\": 19,\n",
    "            \"paper\": \"On Persistence in Mutual Fund Performance\",\n",
    "            \"factor\": \"UMD\",\n",
    "            \"asset_class\": \"Equity\",\n",
    "            \"headers\": [\"DATE\", \"AUS\", \"AUT\", \"BEL\", \"CAN\", \"CHE\", \"DEU\", \"DNK\", \"ESP\", \"FIN\", \"FRA\", \"GBR\", \n",
    "                        \"GRC\", \"HKG\", \"IRL\", \"ISR\", \"ITA\", \"JPN\", \"NLD\", \"NOR\", \"NZL\", \"PRT\", \"SGP\", \"SWE\", \n",
    "                        \"USA\", \"Global\", \"Global Ex USA\", \"Europe\", \"North America\", \"Pacific\"],\n",
    "            \"columns\": [\"DATE\", \"USA\", \"Global\", \"Global Ex USA\"]\n",
    "        },\n",
    "        \"ME\": {\n",
    "            \"sheet\": \"ME(t-1)\",\n",
    "            \"start_row\": 19,\n",
    "            \"paper\": \"AQR Factors\",\n",
    "            \"factor\": \"ME\",\n",
    "            \"asset_class\": \"Equity\",\n",
    "            \"headers\": [\"DATE\", \"AUT\", \"HKG\", \"ESP\", \"GBR\", \"ITA\", \"DEU\", \"DNK\", \"NZL\", \"NLD\", \"USA\", \"PRT\", \n",
    "                        \"BEL\", \"ISR\", \"GRC\", \"NOR\", \"SGP\", \"CHE\", \"IRL\", \"CAN\", \"FIN\", \"JPN\", \"SWE\", \"FRA\", \n",
    "                        \"AUS\", \"Global Ex USA\", \"Global\", \"Europe\", \"North America\", \"Pacific\"],\n",
    "            \"columns\": [\"DATE\", \"USA\", \"Global\", \"Global Ex USA\"]\n",
    "        },\n",
    "        \"RF\": {\n",
    "            \"sheet\": \"RF\",\n",
    "            \"start_row\": 19,\n",
    "            \"paper\": None,\n",
    "            \"factor\": \"RF\",\n",
    "            \"asset_class\": \"Fixed Income\",\n",
    "            \"headers\": [\"DATE\", \"Risk Free Rate\"],\n",
    "            \"columns\": [\"DATE\", \"Risk Free Rate\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def clean_date(date_val):\n",
    "    \"\"\"Attempt to clean and parse a date value, returning None if unfixable.\"\"\"\n",
    "    if pd.isna(date_val) or str(date_val).strip().lower() in ['', 'nan', 'risk free rate']:\n",
    "        return None\n",
    "    try:\n",
    "        return pd.to_datetime(date_val, errors='coerce')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        return parse(str(date_val), fuzzy=False)\n",
    "    except:\n",
    "        pass\n",
    "    date_str = str(date_val).strip()\n",
    "    if re.match(r'\\d{4}-\\d{2}-\\d{2}', date_str):\n",
    "        try:\n",
    "            year, month, day = map(int, date_str.split('-'))\n",
    "            if month > 12:\n",
    "                month = 12\n",
    "            if day > 31:\n",
    "                day = 31\n",
    "            return pd.to_datetime(f\"{year}-{month:02d}-{day:02d}\")\n",
    "        except:\n",
    "            pass\n",
    "    logger.warning(f\"Unable to parse date: {date_val}\")\n",
    "    return None\n",
    "\n",
    "def process_sheet(sheet_config, parent_path):\n",
    "    file_path = parent_path\n",
    "    sheet_name = sheet_config['sheet']\n",
    "    logger.info(f\"Processing {file_path} (sheet {sheet_name})\")\n",
    "    \n",
    "    try:\n",
    "        # Read the Excel sheet\n",
    "        df = pd.read_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            header=None,\n",
    "            skiprows=sheet_config['start_row']\n",
    "        )\n",
    "        df.columns = sheet_config['headers']\n",
    "        logger.info(f\"Read {len(df)} rows with columns: {list(df.columns)}\")\n",
    "\n",
    "        # Select only the required columns\n",
    "        valid_cols = [col for col in sheet_config['columns'] if col in df.columns]\n",
    "        if not valid_cols:\n",
    "            logger.error(f\"No valid columns found in {file_path} (sheet {sheet_name}). Skipping.\")\n",
    "            return\n",
    "        df = df[valid_cols]\n",
    "        logger.info(f\"Selected columns: {list(df.columns)}\")\n",
    "\n",
    "        # Rename DATE to date\n",
    "        df = df.rename(columns={\"DATE\": \"date\"})\n",
    "        \n",
    "        # Clean and parse dates\n",
    "        invalid_dates = []\n",
    "        df['date'] = df['date'].apply(clean_date)\n",
    "        invalid_indices = df['date'].isna()\n",
    "        if invalid_indices.any():\n",
    "            invalid_dates = df[invalid_indices].index.tolist()\n",
    "            logger.warning(f\"Found {len(invalid_dates)} unfixable dates in {sheet_name} at rows: {invalid_dates}\")\n",
    "            logger.debug(f\"Sample invalid date rows: {df[invalid_indices].head().to_dict('records')}\")\n",
    "        df = df.dropna(subset=['date'])\n",
    "        logger.debug(f\"After date cleaning, {len(df)} rows remain\")\n",
    "\n",
    "        # Normalize dates to date only\n",
    "        df['date'] = df['date'].dt.date\n",
    "\n",
    "        # Melt the DataFrame\n",
    "        value_vars = [col for col in df.columns if col != 'date']\n",
    "        region_map = {\n",
    "            \"USA\": \"USA\",\n",
    "            \"Global\": \"Global\",\n",
    "            \"Global Ex USA\": \"Intl\",\n",
    "            \"Risk Free Rate\": \"USA\"\n",
    "        }\n",
    "        df_long = df.melt(\n",
    "            id_vars=['date'],\n",
    "            value_vars=value_vars,\n",
    "            var_name='region_col',\n",
    "            value_name='value'\n",
    "        )\n",
    "        \n",
    "        # Assign factor, region, and other columns\n",
    "        df_long['factor'] = sheet_config['factor']\n",
    "        df_long['region'] = df_long['region_col'].map(region_map)\n",
    "        df_long['asset_class'] = sheet_config['asset_class']\n",
    "        df_long['associated_paper'] = sheet_config['paper'] if sheet_config['paper'] is not None else 'Unknown'\n",
    "        \n",
    "        # Drop unnecessary column and rows with NA values\n",
    "        df_long = df_long.drop(columns=['region_col']).dropna(subset=['value', 'region'])\n",
    "        logger.debug(f\"Rows after melting and dropping NA: {len(df_long)}\")\n",
    "        \n",
    "        # Convert date to datetime for SQL compatibility\n",
    "        df_long['date'] = pd.to_datetime(df_long['date'])\n",
    "        \n",
    "        # Reorder columns to match table schema\n",
    "        df_long = df_long[['factor', 'date', 'associated_paper', 'value', 'region', 'asset_class']]\n",
    "        \n",
    "        # Create key for duplicate checking\n",
    "        df_long['key'] = df_long.apply(\n",
    "            lambda row: (\n",
    "                row['factor'],\n",
    "                row['date'].date(),\n",
    "                row['associated_paper'],\n",
    "                row['region']\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Check for duplicates within the new data\n",
    "        duplicates = df_long.duplicated(subset=['key'], keep=False)\n",
    "        if duplicates.any():\n",
    "            logger.warning(f\"Found {duplicates.sum()} duplicate keys within {sheet_name} data: \"\n",
    "                          f\"{df_long[duplicates][['factor', 'date', 'region', 'value']].to_dict('records')}\")\n",
    "            df_long = df_long.drop_duplicates(subset=['key'], keep='first')\n",
    "            logger.debug(f\"After removing intra-sheet duplicates, {len(df_long)} rows remain\")\n",
    "\n",
    "        # Insert all data with transaction\n",
    "        with engine.connect() as connection:\n",
    "            with connection.begin() as transaction:\n",
    "                existing = pd.read_sql(\n",
    "                    \"SELECT factor, date, associated_paper, region FROM factor_returns\",\n",
    "                    connection\n",
    "                )\n",
    "                logger.debug(f\"Raw existing data sample (first 5 rows): {existing.head().to_dict('records')}\")\n",
    "                existing['date'] = pd.to_datetime(existing['date'], errors='coerce').dt.date\n",
    "                existing_keys = set(\n",
    "                    (row['factor'], row['date'], row['associated_paper'], row['region'])\n",
    "                    for _, row in existing.iterrows()\n",
    "                    if pd.notna(row['date']) and pd.notna(row['region'])\n",
    "                )\n",
    "                logger.debug(f\"Existing keys sample (first 5): {list(existing_keys)[:5]}\")\n",
    "                logger.debug(f\"Total existing keys: {len(existing_keys)}\")\n",
    "\n",
    "                df_new = df_long[~df_long['key'].isin(existing_keys)].drop(columns=['key'])\n",
    "        \n",
    "                if not df_new.empty:\n",
    "                    logger.debug(f\"New rows to insert: {len(df_new)}. Sample (first 5): \"\n",
    "                                f\"{df_new[['factor', 'date', 'region']].head().to_dict('records')}\")\n",
    "                    \n",
    "                    try:\n",
    "                        df_new.to_sql(\n",
    "                            'factor_returns',\n",
    "                            connection,\n",
    "                            if_exists='append',\n",
    "                            index=False,\n",
    "                            dtype={\n",
    "                                'factor': VARCHAR(50),\n",
    "                                'date': DATE,\n",
    "                                'associated_paper': VARCHAR(100),\n",
    "                                'value': DECIMAL(15, 6),\n",
    "                                'region': VARCHAR(50),\n",
    "                                'asset_class': VARCHAR(50)\n",
    "                            }\n",
    "                        )\n",
    "                        logger.info(f\"Inserted {len(df_new)} new rows into factor_returns from {file_path} (sheet {sheet_name})\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error inserting {len(df_new)} rows for {sheet_name}: \"\n",
    "                                    f\"{type(e).__name__}: {str(e)}. First row: {df_new.iloc[0].to_dict()}\")\n",
    "                        transaction.rollback()\n",
    "                        return\n",
    "                else:\n",
    "                    logger.info(f\"No new rows to load into factor_returns from {file_path} (sheet {sheet_name})\")\n",
    "                    logger.debug(f\"Input date range: {df_long['date'].min()} to {df_long['date'].max()}\")\n",
    "                    logger.debug(f\"Database date range: {existing['date'].min() if not existing.empty else 'N/A'} to \"\n",
    "                                f\"{existing['date'].max() if not existing.empty else 'N/A'}\")\n",
    "        \n",
    "        if invalid_dates:\n",
    "            logger.info(f\"Review {sheet_name} for unfixable dates at rows: {invalid_dates}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path} (sheet {sheet_name}): {type(e).__name__}: {str(e)}\")\n",
    "\n",
    "# Process all sheets\n",
    "for sheet_key, sheet_config in config['sheets'].items():\n",
    "    process_sheet(sheet_config, config['path'])\n",
    "\n",
    "logger.info(\"Factor data processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07122b7-2fd9-4353-be0a-4fbdae767647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSMOM Script FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5af5a57-bd81-4d9a-90d3-3794043aa3f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 16:24:06,533 - INFO - Processing C:\\Users\\JulianHeron\\Downloads\\Time Series Momentum Factors Monthly.xlsx (sheet TSMOM Factors)\n",
      "2025-04-11 16:24:06,811 - INFO - Read 481 rows with columns: ['DATE', 'TSMOM', 'TSMOM^CM', 'TSMOM^EQ', 'TSMOM^FI', 'TSMOM^FX']\n",
      "2025-04-11 16:24:06,814 - INFO - Columns after renaming: ['date', 'TSM-MA', 'TSM-Com', 'TSM-EQ', 'TSM-FI', 'TSM-FX']\n",
      "2025-04-11 16:24:06,825 - DEBUG - After date cleaning, 481 rows remain\n",
      "2025-04-11 16:24:06,837 - DEBUG - Rows after melting and dropping NA: 2405\n",
      "2025-04-11 16:24:07,151 - DEBUG - Raw existing data sample (first 5 rows): [{'factor': 'BAB', 'date': datetime.date(1930, 12, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 1, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 2, 28), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 3, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 4, 30), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}]\n",
      "2025-04-11 16:24:09,374 - DEBUG - Existing keys sample (first 5): [('HML_Devil', datetime.date(2022, 10, 31), \"The Devil's in HML's Details\", 'USA'), ('SMB', datetime.date(1979, 6, 30), 'Fama-French Factors', 'USA'), ('BAB', datetime.date(1991, 10, 31), 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'Global'), ('BAB', datetime.date(1998, 4, 30), 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'USA'), ('SMB', datetime.date(1992, 8, 31), 'Fama-French Factors', 'Global')]\n",
      "2025-04-11 16:24:09,376 - DEBUG - Total existing keys: 16190\n",
      "2025-04-11 16:24:09,402 - DEBUG - New rows to insert: 2405. Sample (first 5): [{'factor': 'TSM-MA', 'date': Timestamp('1985-01-31 00:00:00'), 'region': 'Global'}, {'factor': 'TSM-MA', 'date': Timestamp('1985-02-28 00:00:00'), 'region': 'Global'}, {'factor': 'TSM-MA', 'date': Timestamp('1985-03-29 00:00:00'), 'region': 'Global'}, {'factor': 'TSM-MA', 'date': Timestamp('1985-04-30 00:00:00'), 'region': 'Global'}, {'factor': 'TSM-MA', 'date': Timestamp('1985-05-31 00:00:00'), 'region': 'Global'}]\n",
      "2025-04-11 16:24:11,090 - INFO - Inserted 2405 new rows into factor_returns from C:\\Users\\JulianHeron\\Downloads\\Time Series Momentum Factors Monthly.xlsx (sheet TSMOM Factors)\n",
      "2025-04-11 16:24:11,101 - INFO - TSMOM data processing complete!\n"
     ]
    }
   ],
   "source": [
    "# TSMOM Version 2.0 FINAL\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.types import VARCHAR, DATE, DECIMAL\n",
    "import logging\n",
    "from dateutil.parser import parse\n",
    "import re\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(), logging.FileHandler('load_tsmom_data.log')]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Database connection\n",
    "engine = create_engine(\n",
    "    \"mssql+pyodbc://JULIANS_LAPTOP\\\\SQLEXPRESS/CWA_Fund_Database\"\n",
    "    \"?driver=ODBC+Driver+18+for+SQL+Server&trusted_connection=yes&TrustServerCertificate=yes\"\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    \"path\": r\"C:\\Users\\JulianHeron\\Downloads\\Time Series Momentum Factors Monthly.xlsx\",\n",
    "    \"sheet\": \"TSMOM Factors\",\n",
    "    \"start_row\": 18,\n",
    "    \"paper\": \"Time Series Momentum (Moskowitz, Ooi, and Pedersen, 2012)\",\n",
    "    \"headers\": [\"DATE\", \"TSMOM\", \"TSMOM^CM\", \"TSMOM^EQ\", \"TSMOM^FI\", \"TSMOM^FX\"],\n",
    "    \"columns\": {\n",
    "        \"DATE\": \"DATE\",\n",
    "        \"TSMOM\": \"TSM-MA\",\n",
    "        \"TSMOM^CM\": \"TSM-Com\",\n",
    "        \"TSMOM^EQ\": \"TSM-EQ\",\n",
    "        \"TSMOM^FI\": \"TSM-FI\",\n",
    "        \"TSMOM^FX\": \"TSM-FX\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def clean_date(date_val):\n",
    "    \"\"\"Attempt to clean and parse a date value, returning None if unfixable.\"\"\"\n",
    "    if pd.isna(date_val) or str(date_val).strip().lower() in ['', 'nan']:\n",
    "        return None\n",
    "    try:\n",
    "        return pd.to_datetime(date_val, errors='coerce')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        return parse(str(date_val), fuzzy=False)\n",
    "    except:\n",
    "        pass\n",
    "    date_str = str(date_val).strip()\n",
    "    if re.match(r'\\d{4}-\\d{2}-\\d{2}', date_str):\n",
    "        try:\n",
    "            year, month, day = map(int, date_str.split('-'))\n",
    "            if month > 12:\n",
    "                month = 12\n",
    "            if day > 31:\n",
    "                day = 31\n",
    "            return pd.to_datetime(f\"{year}-{month:02d}-{day:02d}\")\n",
    "        except:\n",
    "            pass\n",
    "    logger.warning(f\"Unable to parse date: {date_val}\")\n",
    "    return None\n",
    "\n",
    "def process_tsmom(config):\n",
    "    file_path = config['path']\n",
    "    sheet_name = config['sheet']\n",
    "    logger.info(f\"Processing {file_path} (sheet {sheet_name})\")\n",
    "    \n",
    "    try:\n",
    "        # Read the Excel sheet\n",
    "        df = pd.read_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            header=None,\n",
    "            skiprows=config['start_row']\n",
    "        )\n",
    "        df.columns = config['headers']\n",
    "        logger.info(f\"Read {len(df)} rows with columns: {list(df.columns)}\")\n",
    "\n",
    "        # Select and rename columns\n",
    "        date_col = 'DATE'\n",
    "        valid_cols = [col for col in config['columns'].keys() if col in df.columns]\n",
    "        if not valid_cols or date_col not in valid_cols:\n",
    "            logger.error(f\"No valid columns or missing DATE in {file_path} (sheet {sheet_name}). Skipping.\")\n",
    "            return\n",
    "        df = df[valid_cols]\n",
    "        df.columns = ['date' if col == date_col else config['columns'][col] for col in df.columns]\n",
    "        logger.info(f\"Columns after renaming: {list(df.columns)}\")\n",
    "\n",
    "        # Clean and parse dates\n",
    "        invalid_dates = []\n",
    "        df['date'] = df['date'].apply(clean_date)\n",
    "        invalid_indices = df['date'].isna()\n",
    "        if invalid_indices.any():\n",
    "            invalid_dates = df[invalid_indices].index.tolist()\n",
    "            logger.warning(f\"Found {len(invalid_dates)} unfixable dates in {sheet_name} at rows: {invalid_dates}\")\n",
    "            logger.debug(f\"Sample invalid date rows: {df[invalid_indices].head().to_dict('records')}\")\n",
    "        df = df.dropna(subset=['date'])\n",
    "        logger.debug(f\"After date cleaning, {len(df)} rows remain\")\n",
    "\n",
    "        # Normalize dates to date only\n",
    "        df['date'] = df['date'].dt.date\n",
    "\n",
    "        # Melt the DataFrame\n",
    "        value_vars = [col for col in df.columns if col != 'date']\n",
    "        df_long = df.melt(\n",
    "            id_vars=['date'],\n",
    "            value_vars=value_vars,\n",
    "            var_name='factor',\n",
    "            value_name='value'\n",
    "        )\n",
    "        \n",
    "        # Assign region and other columns\n",
    "        df_long['region'] = \"Global\"\n",
    "        df_long['asset_class'] = df_long['factor'].map({\n",
    "            \"TSM-MA\": \"Multi-Asset\",\n",
    "            \"TSM-Com\": \"Commodities\",\n",
    "            \"TSM-EQ\": \"Equity\",\n",
    "            \"TSM-FI\": \"Fixed Income\",\n",
    "            \"TSM-FX\": \"Currencies\"\n",
    "        })\n",
    "        df_long['associated_paper'] = config['paper']\n",
    "        df_long = df_long.dropna(subset=['value'])\n",
    "        logger.debug(f\"Rows after melting and dropping NA: {len(df_long)}\")\n",
    "        \n",
    "        # Convert date to datetime for SQL compatibility\n",
    "        df_long['date'] = pd.to_datetime(df_long['date'])\n",
    "        \n",
    "        # Reorder columns to match table schema\n",
    "        df_long = df_long[['factor', 'date', 'associated_paper', 'value', 'region', 'asset_class']]\n",
    "        \n",
    "        # Create key for duplicate checking\n",
    "        df_long['key'] = df_long.apply(\n",
    "            lambda row: (\n",
    "                row['factor'],\n",
    "                row['date'].date(),\n",
    "                row['associated_paper'],\n",
    "                row['region']\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Check for duplicates within the new data\n",
    "        duplicates = df_long.duplicated(subset=['key'], keep=False)\n",
    "        if duplicates.any():\n",
    "            logger.warning(f\"Found {duplicates.sum()} duplicate keys within {sheet_name} data: \"\n",
    "                          f\"{df_long[duplicates][['factor', 'date', 'region', 'value']].to_dict('records')}\")\n",
    "            df_long = df_long.drop_duplicates(subset=['key'], keep='first')\n",
    "            logger.debug(f\"After removing intra-sheet duplicates, {len(df_long)} rows remain\")\n",
    "\n",
    "        # Insert all data with transaction\n",
    "        with engine.connect() as connection:\n",
    "            with connection.begin() as transaction:\n",
    "                existing = pd.read_sql(\n",
    "                    \"SELECT factor, date, associated_paper, region FROM factor_returns\",\n",
    "                    connection\n",
    "                )\n",
    "                logger.debug(f\"Raw existing data sample (first 5 rows): {existing.head().to_dict('records')}\")\n",
    "                existing['date'] = pd.to_datetime(existing['date'], errors='coerce').dt.date\n",
    "                existing_keys = set(\n",
    "                    (row['factor'], row['date'], row['associated_paper'], row['region'])\n",
    "                    for _, row in existing.iterrows()\n",
    "                    if pd.notna(row['date']) and pd.notna(row['region'])\n",
    "                )\n",
    "                logger.debug(f\"Existing keys sample (first 5): {list(existing_keys)[:5]}\")\n",
    "                logger.debug(f\"Total existing keys: {len(existing_keys)}\")\n",
    "\n",
    "                df_new = df_long[~df_long['key'].isin(existing_keys)].drop(columns=['key'])\n",
    "        \n",
    "                if not df_new.empty:\n",
    "                    logger.debug(f\"New rows to insert: {len(df_new)}. Sample (first 5): \"\n",
    "                                f\"{df_new[['factor', 'date', 'region']].head().to_dict('records')}\")\n",
    "                    \n",
    "                    try:\n",
    "                        df_new.to_sql(\n",
    "                            'factor_returns',\n",
    "                            connection,\n",
    "                            if_exists='append',\n",
    "                            index=False,\n",
    "                            dtype={\n",
    "                                'factor': VARCHAR(50),\n",
    "                                'date': DATE,\n",
    "                                'associated_paper': VARCHAR(100),\n",
    "                                'value': DECIMAL(15, 6),\n",
    "                                'region': VARCHAR(50),\n",
    "                                'asset_class': VARCHAR(50)\n",
    "                            }\n",
    "                        )\n",
    "                        logger.info(f\"Inserted {len(df_new)} new rows into factor_returns from {file_path} (sheet {sheet_name})\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error inserting {len(df_new)} rows for {sheet_name}: \"\n",
    "                                    f\"{type(e).__name__}: {str(e)}. First row: {df_new.iloc[0].to_dict()}\")\n",
    "                        transaction.rollback()\n",
    "                        return\n",
    "                else:\n",
    "                    logger.info(f\"No new rows to load into factor_returns from {file_path} (sheet {sheet_name})\")\n",
    "                    logger.debug(f\"Input date range: {df_long['date'].min()} to {df_long['date'].max()}\")\n",
    "                    logger.debug(f\"Database date range: {existing['date'].min() if not existing.empty else 'N/A'} to \"\n",
    "                                f\"{existing['date'].max() if not existing.empty else 'N/A'}\")\n",
    "        \n",
    "        if invalid_dates:\n",
    "            logger.info(f\"Review {sheet_name} for unfixable dates at rows: {invalid_dates}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path} (sheet {sheet_name}): {type(e).__name__}: {str(e)}\")\n",
    "\n",
    "# Run the script\n",
    "process_tsmom(config)\n",
    "logger.info(\"TSMOM data processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6714b374-5d52-4dca-be77-ec51f2d8a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QMJ script FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04631222-18f5-4386-9356-7d9401518c5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 18:53:00,684 - INFO - Processing C:\\Users\\JulianHeron\\Downloads\\Quality Minus Junk 10 QualitySorted Portfolios Monthly.xlsx (sheet 10 Portfolios Formed on Quality)\n",
      "2025-04-13 18:53:00,967 - INFO - Read 809 rows with columns: ['DATE', 'P1 (low quality)', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10 (high quality)', 'P10-P1', 'P1 (low quality).1', 'P2.1', 'P3.1', 'P4.1', 'P5.1', 'P6.1', 'P7.1', 'P8.1', 'P9.1', 'P10 (high quality).1', 'P10-P1.1']\n",
      "2025-04-13 18:53:00,970 - INFO - Set rows 0383 for Global columns ['P1 (low quality).1', 'P2.1', 'P3.1', 'P4.1', 'P5.1', 'P6.1', 'P7.1', 'P8.1', 'P9.1', 'P10 (high quality).1', 'P10-P1.1'] to NaN\n",
      "2025-04-13 18:53:00,972 - INFO - US columns: ['DATE', 'P1 (low quality)', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10 (high quality)', 'P10-P1']\n",
      "2025-04-13 18:53:00,973 - INFO - Global columns: ['DATE', 'P1 (low quality).1', 'P2.1', 'P3.1', 'P4.1', 'P5.1', 'P6.1', 'P7.1', 'P8.1', 'P9.1', 'P10 (high quality).1', 'P10-P1.1']\n",
      "2025-04-13 18:53:00,974 - INFO - US columns after renaming: ['date', 'QMJ_P1', 'QMJ_P2', 'QMJ_P3', 'QMJ_P4', 'QMJ_P5', 'QMJ_P6', 'QMJ_P7', 'QMJ_P8', 'QMJ_P9', 'QMJ_P10', 'QMJ']\n",
      "2025-04-13 18:53:00,974 - INFO - Global columns after renaming: ['date', 'QMJ_P1', 'QMJ_P2', 'QMJ_P3', 'QMJ_P4', 'QMJ_P5', 'QMJ_P6', 'QMJ_P7', 'QMJ_P8', 'QMJ_P9', 'QMJ_P10', 'QMJ']\n",
      "2025-04-13 18:53:01,220 - DEBUG - After date cleaning, 809 US rows remain\n",
      "2025-04-13 18:53:01,429 - DEBUG - After date cleaning, 809 Global rows remain\n",
      "2025-04-13 18:53:01,437 - INFO - Rows after melting and dropping NA: 13574\n",
      "2025-04-13 18:53:01,439 - INFO - USA rows: 8899\n",
      "2025-04-13 18:53:01,441 - INFO - Global rows: 4675\n",
      "2025-04-13 18:53:01,683 - DEBUG - Raw existing data sample (first 5 rows): [{'factor': 'BAB', 'date': datetime.date(1930, 12, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 1, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 2, 28), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 3, 31), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}, {'factor': 'BAB', 'date': datetime.date(1931, 4, 30), 'associated_paper': 'Betting Against Beta (Frazzini and Pedersen, 2014)', 'region': 'USA'}]\n",
      "2025-04-13 18:53:02,199 - DEBUG - Existing keys sample (first 5): [('TSM-FI', datetime.date(2005, 9, 30), 'Time Series Momentum (Moskowitz, Ooi, and Pedersen, 2012)', 'Global'), ('HML_Devil', datetime.date(2004, 5, 31), \"The Devil's in HML's Details\", 'Intl'), ('HML_Devil', datetime.date(1954, 7, 31), \"The Devil's in HML's Details\", 'USA'), ('MKT', datetime.date(1989, 9, 30), 'Fama-French Factors', 'Intl'), ('MKT', datetime.date(2008, 10, 31), 'Fama-French Factors', 'Global')]\n",
      "2025-04-13 18:53:02,199 - DEBUG - Total existing keys: 18595\n",
      "2025-04-13 18:53:02,212 - DEBUG - New rows to insert: 13574. Sample (first 5): [{'factor': 'QMJ_P1', 'date': Timestamp('1957-07-31 00:00:00'), 'region': 'USA'}, {'factor': 'QMJ_P1', 'date': Timestamp('1957-08-31 00:00:00'), 'region': 'USA'}, {'factor': 'QMJ_P1', 'date': Timestamp('1957-09-30 00:00:00'), 'region': 'USA'}, {'factor': 'QMJ_P1', 'date': Timestamp('1957-10-31 00:00:00'), 'region': 'USA'}, {'factor': 'QMJ_P1', 'date': Timestamp('1957-11-30 00:00:00'), 'region': 'USA'}]\n",
      "2025-04-13 18:53:03,473 - INFO - Inserted 13574 new rows into factor_returns from C:\\Users\\JulianHeron\\Downloads\\Quality Minus Junk 10 QualitySorted Portfolios Monthly.xlsx (sheet 10 Portfolios Formed on Quality)\n",
      "2025-04-13 18:53:03,481 - ERROR - Error processing C:\\Users\\JulianHeron\\Downloads\\Quality Minus Junk 10 QualitySorted Portfolios Monthly.xlsx (sheet 10 Portfolios Formed on Quality): UnboundLocalError: cannot access local variable 'invalid_dates' where it is not associated with a value\n",
      "2025-04-13 18:53:03,488 - INFO - QMJ data processing complete!\n"
     ]
    }
   ],
   "source": [
    "# QMJ Version 2.1 FINAL\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.types import VARCHAR, DATE, DECIMAL\n",
    "import logging\n",
    "from dateutil.parser import parse\n",
    "import re\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(), logging.FileHandler('load_qmj_data.log')]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Database connection\n",
    "engine = create_engine(\n",
    "    \"mssql+pyodbc://JULIANS_LAPTOP\\\\SQLEXPRESS/CWA_Fund_Database\"\n",
    "    \"?driver=ODBC+Driver+18+for+SQL+Server&trusted_connection=yes&TrustServerCertificate=yes\"\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    \"path\": r\"C:\\Users\\JulianHeron\\Downloads\\Quality Minus Junk 10 QualitySorted Portfolios Monthly.xlsx\",\n",
    "    \"sheet\": \"10 Portfolios Formed on Quality\",\n",
    "    \"start_row\": 19,\n",
    "    \"paper\": \"Quality Minus Junk (Asness, Frazzini and Pedersen, 2014)\",\n",
    "    \"headers\": [\"DATE\", \"P1 (low quality)\", \"P2\", \"P3\", \"P4\", \"P5\", \"P6\", \"P7\", \"P8\", \"P9\", \"P10 (high quality)\", \"P10-P1\", \n",
    "                \"P1 (low quality).1\", \"P2.1\", \"P3.1\", \"P4.1\", \"P5.1\", \"P6.1\", \"P7.1\", \"P8.1\", \"P9.1\", \"P10 (high quality).1\", \"P10-P1.1\"],\n",
    "    \"columns\": {\n",
    "        \"DATE\": \"DATE\",\n",
    "        \"P1 (low quality)\": \"QMJ_P1\",\n",
    "        \"P2\": \"QMJ_P2\",\n",
    "        \"P3\": \"QMJ_P3\",\n",
    "        \"P4\": \"QMJ_P4\",\n",
    "        \"P5\": \"QMJ_P5\",\n",
    "        \"P6\": \"QMJ_P6\",\n",
    "        \"P7\": \"QMJ_P7\",\n",
    "        \"P8\": \"QMJ_P8\",\n",
    "        \"P9\": \"QMJ_P9\",\n",
    "        \"P10 (high quality)\": \"QMJ_P10\",\n",
    "        \"P10-P1\": \"QMJ\",\n",
    "        \"P1 (low quality).1\": \"QMJ_P1\",\n",
    "        \"P2.1\": \"QMJ_P2\",\n",
    "        \"P3.1\": \"QMJ_P3\",\n",
    "        \"P4.1\": \"QMJ_P4\",\n",
    "        \"P5.1\": \"QMJ_P5\",\n",
    "        \"P6.1\": \"QMJ_P6\",\n",
    "        \"P7.1\": \"QMJ_P7\",\n",
    "        \"P8.1\": \"QMJ_P8\",\n",
    "        \"P9.1\": \"QMJ_P9\",\n",
    "        \"P10 (high quality).1\": \"QMJ_P10\",\n",
    "        \"P10-P1.1\": \"QMJ\"\n",
    "    },\n",
    "    \"region_map\": {\n",
    "        \"P1 (low quality)\": \"USA\",\n",
    "        \"P2\": \"USA\",\n",
    "        \"P3\": \"USA\",\n",
    "        \"P4\": \"USA\",\n",
    "        \"P5\": \"USA\",\n",
    "        \"P6\": \"USA\",\n",
    "        \"P7\": \"USA\",\n",
    "        \"P8\": \"USA\",\n",
    "        \"P9\": \"USA\",\n",
    "        \"P10 (high quality)\": \"USA\",\n",
    "        \"P10-P1\": \"USA\",\n",
    "        \"P1 (low quality).1\": \"Global\",\n",
    "        \"P2.1\": \"Global\",\n",
    "        \"P3.1\": \"Global\",\n",
    "        \"P4.1\": \"Global\",\n",
    "        \"P5.1\": \"Global\",\n",
    "        \"P6.1\": \"Global\",\n",
    "        \"P7.1\": \"Global\",\n",
    "        \"P8.1\": \"Global\",\n",
    "        \"P9.1\": \"Global\",\n",
    "        \"P10 (high quality).1\": \"Global\",\n",
    "        \"P10-P1.1\": \"Global\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def clean_date(date_val):\n",
    "    \"\"\"Attempt to clean and parse a date value, returning None if unfixable.\"\"\"\n",
    "    if pd.isna(date_val) or str(date_val).strip().lower() in ['', 'nan']:\n",
    "        return None\n",
    "    try:\n",
    "        return pd.to_datetime(date_val, errors='coerce')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        return parse(str(date_val), fuzzy=False)\n",
    "    except:\n",
    "        pass\n",
    "    date_str = str(date_val).strip()\n",
    "    if re.match(r'\\d{4}-\\d{2}-\\d{2}', date_str):\n",
    "        try:\n",
    "            year, month, day = map(int, date_str.split('-'))\n",
    "            if month > 12:\n",
    "                month = 12\n",
    "            if day > 31:\n",
    "                day = 31\n",
    "            return pd.to_datetime(f\"{year}-{month:02d}-{day:02d}\")\n",
    "        except:\n",
    "            pass\n",
    "    logger.warning(f\"Unable to parse date: {date_val}\")\n",
    "    return None\n",
    "\n",
    "def process_qmj(config):\n",
    "    file_path = config['path']\n",
    "    sheet_name = config['sheet']\n",
    "    logger.info(f\"Processing {file_path} (sheet {sheet_name})\")\n",
    "    \n",
    "    try:\n",
    "        # Read the Excel sheet\n",
    "        df = pd.read_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            header=None,\n",
    "            skiprows=config['start_row']\n",
    "        )\n",
    "        df.columns = config['headers']\n",
    "        logger.info(f\"Read {len(df)} rows with columns: {list(df.columns)}\")\n",
    "\n",
    "        # Set Global columns to NaN before row 404 (index 384 = 404 - 19 - 1)\n",
    "        global_start_row = 404 - config['start_row'] - 1\n",
    "        global_cols = [col for col in df.columns if col.endswith('.1') or col == 'P10-P1.1']\n",
    "        if global_cols:\n",
    "            non_na_count = df.loc[:global_start_row-1, global_cols].notna().sum().sum()\n",
    "            if non_na_count > 0:\n",
    "                logger.warning(f\"Found {non_na_count} non-NaN values in Global columns before row 404\")\n",
    "            df.loc[:global_start_row-1, global_cols] = pd.NA\n",
    "            logger.info(f\"Set rows 0{global_start_row-1} for Global columns {global_cols} to NaN\")\n",
    "\n",
    "        # Split into US and Global DataFrames\n",
    "        date_col = 'DATE'\n",
    "        us_cols = [col for col in df.columns if col == date_col or (col in config['columns'] and not col.endswith('.1') and col != 'P10-P1.1')]\n",
    "        global_cols = [date_col] + [col for col in df.columns if col.endswith('.1') or col == 'P10-P1.1']\n",
    "        \n",
    "        df_us = df[us_cols].copy()\n",
    "        df_global = df[global_cols].copy()\n",
    "        logger.info(f\"US columns: {list(df_us.columns)}\")\n",
    "        logger.info(f\"Global columns: {list(df_global.columns)}\")\n",
    "\n",
    "        # Rename columns\n",
    "        df_us.columns = ['date' if col == date_col else config['columns'][col] for col in df_us.columns]\n",
    "        df_global.columns = ['date' if col == date_col else config['columns'][col] for col in df_global.columns]\n",
    "        logger.info(f\"US columns after renaming: {list(df_us.columns)}\")\n",
    "        logger.info(f\"Global columns after renaming: {list(df_global.columns)}\")\n",
    "\n",
    "        # Clean and parse dates\n",
    "        invalid_dates = []\n",
    "        for df_temp, label in [(df_us, 'US'), (df_global, 'Global')]:\n",
    "            df_temp['date'] = df_temp['date'].apply(clean_date)\n",
    "            invalid_indices = df_temp['date'].isna()\n",
    "            if invalid_indices.any():\n",
    "                invalid_dates.extend(df_temp[invalid_indices].index.tolist())\n",
    "                logger.warning(f\"Found {len(df_temp[invalid_indices])} unfixable dates in {sheet_name} ({label}) at rows: {df_temp[invalid_indices].index.tolist()}\")\n",
    "                logger.debug(f\"Sample invalid date rows ({label}): {df_temp[invalid_indices].head().to_dict('records')}\")\n",
    "            df_temp.dropna(subset=['date'], inplace=True)\n",
    "            logger.debug(f\"After date cleaning, {len(df_temp)} {label} rows remain\")\n",
    "\n",
    "        # Normalize dates to date only\n",
    "        df_us['date'] = df_us['date'].dt.date\n",
    "        df_global['date'] = df_global['date'].dt.date\n",
    "\n",
    "        # Melt DataFrames separately\n",
    "        us_value_vars = [col for col in df_us.columns if col != 'date']\n",
    "        global_value_vars = [col for col in df_global.columns if col != 'date']\n",
    "        \n",
    "        df_us_long = df_us.melt(\n",
    "            id_vars=['date'],\n",
    "            value_vars=us_value_vars,\n",
    "            var_name='factor',\n",
    "            value_name='value'\n",
    "        )\n",
    "        df_us_long['region'] = 'USA'\n",
    "        \n",
    "        df_global_long = df_global.melt(\n",
    "            id_vars=['date'],\n",
    "            value_vars=global_value_vars,\n",
    "            var_name='factor',\n",
    "            value_name='value'\n",
    "        )\n",
    "        df_global_long['region'] = 'Global'\n",
    "        \n",
    "        # Concatenate and clean\n",
    "        df_long = pd.concat([df_us_long, df_global_long], ignore_index=True)\n",
    "        df_long['asset_class'] = \"Equity\"\n",
    "        df_long['associated_paper'] = config['paper']\n",
    "        df_long = df_long.dropna(subset=['value'])\n",
    "        logger.info(f\"Rows after melting and dropping NA: {len(df_long)}\")\n",
    "        logger.info(f\"USA rows: {len(df_long[df_long['region'] == 'USA'])}\")\n",
    "        logger.info(f\"Global rows: {len(df_long[df_long['region'] == 'Global'])}\")\n",
    "        \n",
    "        # Convert date to datetime for SQL compatibility\n",
    "        df_long['date'] = pd.to_datetime(df_long['date'])\n",
    "        \n",
    "        # Reorder columns to match table schema\n",
    "        df_long = df_long[['factor', 'date', 'associated_paper', 'value', 'region', 'asset_class']]\n",
    "        \n",
    "        # Create key for duplicate checking\n",
    "        df_long['key'] = df_long.apply(\n",
    "            lambda row: (\n",
    "                row['factor'],\n",
    "                row['date'].date(),\n",
    "                row['associated_paper'],\n",
    "                row['region']\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Check for duplicates within the new data\n",
    "        duplicates = df_long.duplicated(subset=['key'], keep=False)\n",
    "        if duplicates.any():\n",
    "            logger.warning(f\"Found {duplicates.sum()} duplicate keys within {sheet_name} data. Sample (first 5): \"\n",
    "                          f\"{df_long[duplicates][['factor', 'date', 'region', 'value']].head(5).to_dict('records')}\")\n",
    "            df_long = df_long.drop_duplicates(subset=['key'], keep='first')\n",
    "            logger.debug(f\"After removing intra-sheet duplicates, {len(df_long)} rows remain\")\n",
    "\n",
    "        # Insert all data with transaction\n",
    "        with engine.connect() as connection:\n",
    "            with connection.begin() as transaction:\n",
    "                existing = pd.read_sql(\n",
    "                    \"SELECT factor, date, associated_paper, region FROM factor_returns\",\n",
    "                    connection\n",
    "                )\n",
    "                logger.debug(f\"Raw existing data sample (first 5 rows): {existing.head().to_dict('records')}\")\n",
    "                existing['date'] = pd.to_datetime(existing['date'], errors='coerce').dt.date\n",
    "                existing_keys = set(\n",
    "                    (row['factor'], row['date'], row['associated_paper'], row['region'])\n",
    "                    for _, row in existing.iterrows()\n",
    "                    if pd.notna(row['date']) and pd.notna(row['region'])\n",
    "                )\n",
    "                logger.debug(f\"Existing keys sample (first 5): {list(existing_keys)[:5]}\")\n",
    "                logger.debug(f\"Total existing keys: {len(existing_keys)}\")\n",
    "\n",
    "                df_new = df_long[~df_long['key'].isin(existing_keys)].drop(columns=['key'])\n",
    "        \n",
    "                if not df_new.empty:\n",
    "                    logger.debug(f\"New rows to insert: {len(df_new)}. Sample (first 5): \"\n",
    "                                f\"{df_new[['factor', 'date', 'region']].head().to_dict('records')}\")\n",
    "                    \n",
    "                    try:\n",
    "                        df_new.to_sql(\n",
    "                            'factor_returns',\n",
    "                            connection,\n",
    "                            if_exists='append',\n",
    "                            index=False,\n",
    "                            dtype={\n",
    "                                'factor': VARCHAR(50),\n",
    "                                'date': DATE,\n",
    "                                'associated_paper': VARCHAR(100),\n",
    "                                'value': DECIMAL(15, 6),\n",
    "                                'region': VARCHAR(50),\n",
    "                                'asset_class': VARCHAR(50)\n",
    "                            }\n",
    "                        )\n",
    "                        logger.info(f\"Inserted {len(df_new)} new rows into factor_returns from {file_path} (sheet {sheet_name})\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error inserting {len(df_new)} rows for {sheet_name}: \"\n",
    "                                    f\"{type(e).__name__}: {str(e)}. First row: {df_new.iloc[0].to_dict()}\")\n",
    "                        transaction.rollback()\n",
    "                        return\n",
    "                else:\n",
    "                    logger.info(f\"No new rows to load into factor_returns from {file_path} (sheet {sheet_name})\")\n",
    "                    logger.debug(f\"Input date range: {df_long['date'].min()} to {df_long['date'].max()}\")\n",
    "                    logger.debug(f\"Database date range: {existing['date'].min() if not existing.empty else 'N/A'} to \"\n",
    "                                f\"{existing['date'].max() if not existing.empty else 'N/A'}\")\n",
    "        \n",
    "        if invalid_dates:\n",
    "            logger.info(f\"Review {sheet_name} for unfixable dates at rows: {invalid_dates}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path} (sheet {sheet_name}): {type(e).__name__}: {str(e)}\")\n",
    "\n",
    "# Run the script\n",
    "process_qmj(config)\n",
    "logger.info(\"QMJ data processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30fb5fb-9fc8-407d-98e7-8530fbdf2654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Century Factors Script FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05efc39a-9012-4fad-9f52-5a5a31952ef0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 18:31:26,274 - INFO - Processing C:\\Users\\JulianHeron\\Downloads\\Century of Factor Premia Monthly (1).xlsx (sheet 0)\n",
      "2025-04-13 18:31:26,955 - INFO - Read 1183 rows with columns: ['DATE', 'US Stock Selection Value', 'US Stock Selection Momentum', 'US Stock Selection Defensive', 'US Stock Selection Multi-style', 'Intl Stock Selection Value', 'Intl Stock Selection Momentum', 'Intl Stock Selection Defensive', 'Intl Stock Selection Multi-style', 'Equity indices Value', 'Equity indices Momentum', 'Equity indices Carry', 'Equity indices Defensive', 'Equity indices Multi-style', 'Fixed income Value', 'Fixed income Momentum', 'Fixed income Carry', 'Fixed income Defensive', 'Fixed income Multi-style', 'Currencies Value', 'Currencies Momentum', 'Currencies Carry', 'Currencies Multi-style', 'Commodities Value', 'Commodities Momentum', 'Commodities Carry', 'Commodities Multi-style', 'All Stock Selection Value', 'All Stock Selection Momentum', 'All Stock Selection Defensive', 'All Stock Selection Multi-style', 'All Macro Value', 'All Macro Momentum', 'All Macro Carry', 'All Macro Defensive', 'All Macro Multi-style', 'All asset classes Value', 'All asset classes Momentum', 'All asset classes Carry', 'All asset classes Defensive', 'All asset classes Multi-style', 'Equity indices Market', 'Fixed income Market', 'Commodities Market', 'All Macro Market']\n",
      "2025-04-13 18:31:27,001 - WARNING - Found 1 unfixable dates in 0 at rows: [0]\n",
      "2025-04-13 18:31:27,005 - DEBUG - Sample invalid date rows: [{'DATE': 'Date', 'US Stock Selection Value': 'US Stock Selection Value', 'US Stock Selection Momentum': 'US Stock Selection Momentum', 'US Stock Selection Defensive': 'US Stock Selection Defensive', 'US Stock Selection Multi-style': 'US Stock Selection Multi-style', 'Intl Stock Selection Value': 'Intl Stock Selection Value', 'Intl Stock Selection Momentum': 'Intl Stock Selection Momentum', 'Intl Stock Selection Defensive': 'Intl Stock Selection Defensive', 'Intl Stock Selection Multi-style': 'Intl Stock Selection Multi-style', 'Equity indices Value': 'Equity indices Value', 'Equity indices Momentum': 'Equity indices Momentum', 'Equity indices Carry': 'Equity indices Carry', 'Equity indices Defensive': 'Equity indices Defensive', 'Equity indices Multi-style': 'Equity indices Multi-style', 'Fixed income Value': 'Fixed income Value', 'Fixed income Momentum': 'Fixed income Momentum', 'Fixed income Carry': 'Fixed income Carry', 'Fixed income Defensive': 'Fixed income Defensive', 'Fixed income Multi-style': 'Fixed income Multi-style', 'Currencies Value': 'Currencies Value', 'Currencies Momentum': 'Currencies Momentum', 'Currencies Carry': 'Currencies Carry', 'Currencies Multi-style': 'Currencies Multi-style', 'Commodities Value': 'Commodities Value', 'Commodities Momentum': 'Commodities Momentum', 'Commodities Carry': 'Commodities Carry', 'Commodities Multi-style': 'Commodities Multi-style', 'All Stock Selection Value': 'All Stock Selection Value', 'All Stock Selection Momentum': 'All Stock Selection Momentum', 'All Stock Selection Defensive': 'All Stock Selection Defensive', 'All Stock Selection Multi-style': 'All Stock Selection Multi-style', 'All Macro Value': 'All Macro Value', 'All Macro Momentum': 'All Macro Momentum', 'All Macro Carry': 'All Macro Carry', 'All Macro Defensive': 'All Macro Defensive', 'All Macro Multi-style': 'All Macro Multi-style', 'All asset classes Value': 'All asset classes Value', 'All asset classes Momentum': 'All asset classes Momentum', 'All asset classes Carry': 'All asset classes Carry', 'All asset classes Defensive': 'All asset classes Defensive', 'All asset classes Multi-style': 'All asset classes Multi-style', 'Equity indices Market': 'Equity indices Market', 'Fixed income Market': 'Fixed income Market', 'Commodities Market': 'Commodities Market', 'All Macro Market': 'All Macro Market', 'date': NaT}]\n",
      "2025-04-13 18:31:27,008 - DEBUG - After date cleaning, 1182 rows remain\n",
      "2025-04-13 18:31:27,076 - DEBUG - Rows after melting and dropping NA: 46491\n",
      "2025-04-13 18:31:27,985 - DEBUG - Raw existing data sample (first 5 rows): [{'factor': 'Carry', 'portfolio': 'All asset classes Carry', 'date': datetime.date(1926, 7, 30), 'associated_paper': 'Century of Factor Premia Monthly-Ilmanen et al. (2021)', 'region': 'Global'}, {'factor': 'Carry', 'portfolio': 'All asset classes Carry', 'date': datetime.date(1926, 8, 31), 'associated_paper': 'Century of Factor Premia Monthly-Ilmanen et al. (2021)', 'region': 'Global'}, {'factor': 'Carry', 'portfolio': 'All asset classes Carry', 'date': datetime.date(1926, 9, 30), 'associated_paper': 'Century of Factor Premia Monthly-Ilmanen et al. (2021)', 'region': 'Global'}, {'factor': 'Carry', 'portfolio': 'All asset classes Carry', 'date': datetime.date(1926, 10, 29), 'associated_paper': 'Century of Factor Premia Monthly-Ilmanen et al. (2021)', 'region': 'Global'}, {'factor': 'Carry', 'portfolio': 'All asset classes Carry', 'date': datetime.date(1926, 11, 30), 'associated_paper': 'Century of Factor Premia Monthly-Ilmanen et al. (2021)', 'region': 'Global'}]\n",
      "2025-04-13 18:31:29,445 - DEBUG - Existing keys sample (first 5): [('Carry', 'Commodities Carry', datetime.date(1965, 1, 29), 'Century of Factor Premia Monthly-Ilmanen et al. (2021)', 'Global'), ('Momentum', 'Currencies Momentum', datetime.date(2002, 5, 31), 'Century of Factor Premia Monthly-Ilmanen et al. (2021)', 'Global'), ('Defensive', 'All Stock Selection Defensive', datetime.date(1994, 4, 29), 'Century of Factor Premia Monthly-Ilmanen et al. (2021)', 'Global'), ('Momentum', 'All asset classes Momentum', datetime.date(1969, 5, 30), 'Century of Factor Premia Monthly-Ilmanen et al. (2021)', 'Global'), ('Value', 'US Stock Selection Value', datetime.date(1996, 2, 29), 'Century of Factor Premia Monthly-Ilmanen et al. (2021)', 'USA')]\n",
      "2025-04-13 18:31:29,446 - DEBUG - Total existing keys: 46491\n",
      "2025-04-13 18:31:29,490 - INFO - No new rows to load into aqr_century_factors from C:\\Users\\JulianHeron\\Downloads\\Century of Factor Premia Monthly (1).xlsx (sheet 0)\n",
      "2025-04-13 18:31:29,492 - DEBUG - Input date range: 1926-07-30 00:00:00 to 2024-12-31 00:00:00\n",
      "2025-04-13 18:31:29,495 - DEBUG - Database date range: 1926-07-30 to 2024-12-31\n",
      "2025-04-13 18:31:29,496 - INFO - Review 0 for unfixable dates at rows: [0]\n",
      "2025-04-13 18:31:29,520 - INFO - Century data processing complete!\n"
     ]
    }
   ],
   "source": [
    "# Century Version 2.4 FINAL\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.types import VARCHAR, DATE, DECIMAL\n",
    "import logging\n",
    "from dateutil.parser import parse\n",
    "import re\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(), logging.FileHandler('load_century_data.log')]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Database connection\n",
    "engine = create_engine(\n",
    "    \"mssql+pyodbc://JULIANS_LAPTOP\\\\SQLEXPRESS/CWA_Fund_Database\"\n",
    "    \"?driver=ODBC+Driver+18+for+SQL+Server&trusted_connection=yes&TrustServerCertificate=yes\"\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    \"path\": r\"C:\\Users\\JulianHeron\\Downloads\\Century of Factor Premia Monthly (1).xlsx\",\n",
    "    \"sheet\": 0,\n",
    "    \"start_row\": 18,\n",
    "    \"paper\": \"Century of Factor Premia Monthly-Ilmanen et al. (2021)\",\n",
    "    \"headers\": [\"DATE\", \"US Stock Selection Value\", \"US Stock Selection Momentum\", \"US Stock Selection Defensive\", \"US Stock Selection Multi-style\", \n",
    "                \"Intl Stock Selection Value\", \"Intl Stock Selection Momentum\", \"Intl Stock Selection Defensive\", \"Intl Stock Selection Multi-style\", \n",
    "                \"Equity indices Value\", \"Equity indices Momentum\", \"Equity indices Carry\", \"Equity indices Defensive\", \"Equity indices Multi-style\", \n",
    "                \"Fixed income Value\", \"Fixed income Momentum\", \"Fixed income Carry\", \"Fixed income Defensive\", \"Fixed income Multi-style\", \n",
    "                \"Currencies Value\", \"Currencies Momentum\", \"Currencies Carry\", \"Currencies Multi-style\", \n",
    "                \"Commodities Value\", \"Commodities Momentum\", \"Commodities Carry\", \"Commodities Multi-style\", \n",
    "                \"All Stock Selection Value\", \"All Stock Selection Momentum\", \"All Stock Selection Defensive\", \"All Stock Selection Multi-style\", \n",
    "                \"All Macro Value\", \"All Macro Momentum\", \"All Macro Carry\", \"All Macro Defensive\", \"All Macro Multi-style\", \n",
    "                \"All asset classes Value\", \"All asset classes Momentum\", \"All asset classes Carry\", \"All asset classes Defensive\", \"All asset classes Multi-style\", \n",
    "                \"Equity indices Market\", \"Fixed income Market\", \"Commodities Market\", \"All Macro Market\"]\n",
    "}\n",
    "\n",
    "def clean_date(date_val):\n",
    "    \"\"\"Attempt to clean and parse a date value, returning None if unfixable.\"\"\"\n",
    "    if pd.isna(date_val) or str(date_val).strip().lower() in ['', 'nan']:\n",
    "        return None\n",
    "    try:\n",
    "        return pd.to_datetime(date_val, errors='coerce')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        return parse(str(date_val), fuzzy=False)\n",
    "    except:\n",
    "        pass\n",
    "    date_str = str(date_val).strip()\n",
    "    if re.match(r'\\d{4}-\\d{2}-\\d{2}', date_str):\n",
    "        try:\n",
    "            year, month, day = map(int, date_str.split('-'))\n",
    "            if month > 12:\n",
    "                month = 12\n",
    "            if day > 31:\n",
    "                day = 31\n",
    "            return pd.to_datetime(f\"{year}-{month:02d}-{day:02d}\")\n",
    "        except:\n",
    "            pass\n",
    "    logger.warning(f\"Unable to parse date: {date_val}\")\n",
    "    return None\n",
    "\n",
    "def process_century(config):\n",
    "    file_path = config['path']\n",
    "    sheet_name = config['sheet']\n",
    "    logger.info(f\"Processing {file_path} (sheet {sheet_name})\")\n",
    "    \n",
    "    try:\n",
    "        # Read the Excel sheet\n",
    "        df = pd.read_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            header=None,\n",
    "            skiprows=config['start_row']\n",
    "        )\n",
    "        df.columns = config['headers']\n",
    "        logger.info(f\"Read {len(df)} rows with columns: {list(df.columns)}\")\n",
    "\n",
    "        # Clean and parse dates\n",
    "        date_col = 'DATE'\n",
    "        df['date'] = df[date_col].apply(clean_date)\n",
    "        invalid_indices = df['date'].isna()\n",
    "        if invalid_indices.any():\n",
    "            invalid_dates = df[invalid_indices].index.tolist()\n",
    "            logger.warning(f\"Found {len(invalid_dates)} unfixable dates in {sheet_name} at rows: {invalid_dates}\")\n",
    "            logger.debug(f\"Sample invalid date rows: {df[invalid_indices].head().to_dict('records')}\")\n",
    "        df = df.dropna(subset=['date'])\n",
    "        logger.debug(f\"After date cleaning, {len(df)} rows remain\")\n",
    "\n",
    "        # Normalize dates to date only\n",
    "        df['date'] = df['date'].dt.date\n",
    "\n",
    "        # Melt the DataFrame, excluding DATE\n",
    "        value_vars = [col for col in df.columns if col != date_col]\n",
    "        df_long = df.melt(\n",
    "            id_vars=['date'],\n",
    "            value_vars=value_vars,\n",
    "            var_name='portfolio',\n",
    "            value_name='value'\n",
    "        )\n",
    "        \n",
    "        # Assign columns for aqr_century_factors\n",
    "        df_long['factor'] = df_long['portfolio'].apply(\n",
    "            lambda x: 'Value' if 'Value' in x else 'Momentum' if 'Momentum' in x else 'Defensive' if 'Defensive' in x \n",
    "            else 'Carry' if 'Carry' in x else 'Multi-Style' if 'Multi-style' in x else 'Market' if 'Market' in x else 'Unknown'\n",
    "        )\n",
    "        df_long['region'] = df_long['portfolio'].apply(\n",
    "            lambda x: 'USA' if 'US' in x else 'International' if 'Intl' in x else 'Global'\n",
    "        )\n",
    "        df_long['asset_class'] = df_long['portfolio'].apply(\n",
    "            lambda x: 'Equity' if any(s in x for s in ['Stock Selection', 'Equity indices']) \n",
    "            else 'Fixed Income' if 'Fixed income' in x \n",
    "            else 'Currencies' if 'Currencies' in x \n",
    "            else 'Commodities' if 'Commodities' in x \n",
    "            else 'Multi-Asset' if any(s in x for s in ['All asset classes', 'All Macro']) \n",
    "            else 'Equity' if 'Market' in x \n",
    "            else 'Equity'  # Fallback\n",
    "        )\n",
    "        df_long['associated_paper'] = config['paper']\n",
    "        df_long = df_long.dropna(subset=['value'])\n",
    "        logger.debug(f\"Rows after melting and dropping NA: {len(df_long)}\")\n",
    "        \n",
    "        # Convert date to datetime for SQL compatibility\n",
    "        df_long['date'] = pd.to_datetime(df_long['date'])\n",
    "        \n",
    "        # Reorder columns to match aqr_century_factors schema\n",
    "        df_long = df_long[['factor', 'portfolio', 'asset_class', 'region', 'date', 'value', 'associated_paper']]\n",
    "        \n",
    "        # Create key for duplicate checking\n",
    "        df_long['key'] = df_long.apply(\n",
    "            lambda row: (\n",
    "                row['factor'],\n",
    "                row['portfolio'],\n",
    "                row['date'].date(),\n",
    "                row['associated_paper'],\n",
    "                row['region'] if pd.notna(row['region']) else ''\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Check for duplicates within the new data\n",
    "        duplicates = df_long.duplicated(subset=['key'], keep=False)\n",
    "        if duplicates.any():\n",
    "            logger.warning(f\"Found {duplicates.sum()} duplicate keys within {sheet_name} data. Sample (first 5): \"\n",
    "                          f\"{df_long[duplicates][['factor', 'portfolio', 'date', 'region', 'value']].head(5).to_dict('records')}\")\n",
    "            df_long = df_long.drop_duplicates(subset=['key'], keep='first')\n",
    "            logger.debug(f\"After removing intra-sheet duplicates, {len(df_long)} rows remain\")\n",
    "\n",
    "        # Insert all data with transaction into aqr_century_factors\n",
    "        with engine.connect() as connection:\n",
    "            with connection.begin() as transaction:\n",
    "                existing = pd.read_sql(\n",
    "                    \"SELECT factor, portfolio, date, associated_paper, region FROM aqr_century_factors\",\n",
    "                    connection\n",
    "                )\n",
    "                logger.debug(f\"Raw existing data sample (first 5 rows): {existing.head().to_dict('records')}\")\n",
    "                existing['date'] = pd.to_datetime(existing['date'], errors='coerce').dt.date\n",
    "                existing_keys = set(\n",
    "                    (row['factor'], row['portfolio'], row['date'], row['associated_paper'], row['region'] if pd.notna(row['region']) else '')\n",
    "                    for _, row in existing.iterrows()\n",
    "                    if pd.notna(row['date'])\n",
    "                )\n",
    "                logger.debug(f\"Existing keys sample (first 5): {list(existing_keys)[:5]}\")\n",
    "                logger.debug(f\"Total existing keys: {len(existing_keys)}\")\n",
    "\n",
    "                df_new = df_long[~df_long['key'].isin(existing_keys)].drop(columns=['key'])\n",
    "        \n",
    "                if not df_new.empty:\n",
    "                    logger.debug(f\"New rows to insert: {len(df_new)}. Sample (first 5): \"\n",
    "                                f\"{df_new[['factor', 'portfolio', 'region']].head().to_dict('records')}\")\n",
    "                    \n",
    "                    try:\n",
    "                        df_new.to_sql(\n",
    "                            'aqr_century_factors',\n",
    "                            connection,\n",
    "                            if_exists='append',\n",
    "                            index=False,\n",
    "                            dtype={\n",
    "                                'factor': VARCHAR(50),\n",
    "                                'portfolio': VARCHAR(100),\n",
    "                                'asset_class': VARCHAR(50),\n",
    "                                'region': VARCHAR(50),\n",
    "                                'date': DATE,\n",
    "                                'value': DECIMAL(15, 6),\n",
    "                                'associated_paper': VARCHAR(150)\n",
    "                            }\n",
    "                        )\n",
    "                        logger.info(f\"Inserted {len(df_new)} new rows into aqr_century_factors from {file_path} (sheet {sheet_name})\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error inserting {len(df_new)} rows for {sheet_name}: \"\n",
    "                                    f\"{type(e).__name__}: {str(e)}. First row: {df_new.iloc[0].to_dict()}\")\n",
    "                        transaction.rollback()\n",
    "                        return\n",
    "                else:\n",
    "                    logger.info(f\"No new rows to load into aqr_century_factors from {file_path} (sheet {sheet_name})\")\n",
    "                    logger.debug(f\"Input date range: {df_long['date'].min()} to {df_long['date'].max()}\")\n",
    "                    logger.debug(f\"Database date range: {existing['date'].min() if not existing.empty else 'N/A'} to \"\n",
    "                                f\"{existing['date'].max() if not existing.empty else 'N/A'}\")\n",
    "        \n",
    "        if invalid_dates:\n",
    "            logger.info(f\"Review {sheet_name} for unfixable dates at rows: {invalid_dates}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path} (sheet {sheet_name}): {type(e).__name__}: {str(e)}\")\n",
    "\n",
    "# Run the script\n",
    "process_century(config)\n",
    "logger.info(\"Century data processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad3e1e0-0c65-4377-88a5-1f189c11ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commodities for the long run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303f6ccc-3567-4dda-8ce7-0318cf0e79bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.types import VARCHAR, DATE, DECIMAL\n",
    "import logging\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(), logging.FileHandler('load_com_data.log')]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Database connection\n",
    "engine = create_engine(\n",
    "    \"mssql+pyodbc://JULIANS_LAPTOP\\\\SQLEXPRESS/CWA_Fund_Database\"\n",
    "    \"?driver=ODBC+Driver+18+for+SQL+Server&trusted_connection=yes&TrustServerCertificate=yes\"\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    \"path\": r\"C:\\Users\\JulianHeron\\Downloads\\Commodities for the Long Run Index Level Data Monthly.xlsx\",\n",
    "    \"sheet\": 0,\n",
    "    \"start_row\": 11,\n",
    "    \"paper\": \"Commodities for the Long Run\",\n",
    "    \"headers\": [\"DATE\", \"Excess return of equal-weight commodities portfolio\", \"Excess spot return of equal-weight commodities portfolio\", \"Interest rate adjusted carry of equal-weight commodities portfolio\", \"Spot return of equal-weight commodities portfolio\", \"Carry of equal-weight commodities portfolio\", \"Excess return of long/short commodities portfolio\", \"Excess spot return of long/short commodities portfolio\", \"Interest rate adjusted carry of long/short commodities portfolio\", \"Aggregate backwardation/contango\", \"State of backwardation/contango\", \"State of inflation\"],\n",
    "    \"columns\": {\n",
    "        \"DATE\": \"date\",\n",
    "        \"Excess return of equal-weight commodities portfolio\": \"excess_return_eqwt\",\n",
    "        \"Excess spot return of equal-weight commodities portfolio\": \"excess_spot_return_eqwt\",\n",
    "        \"Interest rate adjusted carry of equal-weight commodities portfolio\": \"ir_adjusted_carry_eqwt\",\n",
    "        \"Spot return of equal-weight commodities portfolio\": \"spot_return_eqwt\",\n",
    "        \"Carry of equal-weight commodities portfolio\": \"carry_eqwt\",\n",
    "        \"Excess return of long/short commodities portfolio\": \"excess_return_long_short\",\n",
    "        \"Excess spot return of long/short commodities portfolio\": \"excess_spot_return_long_short\",\n",
    "        \"Interest rate adjusted carry of long/short commodities portfolio\": \"ir_adjusted_carry_long_short\",\n",
    "        \"Aggregate backwardation/contango\": \"aggregate_backwardation_contango\",\n",
    "        \"State of backwardation/contango\": \"state_backwardation_contango\",\n",
    "        \"State of inflation\": \"state_inflation\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def process_com(config):\n",
    "    file_path = config['path']\n",
    "    sheet_name = config['sheet']\n",
    "    logger.info(f\"Processing {file_path} (sheet {sheet_name})\")\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_name, header=None, skiprows=config['start_row'])\n",
    "        df.columns = config['headers']\n",
    "        logger.info(f\"Read {len(df)} rows with set columns: {list(df.columns)}\")\n",
    "        \n",
    "        expected_cols = list(config['columns'].keys())\n",
    "        df = df[expected_cols]\n",
    "        df.columns = [config['columns'][col] for col in df.columns]\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        df['associated_paper'] = config['paper']\n",
    "        df = df.dropna(subset=['date'])\n",
    "        logger.debug(f\"After dropping NA dates, {len(df)} rows remain\")\n",
    "        \n",
    "        with engine.connect() as connection:\n",
    "            existing = pd.read_sql(\"SELECT date FROM aqr_cmdty_factors\", connection)\n",
    "            existing['date'] = pd.to_datetime(existing['date']).dt.strftime('%Y-%m-%d')\n",
    "            existing_keys = set(existing['date'])\n",
    "            logger.debug(f\"Existing commodity dates sample (first 5): {list(existing_keys)[:5]}\")\n",
    "        \n",
    "        df['date_str'] = df['date'].dt.strftime('%Y-%m-%d')\n",
    "        df_new = df[~df['date_str'].isin(existing_keys)].drop(columns=['date_str'])\n",
    "        \n",
    "        if not df_new.empty:\n",
    "            for chunk in [df_new[i:i+10000] for i in range(0, len(df_new), 10000)]:\n",
    "                chunk.to_sql(\n",
    "                    'aqr_cmdty_factors',\n",
    "                    engine,\n",
    "                    if_exists='append',\n",
    "                    index=False,\n",
    "                    dtype={\n",
    "                        'date': DATE,\n",
    "                        'excess_return_eqwt': DECIMAL(15, 6),\n",
    "                        'excess_spot_return_eqwt': DECIMAL(15, 6),\n",
    "                        'ir_adjusted_carry_eqwt': DECIMAL(15, 6),\n",
    "                        'spot_return_eqwt': DECIMAL(15, 6),\n",
    "                        'carry_eqwt': DECIMAL(15, 6),\n",
    "                        'excess_return_long_short': DECIMAL(15, 6),\n",
    "                        'excess_spot_return_long_short': DECIMAL(15, 6),\n",
    "                        'ir_adjusted_carry_long_short': DECIMAL(15, 6),\n",
    "                        'aggregate_backwardation_contango': DECIMAL(15, 6),\n",
    "                        'state_backwardation_contango': VARCHAR(50),\n",
    "                        'state_inflation': VARCHAR(50),\n",
    "                        'associated_paper': VARCHAR(100)\n",
    "                    }\n",
    "                )\n",
    "            logger.info(f\"Loaded {len(df_new)} new rows into aqr_cmdty_factors from {file_path} (sheet {sheet_name})\")\n",
    "        else:\n",
    "            logger.info(f\"No new rows to load into aqr_cmdty_factors from {file_path} (sheet {sheet_name})\")\n",
    "            logger.debug(f\"Input date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "            logger.debug(f\"Database date range: {existing['date'].min() if not existing.empty else 'N/A'} to {existing['date'].max() if not existing.empty else 'N/A'}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path} (sheet {sheet_name}): {type(e).__name__}: {str(e)}\")\n",
    "\n",
    "# Run the script\n",
    "process_com(config)\n",
    "logger.info(\"COM data processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66238cde-7475-4217-927a-e5d3aca52790",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 18:43:00,225 - INFO - Processing C:\\Users\\JulianHeron\\Downloads\\Commodities for the Long Run Index Level Data Monthly.xlsx (sheet 0)\n",
      "2025-04-11 18:43:00,534 - INFO - Read 1776 rows with columns: ['date', 'Excess return of equal-weight commodities portfolio', 'Excess spot return of equal-weight commodities portfolio', 'Interest rate adjusted carry of equal-weight commodities portfolio', 'Spot return of equal-weight commodities portfolio', 'Carry of equal-weight commodities portfolio', 'Excess return of long/short commodities portfolio', 'Excess spot return of long/short commodities portfolio', 'Interest rate adjusted carry of long/short commodities portfolio', 'Aggregate backwardation/contango', 'State of backwardation/contango', 'State of inflation']\n",
      "C:\\Users\\JulianHeron\\AppData\\Local\\Temp\\ipykernel_40512\\325418905.py:90: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  df['date'] = pd.date_range(start='1871-01-31', periods=len(df), freq='M')\n",
      "2025-04-11 18:43:00,572 - DEBUG - After dropping NA dates, 1776 rows remain\n",
      "2025-04-11 18:43:00,573 - DEBUG - Row count for date: 1776\n",
      "2025-04-11 18:43:00,575 - DEBUG - Row count for excess_return_eqwt: 1776\n",
      "2025-04-11 18:43:00,576 - DEBUG - Row count for excess_spot_return_eqwt: 1776\n",
      "2025-04-11 18:43:00,577 - DEBUG - Row count for ir_adjusted_carry_eqwt: 1776\n",
      "2025-04-11 18:43:00,578 - DEBUG - Row count for spot_return_eqwt: 1776\n",
      "2025-04-11 18:43:00,579 - DEBUG - Row count for carry_eqwt: 1776\n",
      "2025-04-11 18:43:00,580 - DEBUG - Row count for excess_return_long_short: 1776\n",
      "2025-04-11 18:43:00,581 - DEBUG - Row count for excess_spot_return_long_short: 1776\n",
      "2025-04-11 18:43:00,582 - DEBUG - Row count for ir_adjusted_carry_long_short: 1776\n",
      "2025-04-11 18:43:00,583 - DEBUG - Row count for aggregate_backwardation_contango: 1776\n",
      "2025-04-11 18:43:00,584 - DEBUG - Row count for state_backwardation_contango: 1776\n",
      "2025-04-11 18:43:00,585 - DEBUG - Row count for state_inflation: 1775\n",
      "2025-04-11 18:43:00,586 - DEBUG - Row count for associated_paper: 1776\n",
      "2025-04-11 18:43:00,598 - DEBUG - Existing commodity dates sample (first 5): []\n",
      "2025-04-11 18:43:00,603 - DEBUG - New rows to insert: 1776. Sample (first 5): [{'date': Timestamp('1871-01-31 00:00:00'), 'excess_return_eqwt': -0.08256594049005798, 'excess_spot_return_eqwt': -0.07830582927187074, 'ir_adjusted_carry_eqwt': -0.004692970490637905, 'spot_return_eqwt': -0.07582324843016702, 'carry_eqwt': -0.00736662589137782, 'excess_return_long_short': 0.03758747336683327, 'excess_spot_return_long_short': 0.03622574280367108, 'ir_adjusted_carry_long_short': 0.001940714381038433, 'aggregate_backwardation_contango': -0.01861043068398843, 'state_backwardation_contango': 'Contango', 'state_inflation': 'Inflation Up', 'associated_paper': 'Commodities for the Long Run'}, {'date': Timestamp('1871-02-28 00:00:00'), 'excess_return_eqwt': -0.0428978216502566, 'excess_spot_return_eqwt': -0.029285964698269294, 'ir_adjusted_carry_eqwt': -0.013602803546657062, 'spot_return_eqwt': -0.02672432960816462, 'carry_eqwt': -0.016198974136432988, 'excess_return_long_short': 0.021791232875870425, 'excess_spot_return_long_short': 0.03855470206384681, 'ir_adjusted_carry_long_short': -0.016068182155315303, 'aggregate_backwardation_contango': -0.01812398600873999, 'state_backwardation_contango': 'Contango', 'state_inflation': 'Inflation Down', 'associated_paper': 'Commodities for the Long Run'}, {'date': Timestamp('1871-03-31 00:00:00'), 'excess_return_eqwt': 0.21568430014419765, 'excess_spot_return_eqwt': 0.24509172216238675, 'ir_adjusted_carry_eqwt': -0.022364834321063443, 'spot_return_eqwt': 0.24837741756793286, 'carry_eqwt': -0.02493794348417984, 'excess_return_long_short': -0.20138726728670386, 'excess_spot_return_long_short': -0.256191900055743, 'ir_adjusted_carry_long_short': 0.03924418530194085, 'aggregate_backwardation_contango': -0.04148686570217361, 'state_backwardation_contango': 'Contango', 'state_inflation': 'Inflation Up', 'associated_paper': 'Commodities for the Long Run'}, {'date': Timestamp('1871-04-30 00:00:00'), 'excess_return_eqwt': -0.1503676790275369, 'excess_spot_return_eqwt': -0.14138650259681618, 'ir_adjusted_carry_eqwt': -0.010990368644419334, 'spot_return_eqwt': -0.13916428376342305, 'carry_eqwt': -0.013543464186056408, 'excess_return_long_short': 0.07768058539344846, 'excess_spot_return_long_short': 0.045013794983336075, 'ir_adjusted_carry_long_short': 0.03891944718677348, 'aggregate_backwardation_contango': -0.012697375209592299, 'state_backwardation_contango': 'Contango', 'state_inflation': 'Inflation Up', 'associated_paper': 'Commodities for the Long Run'}, {'date': Timestamp('1871-05-31 00:00:00'), 'excess_return_eqwt': -0.05927467504027732, 'excess_spot_return_eqwt': -0.06291137711090805, 'ir_adjusted_carry_eqwt': 0.009671966589570192, 'spot_return_eqwt': -0.06053774430545658, 'carry_eqwt': 0.00712094286497162, 'excess_return_long_short': -0.040315627938210086, 'excess_spot_return_long_short': -0.14690640843535324, 'ir_adjusted_carry_long_short': 0.11912285850299875, 'aggregate_backwardation_contango': -0.012167812534629065, 'state_backwardation_contango': 'Contango', 'state_inflation': 'Inflation Up', 'associated_paper': 'Commodities for the Long Run'}]\n",
      "2025-04-11 18:43:01,783 - INFO - Inserted 1776 new rows into aqr_cmdty_factors from C:\\Users\\JulianHeron\\Downloads\\Commodities for the Long Run Index Level Data Monthly.xlsx (sheet 0)\n",
      "2025-04-11 18:43:01,786 - INFO - COM data processing complete!\n"
     ]
    }
   ],
   "source": [
    "# COM Version 2.0 (Draft)\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.types import VARCHAR, DATE, DECIMAL\n",
    "import logging\n",
    "from dateutil.parser import parse\n",
    "import re\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(), logging.FileHandler('load_com_data.log')]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "engine = create_engine(\n",
    "    \"mssql+pyodbc://JULIANS_LAPTOP\\\\SQLEXPRESS/CWA_Fund_Database\"\n",
    "    \"?driver=ODBC+Driver+18+for+SQL+Server&trusted_connection=yes&TrustServerCertificate=yes\"\n",
    ")\n",
    "\n",
    "config = {\n",
    "    \"path\": r\"C:\\Users\\JulianHeron\\Downloads\\Commodities for the Long Run Index Level Data Monthly.xlsx\",\n",
    "    \"sheet\": 0,\n",
    "    \"start_row\": 11,\n",
    "    \"paper\": \"Commodities for the Long Run\",\n",
    "    \"headers\": [\"date\", \"Excess return of equal-weight commodities portfolio\", \"Excess spot return of equal-weight commodities portfolio\", \n",
    "                \"Interest rate adjusted carry of equal-weight commodities portfolio\", \"Spot return of equal-weight commodities portfolio\", \n",
    "                \"Carry of equal-weight commodities portfolio\", \"Excess return of long/short commodities portfolio\", \n",
    "                \"Excess spot return of long/short commodities portfolio\", \"Interest rate adjusted carry of long/short commodities portfolio\", \n",
    "                \"Aggregate backwardation/contango\", \"State of backwardation/contango\", \"State of inflation\"],\n",
    "    \"columns\": {\n",
    "        \"date\": \"date\",\n",
    "        \"Excess return of equal-weight commodities portfolio\": \"excess_return_eqwt\",\n",
    "        \"Excess spot return of equal-weight commodities portfolio\": \"excess_spot_return_eqwt\",\n",
    "        \"Interest rate adjusted carry of equal-weight commodities portfolio\": \"ir_adjusted_carry_eqwt\",\n",
    "        \"Spot return of equal-weight commodities portfolio\": \"spot_return_eqwt\",\n",
    "        \"Carry of equal-weight commodities portfolio\": \"carry_eqwt\",\n",
    "        \"Excess return of long/short commodities portfolio\": \"excess_return_long_short\",\n",
    "        \"Excess spot return of long/short commodities portfolio\": \"excess_spot_return_long_short\",\n",
    "        \"Interest rate adjusted carry of long/short commodities portfolio\": \"ir_adjusted_carry_long_short\",\n",
    "        \"Aggregate backwardation/contango\": \"aggregate_backwardation_contango\",\n",
    "        \"State of backwardation/contango\": \"state_backwardation_contango\",\n",
    "        \"State of inflation\": \"state_inflation\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def clean_date(date_val):\n",
    "    if pd.isna(date_val) or str(date_val).strip().lower() in ['', 'nan']:\n",
    "        return None\n",
    "    try:\n",
    "        return pd.to_datetime(date_val, errors='coerce')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        return parse(str(date_val), fuzzy=False)\n",
    "    except:\n",
    "        pass\n",
    "    date_str = str(date_val).strip()\n",
    "    if re.match(r'\\d{4}-\\d{2}-\\d{2}', date_str):\n",
    "        try:\n",
    "            year, month, day = map(int, date_str.split('-'))\n",
    "            if month > 12:\n",
    "                month = 12\n",
    "            if day > 31:\n",
    "                day = 31\n",
    "            return pd.to_datetime(f\"{year}-{month:02d}-{day:02d}\")\n",
    "        except:\n",
    "            pass\n",
    "    logger.warning(f\"Unable to parse date: {date_val}\")\n",
    "    return None\n",
    "\n",
    "def process_com(config):\n",
    "    file_path = config['path']\n",
    "    sheet_name = config['sheet']\n",
    "    logger.info(f\"Processing {file_path} (sheet {sheet_name})\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            header=None,\n",
    "            skiprows=config['start_row']\n",
    "        )\n",
    "        df.columns = config['headers']\n",
    "        logger.info(f\"Read {len(df)} rows with columns: {list(df.columns)}\")\n",
    "\n",
    "        # Generate dates for blank column A (1871-01-31 to 2018-12-31, monthly)\n",
    "        df['date'] = pd.date_range(start='1871-01-31', periods=len(df), freq='M')\n",
    "        \n",
    "        expected_cols = list(config['columns'].keys())\n",
    "        df = df[expected_cols]\n",
    "        df.columns = [config['columns'][col] for col in df.columns]\n",
    "        df['date'] = df['date'].apply(clean_date)\n",
    "        df['associated_paper'] = config['paper']\n",
    "        df = df.dropna(subset=['date'])\n",
    "        logger.debug(f\"After dropping NA dates, {len(df)} rows remain\")\n",
    "        \n",
    "        # Debug: Log row counts per column\n",
    "        for col in df.columns:\n",
    "            logger.debug(f\"Row count for {col}: {df[col].notna().sum()}\")\n",
    "\n",
    "        with engine.connect() as connection:\n",
    "            with connection.begin() as transaction:\n",
    "                existing = pd.read_sql(\"SELECT date FROM aqr_cmdty_factors\", connection)\n",
    "                existing['date'] = pd.to_datetime(existing['date'], errors='coerce').dt.date\n",
    "                existing_keys = set(existing['date'])\n",
    "                logger.debug(f\"Existing commodity dates sample (first 5): {list(existing_keys)[:5]}\")\n",
    "\n",
    "                df_new = df[~df['date'].dt.date.isin(existing_keys)]\n",
    "                \n",
    "                if not df_new.empty:\n",
    "                    logger.debug(f\"New rows to insert: {len(df_new)}. Sample (first 5): {df_new.head().to_dict('records')}\")\n",
    "                    df_new.to_sql(\n",
    "                        'aqr_cmdty_factors',\n",
    "                        connection,\n",
    "                        if_exists='append',\n",
    "                        index=False,\n",
    "                        dtype={\n",
    "                            'date': DATE,\n",
    "                            'excess_return_eqwt': DECIMAL(15, 6),\n",
    "                            'excess_spot_return_eqwt': DECIMAL(15, 6),\n",
    "                            'ir_adjusted_carry_eqwt': DECIMAL(15, 6),\n",
    "                            'spot_return_eqwt': DECIMAL(15, 6),\n",
    "                            'carry_eqwt': DECIMAL(15, 6),\n",
    "                            'excess_return_long_short': DECIMAL(15, 6),\n",
    "                            'excess_spot_return_long_short': DECIMAL(15, 6),\n",
    "                            'ir_adjusted_carry_long_short': DECIMAL(15, 6),\n",
    "                            'aggregate_backwardation_contango': DECIMAL(15, 6),\n",
    "                            'state_backwardation_contango': VARCHAR(50),\n",
    "                            'state_inflation': VARCHAR(50),\n",
    "                            'associated_paper': VARCHAR(100)\n",
    "                        }\n",
    "                    )\n",
    "                    logger.info(f\"Inserted {len(df_new)} new rows into aqr_cmdty_factors from {file_path} (sheet {sheet_name})\")\n",
    "                else:\n",
    "                    logger.info(f\"No new rows to load into aqr_cmdty_factors from {file_path} (sheet {sheet_name})\")\n",
    "                    logger.debug(f\"Input date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "                    logger.debug(f\"Database date range: {existing['date'].min() if not existing.empty else 'N/A'} to \"\n",
    "                                f\"{existing['date'].max() if not existing.empty else 'N/A'}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path} (sheet {sheet_name}): {type(e).__name__}: {str(e)}\")\n",
    "\n",
    "process_com(config)\n",
    "logger.info(\"COM data processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b612e68c-f551-423d-9b9e-01385e327f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0be252-41bf-493b-a3c6-25d03a691b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ee5f8-5616-4e98-9eee-2c2b509ff6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f3402-156d-4367-94e5-c53e5fedbaef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43241c9-3fbd-4c96-af4a-510fe8e96684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6027529-fc94-4105-bc99-b4112e247cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd72c1e-51e4-46ed-aa54-5b0079c1a3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a44f58c-3ed9-46d1-98b3-005c7da83e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb36da8-1ca2-4e11-b6b1-62c641e0f3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e024e15-ea80-4719-92e6-e7743722cca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df9a711-9617-46af-a5cc-e759d5da01f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0b809d-da9a-4635-8483-f6ae22dee250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d939334-ed88-4e07-8cd0-388b1b302e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0e5244-2ac4-44a6-9592-21898a988be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cf9d1d-acaa-4bb3-9b5e-2aa160b78beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707fbfe5-6740-446f-9c56-e5a4e95142d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc447c-b234-4161-a78f-9b5b5570cd5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Database Administration)",
   "language": "python",
   "name": "databaseadminenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
