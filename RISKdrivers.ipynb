{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4726f2-27aa-4b26-91de-6ef8831218e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison code for grok scripts\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "def list_files(directory, pattern=\"*.xlsx\"):\n",
    "    return sorted(glob.glob(os.path.join(directory, pattern)))\n",
    "\n",
    "def select_file(directory, prompt):\n",
    "    files = list_files(directory)\n",
    "    if not files:\n",
    "        print(f\"No files found in {directory}\")\n",
    "        sys.exit(1)\n",
    "    print(f\"\\n{prompt}\")\n",
    "    for idx, file in enumerate(files, 1):\n",
    "        print(f\"{idx}. {os.path.basename(file)}\")\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"Enter the number of your selection: \")) - 1\n",
    "            if 0 <= choice < len(files):\n",
    "                return files[choice]\n",
    "            print(\"Invalid selection. Try again.\")\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number.\")\n",
    "\n",
    "def compare_runs(file1_path=None, file2_path=None, output_base_path=None):\n",
    "    # Default paths\n",
    "    input_base_path = r\"C:\\Users\\JulianHeron\\Software Projects\\Test files\"\n",
    "    output_base_path = output_base_path or r\"C:\\Users\\JulianHeron\\Software Projects\\Test files\\Comparison files\"\n",
    "\n",
    "    # If files not provided, prompt user to select\n",
    "    if not file1_path:\n",
    "        file1_path = select_file(input_base_path, \"Select the first file to compare:\")\n",
    "    if not file2_path:\n",
    "        file2_path = select_file(input_base_path, \"Select the second file to compare:\")\n",
    "\n",
    "    # Read the two Excel files\n",
    "    try:\n",
    "        df1 = pd.read_excel(file1_path)\n",
    "        df2 = pd.read_excel(file2_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading files: {e}\")\n",
    "        return\n",
    "\n",
    "    # Ensure SymbolCUSIP is present\n",
    "    if 'SymbolCUSIP' not in df1.columns or 'SymbolCUSIP' not in df2.columns:\n",
    "        print(\"SymbolCUSIP column missing in one or both files\")\n",
    "        return\n",
    "\n",
    "    df1.set_index('SymbolCUSIP', inplace=True)\n",
    "    df2.set_index('SymbolCUSIP', inplace=True)\n",
    "\n",
    "    common_cusips = df1.index.intersection(df2.index)\n",
    "    df_merged = pd.DataFrame(index=common_cusips)\n",
    "\n",
    "    columns_to_compare = ['Final_Classification', 'Method_Used', 'Audit_Log', 'ProductName', 'fund_family', 'investment_strategy', 'FS_insight']\n",
    "    differences = []\n",
    "    for col in columns_to_compare:\n",
    "        if col in df1.columns and col in df2.columns:\n",
    "            df_merged[f'{col}_Run1'] = df1.loc[common_cusips, col]\n",
    "            df_merged[f'{col}_Run2'] = df2.loc[common_cusips, col]\n",
    "            diff_mask = df_merged[f'{col}_Run1'].astype(str) != df_merged[f'{col}_Run2'].astype(str)\n",
    "            df_merged[f'{col}_Changed'] = diff_mask\n",
    "            differences.append(diff_mask)\n",
    "\n",
    "    any_diff = pd.DataFrame(differences).T.any(axis=0)\n",
    "    df_merged['Has_Changes'] = any_diff\n",
    "    df_merged.reset_index(inplace=True)\n",
    "\n",
    "    df_diff = df_merged[df_merged['Has_Changes']].copy()\n",
    "    summary_stats = {\n",
    "        'Total_Funds_Compared': len(common_cusips),\n",
    "        'Funds_With_Changes': len(df_diff),\n",
    "        'Classification_Changes': df_diff['Final_Classification_Changed'].sum(),\n",
    "        'Method_Changes': df_diff['Method_Used_Changed'].sum(),\n",
    "        'Audit_Log_Changes': df_diff['Audit_Log_Changed'].sum(),\n",
    "        'File1': file1_path,\n",
    "        'File2': file2_path\n",
    "    }\n",
    "\n",
    "    # Generate output filename based on input filenames\n",
    "    file1_name = os.path.splitext(os.path.basename(file1_path))[0]\n",
    "    file2_name = os.path.splitext(os.path.basename(file2_path))[0]\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_filename = f\"Comparison_{file1_name}_vs_{file2_name}_{timestamp}.xlsx\"\n",
    "    output_path = os.path.join(output_base_path, output_filename)\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with pd.ExcelWriter(output_path) as writer:\n",
    "        df_diff.to_excel(writer, sheet_name='Differences', index=False)\n",
    "        pd.Series(summary_stats).to_frame('Count').to_excel(writer, sheet_name='Summary')\n",
    "\n",
    "    print(f\"Comparison results exported to {output_path}\")\n",
    "    print(\"Summary Statistics:\")\n",
    "    for key, value in summary_stats.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Command-line arguments or interactive selection\n",
    "    if len(sys.argv) == 3:\n",
    "        file1_path = sys.argv[1]\n",
    "        file2_path = sys.argv[2]\n",
    "        compare_runs(file1_path, file2_path)\n",
    "    else:\n",
    "        compare_runs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "451a70d9-c288-4a7a-99c6-857fd36edb8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 17:53:42,984 - INFO - === Detailed Classification Summary ===\n",
      "2025-03-05 17:53:42,986 - INFO - Total Funds Processed: 5586\n",
      "2025-03-05 17:53:42,986 - INFO - \n",
      "Distribution of Classifications:\n",
      "2025-03-05 17:53:42,987 - INFO -   Persistent Systematic: 1019 (18.24%)\n",
      "2025-03-05 17:53:42,988 - INFO -   Moderate: 888 (15.90%)\n",
      "2025-03-05 17:53:42,988 - INFO -   Heavy Amplification: 353 (6.32%)\n",
      "2025-03-05 17:53:42,989 - INFO -   Slight/None: 3326 (59.54%)\n",
      "2025-03-05 17:53:42,990 - INFO - \n",
      "Classification Methods Breakdown:\n",
      "2025-03-05 17:53:42,991 - INFO -   Auto-Classification: 0\n",
      "2025-03-05 17:53:42,991 - INFO -     By Fund Family:\n",
      "2025-03-05 17:53:42,992 - INFO -       Return Stacked ETFs: 0\n",
      "2025-03-05 17:53:42,993 - INFO -     By Keywords:\n",
      "2025-03-05 17:53:42,993 - INFO -       Slight/None: 0\n",
      "2025-03-05 17:53:42,994 - INFO -       Moderate: 0\n",
      "2025-03-05 17:53:42,994 - INFO -       Persistent Systematic: 0\n",
      "2025-03-05 17:53:42,995 - INFO -       Heavy Amplification: 0\n",
      "2025-03-05 17:53:42,996 - INFO -     By Category:\n",
      "2025-03-05 17:53:42,997 - INFO -       Persistent Systematic: 0\n",
      "2025-03-05 17:53:42,997 - INFO -       Heavy Amplification: 0\n",
      "2025-03-05 17:53:42,999 - INFO -   Exposure-Based: 5586\n",
      "2025-03-05 17:53:42,999 - INFO -     By Tier:\n",
      "2025-03-05 17:53:43,000 - INFO -       Slight/None: 3326\n",
      "2025-03-05 17:53:43,001 - INFO -       Moderate: 888\n",
      "2025-03-05 17:53:43,002 - INFO -       Persistent Systematic: 1019\n",
      "2025-03-05 17:53:43,002 - INFO -       Heavy Amplification: 4\n",
      "2025-03-05 17:53:43,004 - INFO -     Default to Heavy Amplification: 349\n",
      "2025-03-05 17:53:43,004 - INFO -     Proof/Disproof Triggers:\n",
      "2025-03-05 17:53:43,005 - INFO -       Slight/None Proof Keywords: 718\n",
      "2025-03-05 17:53:43,005 - INFO -       Slight/None Disproof Keywords: 946\n",
      "2025-03-05 17:53:43,006 - INFO -       Moderate Proof Keywords: 433\n",
      "2025-03-05 17:53:43,007 - INFO -       Moderate Disproof Keywords: 2146\n",
      "2025-03-05 17:53:43,008 - INFO -       Persistent Systematic Proof Keywords: 1698\n",
      "2025-03-05 17:53:43,008 - INFO -       Persistent Systematic Disproof Keywords: 1768\n",
      "2025-03-05 17:53:43,010 - INFO -       Heavy Amplification Proof Keywords: 48\n",
      "2025-03-05 17:53:43,010 - INFO -       Heavy Amplification Disproof Keywords: 3468\n",
      "2025-03-05 17:53:43,011 - INFO -     Slight/None Specific Proof/Disproof:\n",
      "2025-03-05 17:53:43,012 - INFO -       Proof Phrase 'used for minor duration or risk tweaks': 0\n",
      "2025-03-05 17:53:43,013 - INFO -       Proof Phrase 'occasional use for limited exposure adjustments': 0\n",
      "2025-03-05 17:53:43,013 - INFO -       Proof Phrase 'used for position adjustments on a case-by-case basis': 0\n",
      "2025-03-05 17:53:43,014 - INFO -       Proof Phrase 'applied sparingly to fine-tune risk': 0\n",
      "2025-03-05 17:53:43,015 - INFO -       Proof Phrase 'used on an ad hoc basis for hedging': 0\n",
      "2025-03-05 17:53:43,016 - INFO -       Proof Phrase 'employed occasionally for cash management': 0\n",
      "2025-03-05 17:53:43,018 - INFO -       Proof Phrase 'derivatives used optionally for risk': 0\n",
      "2025-03-05 17:53:43,018 - INFO -       Proof Phrase 'can utilize swaps for adjustments': 0\n",
      "2025-03-05 17:53:43,020 - INFO -       Proof Phrase 'can use futures to track index': 0\n",
      "2025-03-05 17:53:43,021 - INFO -       Proof Phrase 'can employ derivatives occasionally': 0\n",
      "2025-03-05 17:53:43,021 - INFO -       Proof Phrase 'derivatives used discretionarily': 0\n",
      "2025-03-05 17:53:43,023 - INFO -       Proof Phrase 'derivatives permitted for limited purposes': 0\n",
      "2025-03-05 17:53:43,023 - INFO -       Proof Phrase 'may invest in derivatives sparingly': 0\n",
      "2025-03-05 17:53:43,025 - INFO -       Proof Phrase 'may employ futures for cash flow': 0\n",
      "2025-03-05 17:53:43,025 - INFO -       Proof Phrase 'uses derivatives to adjust exposure': 0\n",
      "2025-03-05 17:53:43,026 - INFO -       Proof Phrase 'utilizes futures contracts to equitize cash': 0\n",
      "2025-03-05 17:53:43,027 - INFO -       Proof Phrase 'will not use it to increase leveraged exposure': 0\n",
      "2025-03-05 17:53:43,028 - INFO -       Proof Phrase 'use of derivatives is permitted within limits': 0\n",
      "2025-03-05 17:53:43,028 - INFO -       Proof Phrase 'may use derivatives': 0\n",
      "2025-03-05 17:53:43,029 - INFO -       Proof Phrase 'may invest in derivatives': 0\n",
      "2025-03-05 17:53:43,030 - INFO -       Proof Phrase 'derivatives only to mitigate': 0\n",
      "2025-03-05 17:53:43,031 - INFO -       Proof Phrase 'may utilize derivatives for managing duration, sector exposure, yield curve and risk mitigation': 0\n",
      "2025-03-05 17:53:43,031 - INFO -       Proof Phrase 'may utilize derivatives for managing duration': 0\n",
      "2025-03-05 17:53:43,032 - INFO -       Proof Phrase 'may invest in derivatives, including foreign currency derivatives': 0\n",
      "2025-03-05 17:53:43,033 - INFO -       Proof Phrase 'may also invest in futures contracts and options to manage market exposure': 0\n",
      "2025-03-05 17:53:43,034 - INFO -       Proof Phrase 'may use derivatives to leverage exposure or manage cash': 0\n",
      "2025-03-05 17:53:43,034 - INFO -       Proof Phrase 'may also use derivatives to leverage or hedge exposure': 0\n",
      "2025-03-05 17:53:43,036 - INFO -       Proof Phrase 'may hedge foreign currency exposure through derivatives, although it is not required to do so': 0\n",
      "2025-03-05 17:53:43,037 - INFO -       Disproof Phrase 'employs currency forward contracts to hedge exposure': 0\n",
      "2025-03-05 17:53:43,038 - INFO -       Disproof Phrase 'derivatives are integral to its hedging strategy': 0\n",
      "2025-03-05 17:53:43,038 - INFO -       Disproof Phrase 'systematically uses derivatives': 0\n",
      "2025-03-05 17:53:43,039 - INFO -       Disproof Phrase 'uses a quantitative model to generate derivative signals': 0\n",
      "2025-03-05 17:53:43,039 - INFO -       Disproof Phrase 'systematic use of derivatives': 0\n",
      "2025-03-05 17:53:43,040 - INFO -       Disproof Phrase 'derivatives are central': 0\n",
      "2025-03-05 17:53:43,041 - INFO -       Disproof Phrase 'invests primarily in futures, call options, and put options': 0\n",
      "2025-03-05 17:53:43,042 - INFO -       Disproof Phrase 'hedges currency exposure with derivatives': 0\n",
      "2025-03-05 17:53:43,043 - INFO -       Disproof Phrase 'writes call options on index': 0\n",
      "2025-03-05 17:53:43,043 - INFO -       Disproof Phrase 'invests in futures to offset risk': 0\n",
      "2025-03-05 17:53:43,045 - INFO -       Disproof Phrase 'enters swap transactions for protection': 0\n",
      "2025-03-05 17:53:43,045 - INFO -       Disproof Phrase 'uses futures to enhance exposure': 0\n",
      "2025-03-05 17:53:43,046 - INFO -       Disproof Phrase 'invests in derivatives through subsidiary': 0\n",
      "2025-03-05 17:53:43,047 - INFO -       Disproof Phrase 'hedges interest rates with options': 0\n",
      "2025-03-05 17:53:43,048 - INFO -       Disproof Phrase 'allocates assets to options strategy': 0\n",
      "2025-03-05 17:53:43,048 - INFO -       Disproof Phrase 'employs leverage through inverse floaters': 0\n",
      "2025-03-05 17:53:43,050 - INFO -       Disproof Phrase 'employs options strategies regularly': 0\n",
      "2025-03-05 17:53:43,050 - INFO -       Disproof Phrase 'rolled according to a fixed schedule': 0\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\JulianHeron\\anaconda3\\Lib\\logging\\__init__.py\", line 1163, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Users\\JulianHeron\\anaconda3\\Lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\u2010' in position 64: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"C:\\Users\\JulianHeron\\anaconda3\\Lib\\runpy.py\", line 198, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\JulianHeron\\anaconda3\\Lib\\runpy.py\", line 88, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\JulianHeron\\Software Projects\\Database Administration\\env\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\JulianHeron\\Software Projects\\Database Administration\\env\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\JulianHeron\\Software Projects\\Database Administration\\env\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\JulianHeron\\Software Projects\\Database Administration\\env\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\JulianHeron\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\JulianHeron\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\JulianHeron\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\JulianHeron\\Software Projects\\Database Administration\\env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\JulianHeron\\Software Projects\\Database Administration\\env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\JulianHeron\\Software Projects\\Database Administration\\env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\JulianHeron\\Software Projects\\Database Administration\\env\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\JulianHeron\\Software Projects\\Database Administration\\env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\JulianHeron\\Software Projects\\Database Administration\\env\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\JulianHeron\\Software Projects\\Database Administration\\env\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\JulianHeron\\Software Projects\\Database Administration\\env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\JulianHeron\\Software Projects\\Database Administration\\env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\JulianHeron\\Software Projects\\Database Administration\\env\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\JulianHeron\\Software Projects\\Database Administration\\env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\JulianHeron\\Software Projects\\Database Administration\\env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\JulianHeron\\Software Projects\\Database Administration\\env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\JulianHeron\\AppData\\Local\\Temp\\ipykernel_3176\\1725770642.py\", line 724, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\JulianHeron\\AppData\\Local\\Temp\\ipykernel_3176\\1725770642.py\", line 705, in main\n",
      "    logging.info(f\"      Disproof Phrase '{phrase}': {stats_counter.get(f'disproof_slight_none_{phrase}', 0)}\")\n",
      "Message: \"      Disproof Phrase 'currency‐related derivatives to hedge': 0\"\n",
      "Arguments: ()\n",
      "2025-03-05 17:53:43,051 - INFO -       Disproof Phrase 'currency‐related derivatives to hedge': 0\n",
      "2025-03-05 17:53:43,052 - INFO -       Disproof Phrase 'may invest up to 15% of its total assets in credit default swaps': 0\n",
      "2025-03-05 17:53:43,053 - INFO -       Disproof Phrase 'applies an options collar strategy': 0\n",
      "2025-03-05 17:53:43,054 - INFO -       Disproof Phrase 'derivatives to hedge currency exposure': 0\n",
      "2025-03-05 17:53:43,055 - INFO -       Disproof Phrase 'writing covered calls': 0\n",
      "2025-03-05 17:53:43,056 - INFO -       Disproof Phrase 'selects put options through a laddered approach that rolls monthly': 0\n",
      "2025-03-05 17:53:43,056 - INFO -   Fallback Evaluation: 0\n",
      "2025-03-05 17:53:43,057 - INFO -     Total Questionable Classifications: 0\n",
      "2025-03-05 17:53:43,058 - INFO -     Validation Conflicts:\n",
      "2025-03-05 17:53:43,059 - INFO -       Long-Only Conflicts: 0\n",
      "2025-03-05 17:53:43,059 - INFO -       Defined Outcome Conflicts: 0\n",
      "2025-03-05 17:53:43,060 - INFO -       YC_Category Conflicts: 0\n",
      "2025-03-05 17:53:43,061 - INFO -       YC_Global_Category Conflicts: 0\n",
      "2025-03-05 17:53:43,061 - INFO -     Fallback Classifications:\n",
      "2025-03-05 17:53:43,062 - INFO -       Slight/None: 0\n",
      "2025-03-05 17:53:43,062 - INFO -       Moderate: 0\n",
      "2025-03-05 17:53:43,063 - INFO -       Persistent Systematic: 0\n",
      "2025-03-05 17:53:43,063 - INFO -       Heavy Amplification: 0\n",
      "2025-03-05 17:53:43,064 - INFO -     Fallback Conservative Tiebreakers:\n",
      "2025-03-05 17:53:43,065 - INFO -       Slight/None: 0\n",
      "2025-03-05 17:53:43,066 - INFO -       Moderate: 0\n",
      "2025-03-05 17:53:43,067 - INFO -       Persistent Systematic: 0\n",
      "2025-03-05 17:53:43,067 - INFO -       Heavy Amplification: 0\n",
      "2025-03-05 17:53:43,068 - INFO - Exported classification results => C:\\Users\\JulianHeron\\Software Projects\\Test files\\Risk_Overlays_20250305_175340_No_UAC_FFM_DKM_DCM_UKS_plus_1_more.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "from collections import Counter\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "# Setup logging to both console and file with UTF-8 encoding\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"))\n",
    "try:\n",
    "    console_handler.stream.reconfigure(encoding='utf-8')\n",
    "except AttributeError:\n",
    "    # Fallback for environments where reconfigure isn't available\n",
    "    pass\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"classification.log\", encoding='utf-8'),\n",
    "        console_handler\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Configuration Flags\n",
    "CONFIG = {\n",
    "    # Auto-Classification Flags\n",
    "    \"use_auto_classification\": False,          # UAC\n",
    "    \"use_fund_family_mapping\": False,          # FFM\n",
    "    \"use_direct_keyword_mapping\": False,       # DKM\n",
    "    \"use_direct_category_mapping\": False,      # DCM\n",
    "\n",
    "    # Exposure-Based Classification Flags\n",
    "    \"use_exposure_classification\": True,     # UEC\n",
    "    \"use_proof_disproof_all_tiers\": True,    # PDT\n",
    "    \"use_slight_none_proof_disproof\": True,  # SNP\n",
    "\n",
    "    # Validation and Fallback Flags\n",
    "    \"use_validation\": True,                  # UVL\n",
    "    \"use_fallback_evaluation\": True,         # UFE\n",
    "    \"use_fallback_scoring\": True,            # UFS\n",
    "    \"use_fallback_tiebreaker\": True,         # UFT\n",
    "\n",
    "    # Scoring and Decision Weights\n",
    "    \"use_keyword_scoring\": False,             # UKS\n",
    "    \"keyword_score_weight\": 1.0,              # KSW\n",
    "    \"category_score_weight\": 2.0,             # CSW\n",
    "\n",
    "    # Concurrency Settings\n",
    "    \"use_concurrency\": False,                 # UCN\n",
    "    \"max_workers\": 4,                         # MXW\n",
    "    \"concurrency_executor\": \"process\",        # CEX\n",
    "\n",
    "    # Detailed Logging\n",
    "    \"use_detailed_logging\": True              # UDL\n",
    "}\n",
    "\n",
    "# Connection string\n",
    "connection_string = (\n",
    "    \"mssql+pyodbc://JULIANS_LAPTOP\\\\SQLEXPRESS/\"\n",
    "    \"CWA_Fund_Database?driver=ODBC+Driver+18+for+SQL+Server\"\n",
    "    \"&trusted_connection=yes&TrustServerCertificate=yes\"\n",
    ")\n",
    "\n",
    "# Define keyword mappings for general scoring\n",
    "keyword_mappings = {\n",
    "    \"Slight/None\": [\n",
    "        \"long-only\", \"no derivatives\", \"no hedging\", \"no leverage\", \"no options\", \"no short\",\n",
    "        \"position adjustment\", \"occasional hedging\", \"covered call\", \"put-write\", \"light hedge\",\n",
    "        \"may include options\", \"limited use of derivatives\", \"for risk management purposes\",\n",
    "        \"minor hedging\", \"occasional short positions\", \"overwrite\", \"investment grade\", \"core\"\n",
    "    ],\n",
    "    \"Moderate\": [\n",
    "        \"hedged\", \"currency hedge\", \"protective put\", \"partial hedge\", \"hedged equity\",\n",
    "        \"covered call\", \"convexity option overlay\", \"option overlay\", \"put/spread collar\",\n",
    "        \"forward agreement\", \"enhanced index strategy\", \"BuyWrite\", \"Buy-Write\", \"buy write\",\n",
    "        \"option spread\", \"volatility hedge\", \"put options\", \"enhance\", \"options-based income\",\n",
    "        \"ELN\", \"premium income\", \"call option\", \"FLEX options\", \"option premium\", \"write calls\",\n",
    "        \"sell calls\", \"protective puts\", \"equity-linked notes\", \"structured notes\",\n",
    "        \"risk mitigation\", \"downside protection\", \"limited hedging\", \"multi-asset\"\n",
    "    ],\n",
    "    \"Persistent Systematic\": [\n",
    "        \"tail-risk\", \"trend-following\", \"systematic hedging\", \"overlay\", \"CTA\", \"managed futures\",\n",
    "        \"defined outcome\", \"long-short\", \"market neutral\", \"systematic strategy\", \"return stacking\",\n",
    "        \"option writing\", \"straddle\", \"derivative income\", \"futures contracts\", \"swap contract\",\n",
    "        \"forward agreement\", \"enhanced index strategy\", \"volatility hedge\", \"put options\",\n",
    "        \"options-based income\", \"ELN\", \"option premium\", \"swap\", \"forward\", \"futures\", \"future\",\n",
    "        \"VIX\", \"managed futures strategy\", \"trend strategy\", \"quantitative hedging\",\n",
    "        \"systematic options\", \"options overlay strategy\", \"futures overlay\", \"swaps-based\",\n",
    "        \"multi-asset\", \"Flex Options\", \"Flexible Exchange Options\", \"YieldMax\", \"buffer\"\n",
    "    ],\n",
    "    \"Heavy Amplification\": [\n",
    "        \"2x\", \"3x\", \"Uncapped Accelerator\", \"-2x\", \"-3x\", \"YieldMax\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Direct mapping keywords (often in ProductName)\n",
    "direct_keyword_mappings = {\n",
    "    \"Persistent Systematic\": [\"Market Neutral\", \"managed futures\", \"Premia\", \"Return Stacked ETFs\"]\n",
    "}\n",
    "\n",
    "# Direct mapping categories\n",
    "direct_category_mappings = {\n",
    "    \"Persistent Systematic\": {\n",
    "        \"YC_Category\": [\"Defined Outcome\"],\n",
    "        \"CWA_Broad_Category\": [\"Defined Outcome\"],\n",
    "        \"YC_Global_Category\": [\"market neutral\"]\n",
    "    },\n",
    "    \"Heavy Amplification\": {\n",
    "        \"YC_Category\": [\n",
    "            \"Trading--Leveraged Equity\", \"Trading--Leveraged Debt\", \"Trading--Leveraged Commodities\"\n",
    "        ],\n",
    "        \"CWA_Broad_Category\": [\"Single Stock\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Helper category mappings (narrow possibilities)\n",
    "helper_category_mappings = {\n",
    "    (\"Persistent Systematic\", \"Heavy Amplification\"): {\n",
    "        \"YC_Category\": [\n",
    "            \"Trading--Inverse Commodities\", \"Trading--Inverse Debt\", \"Trading--Inverse Equity\",\n",
    "            \"Trading--Miscellaneous\"\n",
    "        ],\n",
    "        \"CWA_Broad_Category\": [\"Trading/Tactical\"],\n",
    "        \"YC_Global_Category\": [\"Trading Tools\"]\n",
    "    },\n",
    "    (\"Persistent Systematic\", \"Moderate\"): {\n",
    "        \"YC_Global_Category\": [\"Multialternative\", \"Long/Short Equity\"],\n",
    "        \"YC_Category\": [\"Equity Hedged\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Weak helper category mappings (may or may not have overlay)\n",
    "weak_helper_category_mappings = {\n",
    "    (\"Slight/None\", \"Moderate\"): {\n",
    "        \"YC_Global_Category\": [\"Flexible Allocation\", \"Alternative Miscellaneous\"],\n",
    "        \"return_driver\": [\"Index Based\", \"Factor/Smart Beta\"]\n",
    "    },\n",
    "    (\"Slight/None\", \"Moderate\", \"Persistent Systematic\"): {\n",
    "        \"YC_Category\": [\"Relative Value Arbitrage\"]\n",
    "    },\n",
    "    (\"Heavy Amplification\", \"Persistent Systematic\", \"Moderate\", \"Slight/None\"): {\n",
    "        \"return_driver\": [\"Quant/Systematic\"]\n",
    "    },\n",
    "    (\"Persistent Systematic\", \"Moderate\", \"Slight/None\"): {\n",
    "        \"return_driver\": [\"Active Discretionary\", \"Multi-Strategy\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Proof/Disproof definitions for Tier 1 (Slight/None)\n",
    "slight_proof_phrases = [\n",
    "    \"used for minor duration or risk tweaks\",\n",
    "    \"occasional use for limited exposure adjustments\",\n",
    "    \"used for position adjustments on a case-by-case basis\",\n",
    "    \"applied sparingly to fine-tune risk\",\n",
    "    \"used on an ad hoc basis for hedging\",\n",
    "    \"employed occasionally for cash management\",\n",
    "    \"derivatives used optionally for risk\",\n",
    "    \"can utilize swaps for adjustments\",\n",
    "    \"can use futures to track index\",\n",
    "    \"can employ derivatives occasionally\",\n",
    "    \"derivatives used discretionarily\",\n",
    "    \"derivatives permitted for limited purposes\",\n",
    "    \"may invest in derivatives sparingly\",\n",
    "    \"may employ futures for cash flow\",\n",
    "    \"uses derivatives to adjust exposure\",\n",
    "    \"utilizes futures contracts to equitize cash\",\n",
    "    \"will not use it to increase leveraged exposure\",\n",
    "    \"use of derivatives is permitted within limits\",\n",
    "    \"may use derivatives\",\n",
    "    \"may invest in derivatives\",\n",
    "    \"derivatives only to mitigate\",\n",
    "    \"may utilize derivatives for managing duration, sector exposure, yield curve and risk mitigation\",\n",
    "    \"may utilize derivatives for managing duration\",\n",
    "    \"may invest in derivatives, including foreign currency derivatives\",\n",
    "    \"may also invest in futures contracts and options to manage market exposure\",\n",
    "    \"may use derivatives to leverage exposure or manage cash\",\n",
    "    \"may also use derivatives to leverage or hedge exposure\",\n",
    "    \"may hedge foreign currency exposure through derivatives, although it is not required to do so\"\n",
    "]\n",
    "\n",
    "slight_disproof_phrases = [\n",
    "    \"employs currency forward contracts to hedge exposure\",\n",
    "    \"derivatives are integral to its hedging strategy\",\n",
    "    \"systematically uses derivatives\",\n",
    "    \"uses a quantitative model to generate derivative signals\",\n",
    "    \"systematic use of derivatives\",\n",
    "    \"derivatives are central\",\n",
    "    \"invests primarily in futures, call options, and put options\",\n",
    "    \"hedges currency exposure with derivatives\",\n",
    "    \"writes call options on index\",\n",
    "    \"invests in futures to offset risk\",\n",
    "    \"enters swap transactions for protection\",\n",
    "    \"uses futures to enhance exposure\",\n",
    "    \"invests in derivatives through subsidiary\",\n",
    "    \"hedges interest rates with options\",\n",
    "    \"allocates assets to options strategy\",\n",
    "    \"employs leverage through inverse floaters\",\n",
    "    \"employs options strategies regularly\",\n",
    "    \"rolled according to a fixed schedule\",\n",
    "    \"currency‐related derivatives to hedge\",\n",
    "    \"may invest up to 15% of its total assets in credit default swaps\",\n",
    "    \"applies an options collar strategy\",\n",
    "    \"derivatives to hedge currency exposure\",\n",
    "    \"writing covered calls\",\n",
    "    \"selects put options through a laddered approach that rolls monthly\"\n",
    "]\n",
    "\n",
    "# Placeholders for proof/disproof phrases for other tiers\n",
    "moderate_proof_phrases = []\n",
    "moderate_disproof_phrases = []\n",
    "\n",
    "persistent_systematic_proof_phrases = []\n",
    "persistent_systematic_disproof_phrases = []\n",
    "\n",
    "heavy_amplification_proof_phrases = []\n",
    "heavy_amplification_disproof_phrases = []\n",
    "\n",
    "# Update proof_disproof_keywords to incorporate keywords and phrases for all tiers\n",
    "proof_disproof_keywords = {\n",
    "    \"Slight/None\": {\n",
    "        \"proof\": keyword_mappings[\"Slight/None\"] + slight_proof_phrases,\n",
    "        \"disproof\": (\n",
    "            keyword_mappings[\"Moderate\"] +\n",
    "            keyword_mappings[\"Persistent Systematic\"] +\n",
    "            keyword_mappings[\"Heavy Amplification\"] +\n",
    "            slight_disproof_phrases\n",
    "        )\n",
    "    },\n",
    "    \"Moderate\": {\n",
    "        \"proof\": keyword_mappings[\"Moderate\"] + moderate_proof_phrases,\n",
    "        \"disproof\": (\n",
    "            keyword_mappings[\"Slight/None\"] +\n",
    "            keyword_mappings[\"Persistent Systematic\"] +\n",
    "            keyword_mappings[\"Heavy Amplification\"] +\n",
    "            moderate_disproof_phrases\n",
    "        )\n",
    "    },\n",
    "    \"Persistent Systematic\": {\n",
    "        \"proof\": keyword_mappings[\"Persistent Systematic\"] + persistent_systematic_proof_phrases,\n",
    "        \"disproof\": (\n",
    "            keyword_mappings[\"Slight/None\"] +\n",
    "            keyword_mappings[\"Moderate\"] +\n",
    "            keyword_mappings[\"Heavy Amplification\"] +\n",
    "            persistent_systematic_disproof_phrases\n",
    "        )\n",
    "    },\n",
    "    \"Heavy Amplification\": {\n",
    "        \"proof\": keyword_mappings[\"Heavy Amplification\"] + heavy_amplification_proof_phrases,\n",
    "        \"disproof\": (\n",
    "            keyword_mappings[\"Slight/None\"] +\n",
    "            keyword_mappings[\"Moderate\"] +\n",
    "            keyword_mappings[\"Persistent Systematic\"] +\n",
    "            heavy_amplification_disproof_phrases\n",
    "        )\n",
    "    }\n",
    "}\n",
    "\n",
    "# Utility Functions\n",
    "def safe_lower(value):\n",
    "    return value.lower() if isinstance(value, str) else \"\"\n",
    "\n",
    "def search_keywords(text, keywords):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    text = str(text).lower()\n",
    "    return sum(1 for kw in keywords if re.search(r'\\b' + re.escape(kw.lower()) + r'\\b', text))\n",
    "\n",
    "def sanitize_excel_text(value):\n",
    "    if pd.isna(value):\n",
    "        return value\n",
    "    text = str(value).replace(\"<\", \"[lt]\").replace(\">\", \"[gt]\")\n",
    "    if text.startswith((\"=\", \"+\", \"-\", \"@\")):\n",
    "        text = \"'\" + text\n",
    "    return text\n",
    "\n",
    "# Function to generate output filename based on flags\n",
    "def generate_output_filename(base_path):\n",
    "    # Define shorthand abbreviations for each flag (same as in CONFIG comments)\n",
    "    shorthand_map = {\n",
    "        \"use_auto_classification\": \"UAC\",\n",
    "        \"use_fund_family_mapping\": \"FFM\",\n",
    "        \"use_direct_keyword_mapping\": \"DKM\",\n",
    "        \"use_direct_category_mapping\": \"DCM\",\n",
    "        \"use_exposure_classification\": \"UEC\",\n",
    "        \"use_proof_disproof_all_tiers\": \"PDT\",\n",
    "        \"use_slight_none_proof_disproof\": \"SNP\",\n",
    "        \"use_validation\": \"UVL\",\n",
    "        \"use_fallback_evaluation\": \"UFE\",\n",
    "        \"use_fallback_scoring\": \"UFS\",\n",
    "        \"use_fallback_tiebreaker\": \"UFT\",\n",
    "        \"use_keyword_scoring\": \"UKS\",\n",
    "        \"use_concurrency\": \"UCN\",\n",
    "        \"use_detailed_logging\": \"UDL\"\n",
    "    }\n",
    "\n",
    "    disabled_flags = [key for key, value in CONFIG.items() if isinstance(value, bool) and not value]\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if disabled_flags:\n",
    "        # Use shorthand abbreviations for disabled flags\n",
    "        max_flags = 5  # Increased to 5 since shorthands are shorter\n",
    "        short_flags = disabled_flags[:max_flags]\n",
    "        # Map each flag to its shorthand abbreviation\n",
    "        flags_str = \"_\".join([shorthand_map.get(flag, flag.replace(\"use_\", \"\")) for flag in short_flags])\n",
    "        if len(disabled_flags) > max_flags:\n",
    "            flags_str += f\"_plus_{len(disabled_flags)-max_flags}_more\"\n",
    "        filename = f\"Risk_Overlays_{timestamp}_No_{flags_str}.xlsx\"\n",
    "        total_path = os.path.join(base_path, filename)\n",
    "        if len(total_path) > 200:  # Safe limit for Windows path length\n",
    "            existing_files = glob.glob(os.path.join(base_path, \"Risk_Overlays_*.xlsx\"))\n",
    "            increment = len(existing_files) + 1\n",
    "            filename = f\"Risk_Overlays_{timestamp}_Run_{increment}.xlsx\"\n",
    "    else:\n",
    "        existing_files = glob.glob(os.path.join(base_path, \"Risk_Overlays_*.xlsx\"))\n",
    "        increment = len(existing_files) + 1\n",
    "        filename = f\"Risk_Overlays_{timestamp}_Run_{increment}.xlsx\"\n",
    "    \n",
    "    return os.path.join(base_path, filename)\n",
    "\n",
    "# Step 1: Auto-Classification\n",
    "def auto_classify(row, audit_log, stats_counter):\n",
    "    if not CONFIG[\"use_auto_classification\"]:\n",
    "        audit_log.append(\"Auto-classification disabled\")\n",
    "        return None\n",
    "\n",
    "    if CONFIG[\"use_fund_family_mapping\"]:\n",
    "        if pd.notna(row['fund_family']) and \"return stacked etfs\" in row['fund_family'].lower():\n",
    "            audit_log.append(\"Auto-classified: 'Return Stacked ETFs' → Persistent Systematic\")\n",
    "            stats_counter[\"auto_classify_fund_family\"] += 1\n",
    "            return \"Persistent Systematic\"\n",
    "    \n",
    "    if CONFIG[\"use_direct_keyword_mapping\"]:\n",
    "        for category, keywords in direct_keyword_mappings.items():\n",
    "            for field in ['ProductName', 'investment_strategy', 'FS_insight']:\n",
    "                if pd.notna(row[field]) and search_keywords(row[field], keywords) > 0:\n",
    "                    audit_log.append(f\"Auto-classified: {category} via keyword in {field}\")\n",
    "                    stats_counter[f\"auto_classify_keyword_{category}\"] += 1\n",
    "                    return category\n",
    "    \n",
    "    if CONFIG[\"use_direct_category_mapping\"]:\n",
    "        for category, mappings in direct_category_mappings.items():\n",
    "            for db_field, values in mappings.items():\n",
    "                vals_lower = [v.lower() for v in values]\n",
    "                field_val = safe_lower(row.get(db_field, ''))\n",
    "                if field_val in vals_lower:\n",
    "                    audit_log.append(f\"Auto-classified: {category} via {db_field}={field_val}\")\n",
    "                    stats_counter[f\"auto_classify_category_{category}\"] += 1\n",
    "                    return category\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Step 2: Exposure-Based Classification with Proof/Disproof\n",
    "def tally_proof_disproof(row, tier_name, audit_log, stats_counter):\n",
    "    fields = [\"ProductName\", \"investment_strategy\", \"FS_insight\"]\n",
    "    \n",
    "    proof_phrases = {\n",
    "        \"Slight/None\": slight_proof_phrases,\n",
    "        \"Moderate\": moderate_proof_phrases,\n",
    "        \"Persistent Systematic\": persistent_systematic_proof_phrases,\n",
    "        \"Heavy Amplification\": heavy_amplification_proof_phrases\n",
    "    }.get(tier_name, [])\n",
    "    \n",
    "    disproof_phrases = {\n",
    "        \"Slight/None\": slight_disproof_phrases,\n",
    "        \"Moderate\": moderate_disproof_phrases,\n",
    "        \"Persistent Systematic\": persistent_systematic_disproof_phrases,\n",
    "        \"Heavy Amplification\": heavy_amplification_disproof_phrases\n",
    "    }.get(tier_name, [])\n",
    "    \n",
    "    proof_score = 0\n",
    "    disproof_score = 0\n",
    "    proof_found = []\n",
    "    disproof_found = []\n",
    "    \n",
    "    # Check specific proof/disproof phrases for the tier\n",
    "    for field in fields:\n",
    "        if pd.notna(row[field]):\n",
    "            text = row[field].lower()\n",
    "            for phrase in proof_phrases:\n",
    "                if phrase in text:\n",
    "                    proof_score += 1\n",
    "                    proof_found.append(phrase)\n",
    "                    stats_counter[f\"proof_{tier_name}_{phrase}\"] += 1\n",
    "            for phrase in disproof_phrases:\n",
    "                if phrase in text:\n",
    "                    disproof_score += 1\n",
    "                    disproof_found.append(phrase)\n",
    "                    stats_counter[f\"disproof_{tier_name}_{phrase}\"] += 1\n",
    "    \n",
    "    # Also check keyword-based proof/disproof\n",
    "    proof_keywords = proof_disproof_keywords[tier_name][\"proof\"]\n",
    "    disproof_keywords = proof_disproof_keywords[tier_name][\"disproof\"]\n",
    "    \n",
    "    for field in fields:\n",
    "        if pd.notna(row[field]):\n",
    "            text = str(row[field]).lower()\n",
    "            proof_found.extend([kw for kw in proof_keywords if re.search(r'\\b' + re.escape(kw.lower()) + r'\\b', text) and kw not in proof_found])\n",
    "            disproof_found.extend([kw for kw in disproof_keywords if re.search(r'\\b' + re.escape(kw.lower()) + r'\\b', text) and kw not in disproof_found])\n",
    "    \n",
    "    proof_score += len([kw for kw in proof_found if kw not in proof_phrases])\n",
    "    disproof_score += len([kw for kw in disproof_found if kw not in disproof_phrases])\n",
    "    \n",
    "    if proof_found:\n",
    "        stats_counter[f\"proof_{tier_name}_keywords\"] += len(proof_found)\n",
    "        audit_log.append(f\"{tier_name} Proof Keywords/Phrases Found: {proof_found}\")\n",
    "    if disproof_found:\n",
    "        stats_counter[f\"disproof_{tier_name}_keywords\"] += len(disproof_found)\n",
    "        audit_log.append(f\"{tier_name} Disproof Keywords/Phrases Found: {disproof_found}\")\n",
    "    \n",
    "    audit_log.append(f\"{tier_name} Proof Score={proof_score}, Disproof Score={disproof_score}\")\n",
    "    return proof_score, disproof_score\n",
    "\n",
    "def classify_by_exposures_with_disproof(row, audit_log, stats_counter):\n",
    "    if not CONFIG[\"use_exposure_classification\"]:\n",
    "        audit_log.append(\"Exposure-based classification disabled\")\n",
    "        return \"Unclassified\"\n",
    "\n",
    "    exposure_cols = ['cash_long', 'cash_short', 'stock_long', 'stock_short', 'bond_long', 'bond_short', 'other_long', 'other_short']\n",
    "    for col in exposure_cols:\n",
    "        val = pd.to_numeric(row[col], errors='coerce')\n",
    "        row[col] = 0 if pd.isna(val) else val\n",
    "\n",
    "    long_total = (row['cash_long'] + row['stock_long'] + row['bond_long'] + row['other_long']) * 100\n",
    "    short_total = (row['cash_short'] + row['stock_short'] + row['bond_short'] + row['other_short']) * 100\n",
    "    other_total = (row['other_long'] + row['other_short']) * 100\n",
    "\n",
    "    long_r = round(long_total, 4)\n",
    "    short_r = round(short_total, 4)\n",
    "    other_r = round(other_total, 4)\n",
    "    audit_log.append(f\"Exposures: long={long_r}%, short={short_r}%, other={other_r}%\")\n",
    "\n",
    "    tiers = [\n",
    "        (\"Slight/None\", lambda: abs(short_r) < 1 and abs(other_r) < 1 and long_r <= 100.2),\n",
    "        (\"Moderate\", lambda: short_r <= 10 and other_r <= 10),\n",
    "        (\"Persistent Systematic\", lambda: short_r <= 50 or other_r <= 50),\n",
    "        (\"Heavy Amplification\", lambda: True)\n",
    "    ]\n",
    "\n",
    "    best_classification = None\n",
    "    best_proof_score = -1\n",
    "\n",
    "    for tier_name, condition in tiers:\n",
    "        if condition():\n",
    "            audit_log.append(f\"Tier {tier_name} matches exposure criteria\")\n",
    "            if CONFIG[\"use_proof_disproof_all_tiers\"]:\n",
    "                proof_score, disproof_score = tally_proof_disproof(row, tier_name, audit_log, stats_counter)\n",
    "                if proof_score >= disproof_score and proof_score > best_proof_score:\n",
    "                    best_classification = tier_name\n",
    "                    best_proof_score = proof_score\n",
    "                    audit_log.append(f\"New best classification: {tier_name} with Proof={proof_score}, Disproof={disproof_score}\")\n",
    "            else:\n",
    "                return tier_name\n",
    "        else:\n",
    "            audit_log.append(f\"Tier {tier_name} does not match exposure criteria\")\n",
    "\n",
    "    if best_classification:\n",
    "        stats_counter[f\"exposure_classify_{best_classification}\"] += 1\n",
    "        return best_classification\n",
    "    else:\n",
    "        audit_log.append(\"No tier passed proof/disproof, defaulting to Heavy Amplification\")\n",
    "        stats_counter[\"exposure_classify_default_heavy\"] += 1\n",
    "        return \"Heavy Amplification\"\n",
    "\n",
    "# Step 3: Validation and Fallback Evaluation\n",
    "def validate_classification(row, classification, audit_log, stats_counter):\n",
    "    if not CONFIG[\"use_validation\"]:\n",
    "        return False\n",
    "\n",
    "    questionable = False\n",
    "    reasons = []\n",
    "\n",
    "    for cats, mappings in helper_mappings.items():\n",
    "        if classification not in cats:\n",
    "            continue\n",
    "        for field, expected_values in mappings.items():\n",
    "            vals_lower = [v.lower() for v in expected_values]\n",
    "            actual_val = safe_lower(row.get(field, ''))\n",
    "            if actual_val in vals_lower:\n",
    "                questionable = True\n",
    "                reason = f\"Conflicts with {field}={actual_val}\"\n",
    "                reasons.append(reason)\n",
    "                stats_counter[f\"validation_conflict_{field}\"] += 1\n",
    "\n",
    "    if classification == \"Persistent Systematic\" and \"long-only\" in safe_lower(row.get('investment_strategy', '')):\n",
    "        questionable = True\n",
    "        reasons.append(\"Persistent Systematic but 'long-only' found\")\n",
    "        stats_counter[\"validation_conflict_long_only\"] += 1\n",
    "    elif classification == \"Slight/None\" and safe_lower(row.get('YC_Category_Name', '')) == \"defined outcome\":\n",
    "        questionable = True\n",
    "        reasons.append(\"Slight/None but 'Defined Outcome' category\")\n",
    "        stats_counter[\"validation_conflict_defined_outcome\"] += 1\n",
    "\n",
    "    if questionable:\n",
    "        audit_log.append(f\"Questionable classification: {reasons}\")\n",
    "    return questionable\n",
    "\n",
    "def fallback_evaluation(row, audit_log, stats_counter):\n",
    "    if not CONFIG[\"use_fallback_evaluation\"]:\n",
    "        audit_log.append(\"Fallback evaluation disabled, defaulting to Slight/None\")\n",
    "        stats_counter[\"fallback_default_slight_none\"] += 1\n",
    "        return \"Slight/None\"\n",
    "\n",
    "    scores = {\"Slight/None\": 0, \"Moderate\": 0, \"Persistent Systematic\": 0, \"Heavy Amplification\": 0}\n",
    "    \n",
    "    if CONFIG[\"use_fallback_scoring\"]:\n",
    "        if CONFIG[\"use_keyword_scoring\"]:\n",
    "            for category, keywords in keyword_mappings.items():\n",
    "                for field in ['ProductName', 'investment_strategy', 'FS_insight']:\n",
    "                    count = search_keywords(row[field], keywords)\n",
    "                    scores[category] += count * CONFIG[\"keyword_score_weight\"]\n",
    "                    if count > 0:\n",
    "                        stats_counter[f\"fallback_keywords_{category}\"] += count\n",
    "        if row.get('YC_Category_Name', '').lower() in [\"government bond\", \"corporate bond\"]:\n",
    "            scores[\"Slight/None\"] += CONFIG[\"category_score_weight\"]\n",
    "            stats_counter[\"fallback_adjust_slight_none\"] += 1\n",
    "        if row.get('YC_Category_Name', '').lower() in [\"long-short equity\", \"equity hedged\"]:\n",
    "            scores[\"Persistent Systematic\"] += CONFIG[\"category_score_weight\"]\n",
    "            stats_counter[\"fallback_adjust_persistent_systematic\"] += 1\n",
    "\n",
    "    audit_log.append(f\"Fallback scores: {scores}\")\n",
    "    max_score = max(scores.values())\n",
    "    top_categories = [cat for cat, score in scores.items() if score == max_score]\n",
    "    \n",
    "    if len(top_categories) == 1:\n",
    "        stats_counter[f\"fallback_classify_{top_categories[0]}\"] += 1\n",
    "        return top_categories[0]\n",
    "    \n",
    "    if CONFIG[\"use_fallback_tiebreaker\"]:\n",
    "        conservative_order = [\"Slight/None\", \"Moderate\", \"Persistent Systematic\", \"Heavy Amplification\"]\n",
    "        final = min(top_categories, key=lambda x: conservative_order.index(x))\n",
    "        stats_counter[f\"fallback_conservative_{final}\"] += 1\n",
    "        audit_log.append(f\"Fallback conservative tiebreaker: {final}\")\n",
    "        return final\n",
    "    else:\n",
    "        final = top_categories[0]\n",
    "        stats_counter[f\"fallback_first_{final}\"] += 1\n",
    "        audit_log.append(f\"Fallback no tiebreaker, first choice: {final}\")\n",
    "        return final\n",
    "\n",
    "# Main Classification Function\n",
    "def classify_fund(row, stats_counter):\n",
    "    audit_log = []\n",
    "    method_used = None\n",
    "\n",
    "    classification = auto_classify(row, audit_log, stats_counter)\n",
    "    if classification:\n",
    "        method_used = \"Auto-Classification\"\n",
    "        audit_log.append(f\"Final: {classification} via {method_used}\")\n",
    "        return classification, method_used, audit_log\n",
    "\n",
    "    classification = classify_by_exposures_with_disproof(row, audit_log, stats_counter)\n",
    "    method_used = \"Exposure-Based\"\n",
    "\n",
    "    questionable = validate_classification(row, classification, audit_log, stats_counter)\n",
    "    if questionable:\n",
    "        audit_log.append(\"Validation flagged as questionable, moving to fallback\")\n",
    "        classification = fallback_evaluation(row, audit_log, stats_counter)\n",
    "        method_used = \"Fallback Evaluation\"\n",
    "\n",
    "    audit_log.append(f\"Final: {classification} via {method_used}\")\n",
    "    return classification, method_used, audit_log\n",
    "\n",
    "def process_row(row, stats_counter):\n",
    "    classification, method_used, audit_log = classify_fund(row, stats_counter)\n",
    "    return {\n",
    "        'classification': classification,\n",
    "        'method_used': method_used,\n",
    "        'audit_log': audit_log,\n",
    "        'row': row.to_dict()\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    engine = create_engine(connection_string)\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        f.SymbolCUSIP, f.ProductName, f.fund_family, f.investment_strategy, f.FS_insight,\n",
    "        f.index_fund, f.inverse_fund, f.leveraged_fund, f.synthetic_replication_fund,\n",
    "        f.fund_of_funds, f.ycharts_url, f.currency_hedged_fund,\n",
    "        f.cash_long, f.cash_net, f.cash_short, f.stock_long, f.stock_net, f.stock_short,\n",
    "        f.bond_long, f.bond_net, f.bond_short, f.other_long, f.other_net, f.other_short,\n",
    "        f.return_driver, f.YC_BM_Symbol,\n",
    "        cwa.CWA_Broad_Category_Name,\n",
    "        yc.Category_Name AS YC_Category_Name,\n",
    "        ycg.Global_Category_Name,\n",
    "        ycba.YC_Broad_Asset_Class_Name\n",
    "    FROM Funds_to_Screen f\n",
    "    LEFT JOIN CWA_Broad_Category_List cwa ON f.CWA_Broad_Category_ID = cwa.ID\n",
    "    LEFT JOIN YC_Category_List yc ON f.YC_Category_ID = yc.ID\n",
    "    LEFT JOIN YC_Global_Category_List ycg ON f.YC_Global_Category_ID = ycg.ID\n",
    "    LEFT JOIN YC_Broad_Asset_Class_List ycba ON f.YC_Broad_Asset_Class_ID = ycba.ID\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        df = pd.read_sql(query, engine)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Database error: {e}\")\n",
    "        return\n",
    "    finally:\n",
    "        engine.dispose()\n",
    "\n",
    "    stats_counter = Counter()\n",
    "    classifications_counter = Counter()\n",
    "    questionable_counter = 0\n",
    "    results = []\n",
    "\n",
    "    if CONFIG[\"use_concurrency\"]:\n",
    "        with ProcessPoolExecutor(max_workers=CONFIG[\"max_workers\"]) as executor:\n",
    "            process_row_with_stats = partial(process_row, stats_counter=stats_counter)\n",
    "            futures = [executor.submit(process_row_with_stats, row) for _, row in df.iterrows()]\n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    classification = result['classification']\n",
    "                    method_used = result['method_used']\n",
    "                    audit_log = result['audit_log']\n",
    "                    row_dict = result['row']\n",
    "                    \n",
    "                    classifications_counter[classification] += 1\n",
    "                    if \"Fallback Evaluation\" in method_used:\n",
    "                        questionable_counter += 1\n",
    "                    new_row = pd.Series(row_dict)\n",
    "                    new_row['Final_Classification'] = classification\n",
    "                    new_row['Method_Used'] = method_used\n",
    "                    new_row['Audit_Log'] = \"; \".join(audit_log)\n",
    "                    results.append(new_row)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing row in ProcessPoolExecutor: {e}\")\n",
    "                    continue\n",
    "    else:\n",
    "        for idx, row in df.iterrows():\n",
    "            classification, method_used, audit_log = classify_fund(row, stats_counter)\n",
    "            classifications_counter[classification] += 1\n",
    "            if \"Fallback Evaluation\" in method_used:\n",
    "                questionable_counter += 1\n",
    "            new_row = row.copy()\n",
    "            new_row['Final_Classification'] = classification\n",
    "            new_row['Method_Used'] = method_used\n",
    "            new_row['Audit_Log'] = \"; \".join(audit_log)\n",
    "            results.append(new_row)\n",
    "\n",
    "    out_df = pd.DataFrame(results)\n",
    "    out_df['Audit_Log'] = out_df['Audit_Log'].apply(sanitize_excel_text)\n",
    "\n",
    "    columns_front = ['SymbolCUSIP', 'ProductName', 'fund_family', 'Final_Classification', 'Method_Used', 'ycharts_url']\n",
    "    other_cols = [c for c in out_df.columns if c not in columns_front]\n",
    "    out_df = out_df[columns_front + other_cols]\n",
    "\n",
    "    base_path = r\"C:\\Users\\JulianHeron\\Software Projects\\Test files\"\n",
    "    output_path = generate_output_filename(base_path)\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    out_df.to_excel(output_path, index=False)\n",
    "\n",
    "    # Logging section integrated here\n",
    "    if CONFIG.get(\"use_detailed_logging\", True):\n",
    "        logging.info(\"=== Detailed Classification Summary ===\")\n",
    "        logging.info(f\"Total Funds Processed: {len(out_df)}\")\n",
    "        \n",
    "        logging.info(\"\\nDistribution of Classifications:\")\n",
    "        for cat, count in classifications_counter.items():\n",
    "            logging.info(f\"  {cat}: {count} ({count/len(out_df)*100:.2f}%)\")\n",
    "        \n",
    "        logging.info(\"\\nClassification Methods Breakdown:\")\n",
    "        methods = [\"Auto-Classification\", \"Exposure-Based\", \"Fallback Evaluation\"]\n",
    "        for method in methods:\n",
    "            if method == \"Auto-Classification\":\n",
    "                # Sum sub-method counts for top-level Auto-Classification\n",
    "                auto_count = (\n",
    "                    stats_counter.get(\"auto_classify_fund_family\", 0) +\n",
    "                    sum(stats_counter.get(f\"auto_classify_keyword_{cat}\", 0) for cat in keyword_mappings.keys()) +\n",
    "                    sum(stats_counter.get(f\"auto_classify_category_{cat}\", 0) for cat in direct_category_mappings.keys())\n",
    "                )\n",
    "                logging.info(f\"  {method}: {auto_count}\")\n",
    "                logging.info(\"    By Fund Family:\")\n",
    "                logging.info(f\"      Return Stacked ETFs: {stats_counter.get('auto_classify_fund_family', 0)}\")\n",
    "                logging.info(\"    By Keywords:\")\n",
    "                for cat in keyword_mappings.keys():\n",
    "                    logging.info(f\"      {cat}: {stats_counter.get(f'auto_classify_keyword_{cat}', 0)}\")\n",
    "                logging.info(\"    By Category:\")\n",
    "                for cat in direct_category_mappings.keys():\n",
    "                    logging.info(f\"      {cat}: {stats_counter.get(f'auto_classify_category_{cat}', 0)}\")\n",
    "            elif method == \"Exposure-Based\":\n",
    "                # Sum sub-method counts for top-level Exposure-Based\n",
    "                exposure_count = sum(\n",
    "                    stats_counter.get(f\"exposure_classify_{tier}\", 0) for tier in keyword_mappings.keys()\n",
    "                ) + stats_counter.get(\"exposure_classify_default_heavy\", 0)\n",
    "                logging.info(f\"  {method}: {exposure_count}\")\n",
    "                logging.info(\"    By Tier:\")\n",
    "                for tier in keyword_mappings.keys():\n",
    "                    logging.info(f\"      {tier}: {stats_counter.get(f'exposure_classify_{tier}', 0)}\")\n",
    "                logging.info(f\"    Default to Heavy Amplification: {stats_counter.get('exposure_classify_default_heavy', 0)}\")\n",
    "                logging.info(\"    Proof/Disproof Triggers:\")\n",
    "                for tier in keyword_mappings.keys():\n",
    "                    logging.info(f\"      {tier} Proof Keywords: {stats_counter.get(f'proof_{tier}_keywords', 0)}\")\n",
    "                    logging.info(f\"      {tier} Disproof Keywords: {stats_counter.get(f'disproof_{tier}_keywords', 0)}\")\n",
    "                if CONFIG[\"use_slight_none_proof_disproof\"]:\n",
    "                    logging.info(\"    Slight/None Specific Proof/Disproof:\")\n",
    "                    for phrase in slight_proof_phrases:\n",
    "                        logging.info(f\"      Proof Phrase '{phrase}': {stats_counter.get(f'proof_slight_none_{phrase}', 0)}\")\n",
    "                    for phrase in slight_disproof_phrases:\n",
    "                        logging.info(f\"      Disproof Phrase '{phrase}': {stats_counter.get(f'disproof_slight_none_{phrase}', 0)}\")\n",
    "            elif method == \"Fallback Evaluation\":\n",
    "                logging.info(f\"  {method}: {sum(stats_counter.get(f'fallback_classify_{tier}', 0) for tier in keyword_mappings.keys())}\")\n",
    "                logging.info(f\"    Total Questionable Classifications: {questionable_counter}\")\n",
    "                logging.info(\"    Validation Conflicts:\")\n",
    "                logging.info(f\"      Long-Only Conflicts: {stats_counter.get('validation_conflict_long_only', 0)}\")\n",
    "                logging.info(f\"      Defined Outcome Conflicts: {stats_counter.get('validation_conflict_defined_outcome', 0)}\")\n",
    "                for field in ['YC_Category', 'YC_Global_Category']:\n",
    "                    logging.info(f\"      {field} Conflicts: {stats_counter.get(f'validation_conflict_{field}', 0)}\")\n",
    "                logging.info(\"    Fallback Classifications:\")\n",
    "                for tier in keyword_mappings.keys():\n",
    "                    logging.info(f\"      {tier}: {stats_counter.get(f'fallback_classify_{tier}', 0)}\")\n",
    "                logging.info(\"    Fallback Conservative Tiebreakers:\")\n",
    "                for tier in keyword_mappings.keys():\n",
    "                    logging.info(f\"      {tier}: {stats_counter.get(f'fallback_conservative_{tier}', 0)}\")\n",
    "\n",
    "    logging.info(f\"Exported classification results => {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd485075-30dc-4524-9072-3e0724fd3e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exposures Code with overlay.. replacing risk drivers\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def safe_lower(text):\n",
    "    return str(text).lower() if pd.notna(text) else \"\"\n",
    "\n",
    "def classify_by_exposures_with_disproof(row, audit_log, stats_counter):\n",
    "    if not CONFIG[\"use_exposure_classification\"]:\n",
    "        return \"Unclassified Leverage\", \"No Overlay\", \"No Risk Management\", \"Unclassified Purpose\"\n",
    "\n",
    "    exposure_cols = ['cash_long', 'cash_short', 'stock_long', 'stock_short', 'bond_long', 'bond_short', 'other_long', 'other_short']\n",
    "    for col in exposure_cols:\n",
    "        val = pd.to_numeric(row[col], errors='coerce')\n",
    "        row[col] = 0 if pd.isna(val) else val\n",
    "\n",
    "    # Step 1: Total known positions\n",
    "    total_long = (row['cash_long'] + row['stock_long'] + row['bond_long'] + row['other_long']) * 100\n",
    "    total_short = (row['cash_short'] + row['stock_short'] + row['bond_short'] + row['other_short']) * 100\n",
    "    other_total = (row['other_long'] + row['other_short']) * 100\n",
    "\n",
    "    # Step 2: Adjust bond positions to balance to 100%\n",
    "    long_adjustment = 100 - total_long if total_long < 100 else max(0, total_long - 100)\n",
    "    short_adjustment = 100 - total_short if total_short < 100 else max(0, total_short - 100)\n",
    "    adjusted_bond_long = row['bond_long'] * 100 + long_adjustment if total_long < 100 else max(0, row['bond_long'] * 100 - long_adjustment)\n",
    "    adjusted_bond_short = row['bond_short'] * 100 + short_adjustment if total_short < 100 else max(0, row['bond_short'] * 100 - short_adjustment)\n",
    "    adjusted_total_long = (row['cash_long'] + row['stock_long'] + (adjusted_bond_long / 100) + row['other_long']) * 100\n",
    "    adjusted_total_short = (row['cash_short'] + row['stock_short'] + (adjusted_bond_short / 100) + row['other_short']) * 100\n",
    "\n",
    "    # Step 3: Calculate non-derivative long/short\n",
    "    non_deriv_long = (adjusted_total_long - other_total) / (1 - other_total/100) if other_total < 100 else 0\n",
    "    non_deriv_short = (adjusted_total_short - other_total) / (1 - other_total/100) if other_total < 100 else 0\n",
    "    net_non_deriv = non_deriv_long - non_deriv_short\n",
    "\n",
    "    # Step 4: Calculate leverage (excess over 100%)\n",
    "    net_exposure = (adjusted_total_long - adjusted_total_short)\n",
    "    derivative_leverage = 1.5 * other_total\n",
    "    leverage = max(0, net_exposure + derivative_leverage - 100)\n",
    "    other_r = round(other_total, 4)\n",
    "    audit_log.append(f\"Total Long: {round(total_long, 4)}%, Total Short: {round(total_short, 4)}%, Adjusted Net: {round(net_exposure, 4)}%, Derivative Leverage: {round(derivative_leverage, 4)}%, Total Leverage: {round(leverage, 4)}%\")\n",
    "\n",
    "    # Leverage magnitude tiers\n",
    "    if leverage < 5:\n",
    "        leverage_class = \"Slight\"\n",
    "    elif leverage <= 20:\n",
    "        leverage_class = \"Moderate\"\n",
    "    elif leverage <= 50:\n",
    "        leverage_class = \"Substantial\"\n",
    "    else:\n",
    "        leverage_class = \"Heavy\"\n",
    "\n",
    "    # Binary derivative overlay\n",
    "    overlay_keywords = [\"derivatives\", \"overlay\", \"options\", \"warrants\", \"swaps\", \"forwards\", \"collars\", \"futures\", \"hedges\"]\n",
    "    has_overlay = other_r > 0 and any(kw in safe_lower(row.get('investment_strategy', '')) for kw in overlay_keywords)\n",
    "\n",
    "    # Risk management overlay with chained permissive keywords\n",
    "    permissive_patterns = [rf\"(may use|can utilize|permitted to use)\\s+(derivatives|options|warrants|swaps|forwards|collars|futures|hedges)\"]\n",
    "    risk_keywords = [\"hedges\", \"risk mitigation\", \"downside protection\"]\n",
    "    has_permissive = any(re.search(pattern, safe_lower(row.get('investment_strategy', ''))) for pattern in permissive_patterns)\n",
    "    has_risk_specific = any(kw in safe_lower(row.get('investment_strategy', '')) for kw in risk_keywords)\n",
    "    risk_management = \"Yes\" if (has_overlay and (other_r >= 1 or has_risk_specific)) else \"No\"\n",
    "    if has_permissive and other_r < 1:\n",
    "        risk_management = \"No\"  # Incidental use\n",
    "\n",
    "    stats_counter[f\"exposure_leverage_{leverage_class}\"] += 1\n",
    "    stats_counter[f\"overlay_{'Yes' if has_overlay else 'No'}\"] += 1\n",
    "    stats_counter[f\"risk_management_{risk_management}\"] += 1\n",
    "    purpose = determine_purpose(row, audit_log, stats_counter)\n",
    "    audit_log.append(f\"Leverage: {leverage_class}, Overlay: {'Yes' if has_overlay else 'No'}, Risk Management: {risk_management}, Purpose: {purpose}\")\n",
    "    return leverage_class, \"Yes\" if has_overlay else \"No\", risk_management, purpose\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a76b49c1-4909-48b9-a552-be9048ccc90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data file 'your_fund_data.csv' not found. Please provide the correct path.\n"
     ]
    }
   ],
   "source": [
    "# Leverage exposure and overlay information\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize logging to console and file\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),  # Output to console\n",
    "        logging.FileHandler('classification_results.log')  # Save to file\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"use_exposure_classification\": True,\n",
    "    \"use_detailed_logging\": True\n",
    "}\n",
    "\n",
    "def safe_lower(text):\n",
    "    \"\"\"Utility to safely convert text to lowercase.\"\"\"\n",
    "    return str(text).lower() if pd.notna(text) else \"\"\n",
    "\n",
    "def auto_classify(row, audit_log, stats_counter):\n",
    "    \"\"\"Placeholder for auto-classification logic.\"\"\"\n",
    "    return None\n",
    "\n",
    "def determine_purpose(row, audit_log, stats_counter):\n",
    "    \"\"\"Classify the purpose of derivative use based on categories and keywords.\"\"\"\n",
    "    fields = [\"ProductName\", \"investment_strategy\", \"FS_insight\"]\n",
    "    purpose_scores = {\"Incidental\": 0, \"Overlay\": 0, \"Systematic\": 0, \"Amplification\": 0}\n",
    "\n",
    "    # Boolean and category checks\n",
    "    if row.get('index_fund', False) and not row.get('leveraged_fund', False):\n",
    "        purpose_scores[\"Incidental\"] += 2\n",
    "    if row.get('leveraged_fund', False) or \"2x\" in str(row.get('ProductName', '')).lower() or \"3x\" in str(row.get('ProductName', '')).lower():\n",
    "        purpose_scores[\"Amplification\"] += 2\n",
    "    if row.get('YC_Category_Name', '').lower() in [\"market neutral\", \"long-short equity\", \"managed futures\"]:\n",
    "        purpose_scores[\"Systematic\"] += 2\n",
    "    if row.get('YC_Category_Name', '').lower() in [\"option income\", \"covered call\"]:\n",
    "        purpose_scores[\"Overlay\"] += 2\n",
    "\n",
    "    # Keyword/phrase checks\n",
    "    incidental_phrases = [\"no derivatives or hedging strategies used\"]\n",
    "    overlay_phrases = [\"writes covered call options\", \"options overlay strategy\"]\n",
    "    systematic_phrases = [\"long and short positions\", \"managed futures strategy\"]\n",
    "    amplification_phrases = [\"targets 2x daily returns\"]\n",
    "\n",
    "    for field in fields:\n",
    "        if pd.notna(row[field]):\n",
    "            text = row[field].lower()\n",
    "            for phrase in incidental_phrases:\n",
    "                if phrase in text:\n",
    "                    purpose_scores[\"Incidental\"] += 1\n",
    "            for phrase in overlay_phrases:\n",
    "                if phrase in text:\n",
    "                    purpose_scores[\"Overlay\"] += 1\n",
    "            for phrase in systematic_phrases:\n",
    "                if phrase in text:\n",
    "                    purpose_scores[\"Systematic\"] += 1\n",
    "            for phrase in amplification_phrases:\n",
    "                if phrase in text:\n",
    "                    purpose_scores[\"Amplification\"] += 1\n",
    "\n",
    "    max_score = max(purpose_scores.values())\n",
    "    if max_score == 0:\n",
    "        return \"Unclassified Purpose\"\n",
    "    purpose = max(purpose_scores, key=purpose_scores.get)\n",
    "    stats_counter[f\"purpose_{purpose}\"] += 1\n",
    "    audit_log.append(f\"Purpose Scores: {purpose_scores}\")\n",
    "    return purpose\n",
    "\n",
    "def classify_by_exposures_with_disproof(row, audit_log, stats_counter):\n",
    "    \"\"\"Classify funds by leverage, derivative overlay, risk management, and purpose.\"\"\"\n",
    "    if not CONFIG[\"use_exposure_classification\"]:\n",
    "        return \"Unclassified Leverage\", \"No Overlay\", \"No Risk Management\", \"Unclassified Purpose\"\n",
    "\n",
    "    exposure_cols = ['cash_long', 'cash_short', 'stock_long', 'stock_short', 'bond_long', 'bond_short', 'other_long', 'other_short']\n",
    "    for col in exposure_cols:\n",
    "        val = pd.to_numeric(row[col], errors='coerce')\n",
    "        row[col] = 0 if pd.isna(val) else val\n",
    "\n",
    "    # Step 1: Total known positions\n",
    "    total_long = (row['cash_long'] + row['stock_long'] + row['bond_long'] + row['other_long']) * 100\n",
    "    total_short = (row['cash_short'] + row['stock_short'] + row['bond_short'] + row['other_short']) * 100\n",
    "    other_total = (row['other_long'] + row['other_short']) * 100\n",
    "\n",
    "    # Step 2: Adjust bond positions to balance to 100%\n",
    "    long_adjustment = 100 - total_long if total_long < 100 else max(0, total_long - 100)\n",
    "    short_adjustment = 100 - total_short if total_short < 100 else max(0, total_short - 100)\n",
    "    adjusted_bond_long = row['bond_long'] * 100 + long_adjustment if total_long < 100 else max(0, row['bond_long'] * 100 - long_adjustment)\n",
    "    adjusted_bond_short = row['bond_short'] * 100 + short_adjustment if total_short < 100 else max(0, row['bond_short'] * 100 - short_adjustment)\n",
    "    adjusted_total_long = (row['cash_long'] + row['stock_long'] + (adjusted_bond_long / 100) + row['other_long']) * 100\n",
    "    adjusted_total_short = (row['cash_short'] + row['stock_short'] + (adjusted_bond_short / 100) + row['other_short']) * 100\n",
    "\n",
    "    # Step 3: Calculate non-derivative long/short\n",
    "    non_deriv_long = (adjusted_total_long - other_total) / (1 - other_total/100) if other_total < 100 else 0\n",
    "    non_deriv_short = (adjusted_total_short - other_total) / (1 - other_total/100) if other_total < 100 else 0\n",
    "    net_non_deriv = non_deriv_long - non_deriv_short\n",
    "\n",
    "    # Step 4: Calculate leverage (excess over 100%)\n",
    "    net_exposure = (adjusted_total_long - adjusted_total_short)\n",
    "    derivative_leverage = 1.5 * other_total\n",
    "    leverage = max(0, net_exposure + derivative_leverage - 100)\n",
    "    other_r = round(other_total, 4)\n",
    "    audit_log.append(f\"Total Long: {round(total_long, 4)}%, Total Short: {round(total_short, 4)}%, Adjusted Net: {round(net_exposure, 4)}%, Derivative Leverage: {round(derivative_leverage, 4)}%, Total Leverage: {round(leverage, 4)}%\")\n",
    "\n",
    "    # Leverage magnitude tiers\n",
    "    if leverage < 5:\n",
    "        leverage_class = \"Slight\"\n",
    "    elif leverage <= 20:\n",
    "        leverage_class = \"Moderate\"\n",
    "    elif leverage <= 50:\n",
    "        leverage_class = \"Substantial\"\n",
    "    else:\n",
    "        leverage_class = \"Heavy\"\n",
    "\n",
    "    # Binary derivative overlay\n",
    "    overlay_keywords = [\"derivatives\", \"overlay\", \"options\", \"warrants\", \"swaps\", \"forwards\", \"collars\", \"futures\", \"hedges\"]\n",
    "    has_overlay = other_r > 0 and any(kw in safe_lower(row.get('investment_strategy', '')) for kw in overlay_keywords)\n",
    "\n",
    "    # Risk management overlay with chained permissive keywords\n",
    "    permissive_patterns = [rf\"(may use|can utilize|permitted to use)\\s+(derivatives|options|warrants|swaps|forwards|collars|futures|hedges)\"]\n",
    "    risk_keywords = [\"hedges\", \"risk mitigation\", \"downside protection\"]\n",
    "    has_permissive = any(re.search(pattern, safe_lower(row.get('investment_strategy', ''))) for pattern in permissive_patterns)\n",
    "    has_risk_specific = any(kw in safe_lower(row.get('investment_strategy', '')) for kw in risk_keywords)\n",
    "    risk_management = \"Yes\" if (has_overlay and (other_r >= 1 or has_risk_specific)) else \"No\"\n",
    "    if has_permissive and other_r < 1:\n",
    "        risk_management = \"No\"  # Incidental use\n",
    "\n",
    "    stats_counter[f\"exposure_leverage_{leverage_class}\"] += 1\n",
    "    stats_counter[f\"overlay_{'Yes' if has_overlay else 'No'}\"] += 1\n",
    "    stats_counter[f\"risk_management_{risk_management}\"] += 1\n",
    "    purpose = determine_purpose(row, audit_log, stats_counter)\n",
    "    audit_log.append(f\"Leverage: {leverage_class}, Overlay: {'Yes' if has_overlay else 'No'}, Risk Management: {risk_management}, Purpose: {purpose}\")\n",
    "    return leverage_class, \"Yes\" if has_overlay else \"No\", risk_management, purpose\n",
    "\n",
    "def classify_fund(row, stats_counter):\n",
    "    \"\"\"Integrate classification steps.\"\"\"\n",
    "    audit_log = []\n",
    "    method_used = None\n",
    "\n",
    "    classification = auto_classify(row, audit_log, stats_counter)\n",
    "    if classification:\n",
    "        method_used = \"Auto-Classification\"\n",
    "        leverage, overlay, risk_mgmt, purpose = classification, \"No Overlay\", \"No Risk Management\", \"Unclassified Purpose\"\n",
    "    else:\n",
    "        leverage, overlay, risk_mgmt, purpose = classify_by_exposures_with_disproof(row, audit_log, stats_counter)\n",
    "        method_used = \"Exposure-Based\"\n",
    "\n",
    "    audit_log.append(f\"Final: {leverage} {overlay} {risk_mgmt} {purpose} via {method_used}\")\n",
    "    return leverage, overlay, risk_mgmt, purpose, method_used, audit_log\n",
    "\n",
    "def sanitize_excel_text(text):\n",
    "    \"\"\"Sanitize text for Excel output.\"\"\"\n",
    "    return str(text).replace('\\r', '').replace('\\n', ' ')\n",
    "\n",
    "def main(df):\n",
    "    \"\"\"Main function to classify funds and log results.\"\"\"\n",
    "    stats_counter = defaultdict(int)\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            leverage, overlay, risk_mgmt, purpose, method_used, audit_log = classify_fund(row, stats_counter)\n",
    "            new_row = row.copy()\n",
    "            new_row['Leverage'] = leverage\n",
    "            new_row['Overlay'] = overlay\n",
    "            new_row['Risk_Management'] = risk_mgmt\n",
    "            new_row['Purpose'] = purpose\n",
    "            new_row['Method_Used'] = method_used\n",
    "            new_row['Audit_Log'] = \"; \".join(audit_log)\n",
    "            results.append(new_row)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing row {idx}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    out_df = pd.DataFrame(results)\n",
    "    out_df['Classification'] = out_df['Leverage'] + \" \" + out_df['Overlay'] + \" \" + out_df['Risk_Management'] + \" \" + out_df['Purpose']\n",
    "\n",
    "    if CONFIG.get(\"use_detailed_logging\", True):\n",
    "        logging.info(\"=== Detailed Classification Summary ===\")\n",
    "        logging.info(f\"Total Funds Processed: {len(out_df)}\")\n",
    "        logging.info(\"\\nLeverage Distribution:\")\n",
    "        for lev in [\"Slight\", \"Moderate\", \"Substantial\", \"Heavy\"]:\n",
    "            count = stats_counter.get(f\"exposure_leverage_{lev}\", 0)\n",
    "            logging.info(f\"  {lev}: {count} ({count/len(out_df)*100:.2f}%)\")\n",
    "        logging.info(\"\\nOverlay Distribution:\")\n",
    "        for ovr in [\"Yes\", \"No\"]:\n",
    "            count = stats_counter.get(f\"overlay_{ovr}\", 0)\n",
    "            logging.info(f\"  {ovr}: {count} ({count/len(out_df)*100:.2f}%)\")\n",
    "        logging.info(\"\\nRisk Management Distribution:\")\n",
    "        for rm in [\"Yes\", \"No\"]:\n",
    "            count = stats_counter.get(f\"risk_management_{rm}\", 0)\n",
    "            logging.info(f\"  {rm}: {count} ({count/len(out_df)*100:.2f}%)\")\n",
    "        logging.info(\"\\nPurpose Distribution:\")\n",
    "        for purp in [\"Incidental\", \"Overlay\", \"Systematic\", \"Amplification\"]:\n",
    "            count = stats_counter.get(f\"purpose_{purp}\", 0)\n",
    "            logging.info(f\"  {purp}: {count} ({count/len(out_df)*100:.2f}%)\")\n",
    "\n",
    "    # Save results to CSV\n",
    "    out_df.to_csv(\"classified_funds_output.csv\", index=False)\n",
    "    logging.info(\"Results saved to 'classified_funds_output.csv'\")\n",
    "    return out_df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your DataFrame (replace with your file path)\n",
    "    try:\n",
    "        df = pd.read_csv(\"your_fund_data.csv\")  # Adjust path as needed\n",
    "        out_df = main(df)\n",
    "    except FileNotFoundError:\n",
    "        logging.error(\"Data file 'your_fund_data.csv' not found. Please provide the correct path.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error running main: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1ba0ef2-901b-4fe8-946a-a59c12b9f0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 10:14:25,125 - INFO - Processed 5586 funds\n",
      "2025-03-10 10:14:25,127 - INFO - Leverage Categories:\n",
      "2025-03-10 10:14:25,127 - INFO -   Slight: 5093\n",
      "2025-03-10 10:14:25,129 - INFO -   Moderate: 186\n",
      "2025-03-10 10:14:25,130 - INFO -   Substantial: 82\n",
      "2025-03-10 10:14:25,130 - INFO -   Heavy: 225\n",
      "2025-03-10 10:14:25,131 - INFO - Overlay Categories:\n",
      "2025-03-10 10:14:25,131 - INFO -   Incidental/None: 5449\n",
      "2025-03-10 10:14:25,132 - INFO -   Strategic: 137\n",
      "2025-03-10 10:14:25,132 - INFO - Other Exposures Capped: Long=26, Short=8\n",
      "2025-03-10 10:14:25,133 - INFO - Results saved to C:\\Users\\JulianHeron\\Software Projects\\Test files\\exposures_overlays_20250310_101423_Run_1.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Leverage only test\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "from collections import Counter\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "class FundClassifier:\n",
    "    \"\"\"Class to handle fund leverage measurement and overlay strategy detection\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Configuration setup\n",
    "        self.config = {\n",
    "            \"use_concurrency\": False,\n",
    "            \"max_workers\": 4,\n",
    "            \"use_detailed_logging\": True,\n",
    "            \"derivatives_multiplier\": 1.5,  # Multiplier for 'other' (derivatives) exposure\n",
    "            \"other_threshold\": 0.02,  # 2% threshold for overlay strategy detection\n",
    "            \"other_cap\": 0.9  # Cap for 'other' in adjustment formula to avoid division issues\n",
    "        }\n",
    "        \n",
    "        # Database connection\n",
    "        self.connection_string = (\n",
    "            \"mssql+pyodbc://JULIANS_LAPTOP\\\\SQLEXPRESS/\"\n",
    "            \"CWA_Fund_Database?driver=ODBC+Driver+18+for+SQL+Server\"\n",
    "            \"&trusted_connection=yes&TrustServerCertificate=yes\"\n",
    "        )\n",
    "        \n",
    "        # Keyword mappings for overlay detection\n",
    "        self.derivative_keywords = [\"options\", \"futures\", \"forwards\", \"swaps\", \"warrants\"]\n",
    "        self.permissive_phrases = [\"may use\", \"can use\", \"might use\", \"occasionally uses\"]\n",
    "        \n",
    "        # Setup logging\n",
    "        self._setup_logging()\n",
    "\n",
    "    def _setup_logging(self):\n",
    "        \"\"\"Configure logging to console and file\"\"\"\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"))\n",
    "        try:\n",
    "            console_handler.stream.reconfigure(encoding='utf-8')\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            handlers=[\n",
    "                logging.FileHandler(\"exposures_overlays.log\", encoding='utf-8'),\n",
    "                console_handler\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _sanitize_excel_text(self, value) -> str:\n",
    "        \"\"\"Sanitize text for Excel output\"\"\"\n",
    "        if pd.isna(value):\n",
    "            return value\n",
    "        text = str(value).replace(\"<\", \"[lt]\").replace(\">\", \"[gt]\")\n",
    "        return f\"'{text}\" if text.startswith((\"=\", \"+\", \"-\", \"@\")) else text\n",
    "\n",
    "    def _generate_output_filename(self, base_path: str) -> str:\n",
    "        \"\"\"Generate unique output filename based on timestamp\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        increment = len(glob.glob(os.path.join(base_path, \"exposures_overlays_*.xlsx\"))) + 1\n",
    "        return os.path.join(base_path, f\"exposures_overlays_{timestamp}_Run_{increment}.xlsx\")\n",
    "\n",
    "    def _calculate_leverage(self, row: pd.Series, audit_log: List[str], stats: Counter) -> Tuple[float, float, float]:\n",
    "        \"\"\"\n",
    "        Calculate leverage based on exposures, with derivative adjustment\n",
    "        Returns: (adjusted_long, adjusted_short, total_leverage)\n",
    "        \"\"\"\n",
    "        # Define exposure columns\n",
    "        long_cols = ['cash_long', 'stock_long', 'bond_long', 'other_long']\n",
    "        short_cols = ['cash_short', 'stock_short', 'bond_short', 'other_short']\n",
    "        \n",
    "        # Convert to numeric and handle NaN\n",
    "        for col in long_cols + short_cols:\n",
    "            row[col] = pd.to_numeric(row.get(col, 0), errors='coerce') or 0\n",
    "\n",
    "        # Calculate raw totals\n",
    "        total_long = sum(row[col] for col in long_cols) * 100  # Convert to percentage\n",
    "        total_short = sum(row[col] for col in short_cols) * 100\n",
    "        other_long = row['other_long'] * 100\n",
    "        other_short = row['other_short'] * 100\n",
    "        \n",
    "        audit_log.append(f\"Raw Exposures: Total Long={total_long:.2f}%, Total Short={total_short:.2f}%, \"\n",
    "                        f\"Other Long={other_long:.2f}%, Other Short={other_short:.2f}%\")\n",
    "\n",
    "        # Cap 'other' for adjustment to avoid division issues\n",
    "        other_long_adj = min(other_long / 100, self.config[\"other_cap\"])  # Convert back to decimal for formula\n",
    "        other_short_adj = min(other_short / 100, self.config[\"other_cap\"])\n",
    "        \n",
    "        if other_long / 100 > self.config[\"other_cap\"]:\n",
    "            audit_log.append(f\"Capped other_long at {self.config['other_cap']*100}% for adjustment\")\n",
    "            stats[\"other_capped_long\"] += 1\n",
    "        if other_short / 100 > self.config[\"other_cap\"]:\n",
    "            audit_log.append(f\"Capped other_short at {self.config['other_cap']*100}% for adjustment\")\n",
    "            stats[\"other_capped_short\"] += 1\n",
    "\n",
    "        # Calculate adjusted non-derivative holdings\n",
    "        adjusted_long = ((total_long - other_long) / (1 - other_long_adj)) if other_long_adj < 1 else 0\n",
    "        adjusted_short = ((total_short - other_short) / (1 - other_short_adj)) if other_short_adj < 1 else 0\n",
    "        \n",
    "        # Calculate total leverage with derivative multiplier\n",
    "        derivative_impact = (other_long + other_short) * self.config[\"derivatives_multiplier\"]\n",
    "        total_leverage = (adjusted_long - adjusted_short) + derivative_impact\n",
    "        \n",
    "        audit_log.append(f\"Adjusted Long={adjusted_long:.2f}%, Adjusted Short={adjusted_short:.2f}%, \"\n",
    "                        f\"Derivative Impact={derivative_impact:.2f}%, Total Leverage={total_leverage:.2f}%\")\n",
    "        \n",
    "        stats[\"leverage_calculated\"] += 1\n",
    "        return adjusted_long, adjusted_short, total_leverage\n",
    "\n",
    "    def _categorize_leverage(self, total_leverage: float, audit_log: List[str], stats: Counter) -> str:\n",
    "        \"\"\"Categorize leverage based on deviation from 100%\"\"\"\n",
    "        deviation = abs(total_leverage - 100)\n",
    "        \n",
    "        if deviation < 5:\n",
    "            category = \"Slight\"\n",
    "        elif deviation < 15:\n",
    "            category = \"Moderate\"\n",
    "        elif deviation < 30:\n",
    "            category = \"Substantial\"\n",
    "        else:\n",
    "            category = \"Heavy\"\n",
    "            \n",
    "        audit_log.append(f\"Leverage Deviation={deviation:.2f}%, Category={category}\")\n",
    "        stats[f\"leverage_{category.lower()}\"] += 1\n",
    "        return category\n",
    "\n",
    "    def _detect_overlay(self, row: pd.Series, audit_log: List[str], stats: Counter) -> str:\n",
    "        \"\"\"Detect overlay strategy based on 'other' exposure and keywords\"\"\"\n",
    "        # Calculate total 'other' exposure\n",
    "        other_long = pd.to_numeric(row.get('other_long', 0), errors='coerce') or 0\n",
    "        other_short = pd.to_numeric(row.get('other_short', 0), errors='coerce') or 0\n",
    "        total_other = (other_long + other_short) * 100  # Convert to percentage\n",
    "        \n",
    "        # Search for keywords in text fields\n",
    "        text_fields = ['investment_strategy', 'ProductName', 'FS_insight']\n",
    "        text = \" \".join(str(row.get(field, '')).lower() for field in text_fields if pd.notna(row.get(field)))\n",
    "        \n",
    "        # Check for derivative keywords\n",
    "        has_keywords = False\n",
    "        has_permissive = False\n",
    "        found_keywords = []\n",
    "        found_permissive = []\n",
    "        \n",
    "        for keyword in self.derivative_keywords:\n",
    "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', text):\n",
    "                has_keywords = True\n",
    "                found_keywords.append(keyword)\n",
    "                # Check for permissive phrases preceding the keyword\n",
    "                for phrase in self.permissive_phrases:\n",
    "                    if re.search(r'\\b' + re.escape(phrase) + r'\\s+' + re.escape(keyword) + r'\\b', text):\n",
    "                        has_permissive = True\n",
    "                        found_permissive.append(f\"{phrase} {keyword}\")\n",
    "                        break\n",
    "        \n",
    "        audit_log.append(f\"Total Other Exposure={total_other:.2f}%, Keywords Found={found_keywords}, \"\n",
    "                        f\"Permissive Phrases={found_permissive}\")\n",
    "        \n",
    "        # Classify overlay\n",
    "        if total_other < self.config[\"other_threshold\"] * 100:\n",
    "            overlay = \"Incidental/None\"\n",
    "        elif total_other >= self.config[\"other_threshold\"] * 100:\n",
    "            if has_keywords and not has_permissive:\n",
    "                overlay = \"Strategic\"\n",
    "            else:\n",
    "                overlay = \"Incidental/None\"\n",
    "        else:\n",
    "            overlay = \"Incidental/None\"\n",
    "            \n",
    "        audit_log.append(f\"Overlay Category={overlay}\")\n",
    "        stats[f\"overlay_{overlay.lower().replace('/', '_')}\"] += 1\n",
    "        return overlay\n",
    "\n",
    "    def process_fund(self, row: pd.Series, stats: Counter) -> Dict:\n",
    "        \"\"\"Process a single fund row\"\"\"\n",
    "        audit_log = []\n",
    "        \n",
    "        # Calculate leverage\n",
    "        adjusted_long, adjusted_short, total_leverage = self._calculate_leverage(row, audit_log, stats)\n",
    "        leverage_category = self._categorize_leverage(total_leverage, audit_log, stats)\n",
    "        \n",
    "        # Detect overlay strategy\n",
    "        overlay_category = self._detect_overlay(row, audit_log, stats)\n",
    "        \n",
    "        return {\n",
    "            'adjusted_long': adjusted_long,\n",
    "            'adjusted_short': adjusted_short,\n",
    "            'total_leverage': total_leverage,\n",
    "            'leverage_category': leverage_category,\n",
    "            'overlay_category': overlay_category,\n",
    "            'audit_log': audit_log,\n",
    "            'row': row.to_dict()\n",
    "        }\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main execution method\"\"\"\n",
    "        engine = create_engine(self.connection_string)\n",
    "        query = \"\"\"\n",
    "        SELECT SymbolCUSIP, ProductName, fund_family, investment_strategy, FS_insight,\n",
    "               cash_long, cash_short, stock_long, stock_short,\n",
    "               bond_long, bond_short, other_long, other_short\n",
    "        FROM Funds_to_Screen\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            df = pd.read_sql(query, engine)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Database error: {e}\")\n",
    "            return\n",
    "        finally:\n",
    "            engine.dispose()\n",
    "\n",
    "        stats = Counter()\n",
    "        results = []\n",
    "\n",
    "        if self.config[\"use_concurrency\"]:\n",
    "            with ProcessPoolExecutor(max_workers=self.config[\"max_workers\"]) as executor:\n",
    "                futures = [executor.submit(self.process_fund, row, stats) for _, row in df.iterrows()]\n",
    "                results = [future.result() for future in as_completed(futures)]\n",
    "        else:\n",
    "            results = [self.process_fund(row, stats) for _, row in df.iterrows()]\n",
    "\n",
    "        # Process results\n",
    "        out_df = pd.DataFrame([r['row'] for r in results])\n",
    "        out_df['Adjusted_Long'] = [r['adjusted_long'] for r in results]\n",
    "        out_df['Adjusted_Short'] = [r['adjusted_short'] for r in results]\n",
    "        out_df['Total_Leverage'] = [r['total_leverage'] for r in results]\n",
    "        out_df['Leverage_Category'] = [r['leverage_category'] for r in results]\n",
    "        out_df['Overlay_Category'] = [r['overlay_category'] for r in results]\n",
    "        out_df['Audit_Log'] = [self._sanitize_excel_text(\"; \".join(r['audit_log'])) for r in results]\n",
    "\n",
    "        # Column ordering\n",
    "        columns_front = ['SymbolCUSIP', 'ProductName', 'fund_family', \n",
    "                        'Adjusted_Long', 'Adjusted_Short', 'Total_Leverage', \n",
    "                        'Leverage_Category', 'Overlay_Category']\n",
    "        other_cols = [c for c in out_df.columns if c not in columns_front]\n",
    "        out_df = out_df[columns_front + other_cols]\n",
    "\n",
    "        # Export\n",
    "        base_path = r\"C:\\Users\\JulianHeron\\Software Projects\\Test files\"\n",
    "        output_path = self._generate_output_filename(base_path)\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        out_df.to_excel(output_path, index=False)\n",
    "\n",
    "        # Logging\n",
    "        if self.config[\"use_detailed_logging\"]:\n",
    "            logging.info(f\"Processed {len(out_df)} funds\")\n",
    "            logging.info(\"Leverage Categories:\")\n",
    "            for cat in [\"Slight\", \"Moderate\", \"Substantial\", \"Heavy\"]:\n",
    "                logging.info(f\"  {cat}: {stats[f'leverage_{cat.lower()}']}\")\n",
    "            logging.info(\"Overlay Categories:\")\n",
    "            logging.info(f\"  Incidental/None: {stats['overlay_incidental_none']}\")\n",
    "            logging.info(f\"  Strategic: {stats['overlay_strategic']}\")\n",
    "            logging.info(f\"Other Exposures Capped: Long={stats['other_capped_long']}, Short={stats['other_capped_short']}\")\n",
    "            logging.info(f\"Results saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    classifier = FundClassifier()\n",
    "    classifier.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b77240d-e3b7-42e2-b714-d508257ab79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 16:04:19,537 - INFO - Database connection established successfully.\n",
      "2025-03-10 16:04:19,630 - INFO - Loaded 5586 rows from Funds_to_Screen table.\n",
      "2025-03-10 16:04:19,894 - ERROR - Error running main: 'NoneType' object has no attribute 'lower'\n"
     ]
    }
   ],
   "source": [
    "# new gross and net leverage from doge\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging to both console and file with UTF-8 encoding\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"))\n",
    "try:\n",
    "    console_handler.stream.reconfigure(encoding='utf-8')\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"classification.log\", encoding='utf-8'),\n",
    "        console_handler\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"use_detailed_logging\": True\n",
    "}\n",
    "\n",
    "# Database connection string\n",
    "connection_string = (\n",
    "    \"mssql+pyodbc://JULIANS_LAPTOP\\\\SQLEXPRESS/\"\n",
    "    \"CWA_Fund_Database?driver=ODBC+Driver+18+for+SQL+Server\"\n",
    "    \"&trusted_connection=yes&TrustServerCertificate=yes\"\n",
    ")\n",
    "\n",
    "# Create database connection\n",
    "def create_db_connection():\n",
    "    try:\n",
    "        engine = create_engine(connection_string)\n",
    "        logging.info(\"Database connection established successfully.\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error connecting to database: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Retrieve data from Funds_to_Screen table with category joins\n",
    "def load_fund_data():\n",
    "    engine = create_db_connection()\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            f.SymbolCUSIP, f.ProductName, f.fund_family, f.ycharts_url,\n",
    "            f.cash_long, f.cash_net, f.cash_short,\n",
    "            f.stock_long, f.stock_net, f.stock_short,\n",
    "            f.bond_long, f.bond_net, f.bond_short,\n",
    "            f.other_long, f.other_net, f.other_short,\n",
    "            f.preferred_long, f.preferred_net, f.preferred_short,\n",
    "            f.convertible_long, f.convertible_net, f.convertible_short,\n",
    "            yc.Category_Name AS YC_Category_Name\n",
    "        FROM Funds_to_Screen f\n",
    "        LEFT JOIN YC_Category_List yc ON f.YC_Category_ID = yc.ID\n",
    "        \"\"\"\n",
    "        df = pd.read_sql(query, engine)\n",
    "        logging.info(f\"Loaded {len(df)} rows from Funds_to_Screen table.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data from database: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        engine.dispose()\n",
    "\n",
    "# Classify leverage use and effect\n",
    "def classify_leverage(row, audit_log):\n",
    "    # Define exposure columns for each category\n",
    "    exposure_categories = {\n",
    "        'cash': ['cash_long', 'cash_short', 'cash_net'],\n",
    "        'stock': ['stock_long', 'stock_short', 'stock_net'],\n",
    "        'bond': ['bond_long', 'bond_short', 'bond_net'],\n",
    "        'other': ['other_long', 'other_short', 'other_net'],\n",
    "        'preferred': ['preferred_long', 'preferred_short', 'preferred_net'],\n",
    "        'convertible': ['convertible_long', 'convertible_short', 'convertible_net']\n",
    "    }\n",
    "\n",
    "    # Ensure all exposure columns are numeric and handle missing values\n",
    "    for category in exposure_categories:\n",
    "        for col in exposure_categories[category]:\n",
    "            val = pd.to_numeric(row.get(col, 0), errors='coerce')\n",
    "            row[col] = 0 if pd.isna(val) else val * 100  # Convert to percentage\n",
    "\n",
    "    # Calculate gross leverage for each category\n",
    "    gross_leverage = {}\n",
    "    for category in exposure_categories:\n",
    "        long_val = row[exposure_categories[category][0]]\n",
    "        short_val = row[exposure_categories[category][1]]\n",
    "        gross_leverage[category] = long_val + short_val\n",
    "\n",
    "    # Determine the dominant category (highest gross leverage)\n",
    "    dominant_category = max(gross_leverage, key=gross_leverage.get)\n",
    "    total_gross_leverage = gross_leverage[dominant_category]\n",
    "\n",
    "    # Classify Gross Leverage (Use)\n",
    "    if total_gross_leverage < 100:\n",
    "        leverage_use = \"Low\"\n",
    "    elif 100 <= total_gross_leverage <= 250:\n",
    "        leverage_use = \"Medium\"\n",
    "    else:\n",
    "        leverage_use = \"High\"\n",
    "\n",
    "    # Calculate Net Leverage (Effect) for the dominant category\n",
    "    net_leverage = row[exposure_categories[dominant_category][2]]\n",
    "\n",
    "    # Classify Net Leverage (Effect) with long-only adjustment\n",
    "    long_only_categories = [\"Taxable Fixed Income\", \"US Equity\", \"Global Bond-USD Hedged\"]\n",
    "    yc_category = row.get('YC_Category_Name', '').lower()\n",
    "    is_long_only = any(cat.lower() in yc_category for cat in long_only_categories) or dominant_category in ['bond', 'convertible']\n",
    "\n",
    "    if is_long_only and abs(net_leverage) < 10:\n",
    "        leverage_effect = \"Slight\"\n",
    "    else:\n",
    "        if abs(net_leverage) < 10:\n",
    "            leverage_effect = \"Slight\"\n",
    "        elif 10 <= abs(net_leverage) <= 50:\n",
    "            leverage_effect = \"Strategic\"\n",
    "        elif 50 < abs(net_leverage) <= 100:\n",
    "            leverage_effect = \"Systematic\"\n",
    "        else:\n",
    "            leverage_effect = \"Amplified\"\n",
    "\n",
    "    audit_log.append(\n",
    "        f\"Gross Leverage (Use): {round(total_gross_leverage, 4)}% (Dominant: {dominant_category}), \"\n",
    "        f\"Class: {leverage_use}, \"\n",
    "        f\"Net Leverage (Effect): {round(net_leverage, 4)}%, \"\n",
    "        f\"Class: {leverage_effect}\"\n",
    "    )\n",
    "\n",
    "    return leverage_use, leverage_effect\n",
    "\n",
    "# Process and classify each fund\n",
    "def process_fund(row):\n",
    "    audit_log = []\n",
    "    leverage_use, leverage_effect = classify_leverage(row, audit_log)\n",
    "    return {\n",
    "        'SymbolCUSIP': row.get('SymbolCUSIP', ''),\n",
    "        'ProductName': row.get('ProductName', ''),\n",
    "        'fund_family': row.get('fund_family', ''),\n",
    "        'Leverage_Use': leverage_use,\n",
    "        'Leverage_Effect': leverage_effect,\n",
    "        'ycharts_url': row.get('ycharts_url', ''),\n",
    "        'Audit_Log': \"; \".join(audit_log)\n",
    "    }\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load data\n",
    "    df = load_fund_data()\n",
    "\n",
    "    # Process each row\n",
    "    results = [process_fund(row) for _, row in df.iterrows()]\n",
    "\n",
    "    # Create DataFrame\n",
    "    out_df = pd.DataFrame(results)\n",
    "\n",
    "    # Define output path\n",
    "    base_path = r\"C:\\Users\\JulianHeron\\Software Projects\\Test files\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_filename = f\"Risk_Overlays_{timestamp}.xlsx\"\n",
    "    output_path = os.path.join(base_path, output_filename)\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    # Save to Excel\n",
    "    out_df.to_excel(output_path, index=False)\n",
    "    logging.info(f\"Results saved to '{output_path}'\")\n",
    "\n",
    "    # Detailed logging\n",
    "    if CONFIG.get(\"use_detailed_logging\", True):\n",
    "        logging.info(\"=== Detailed Classification Summary ===\")\n",
    "        logging.info(f\"Total Funds Processed: {len(out_df)}\")\n",
    "        logging.info(\"\\nLeverage Use Distribution:\")\n",
    "        for lev in [\"Low\", \"Medium\", \"High\"]:\n",
    "            count = len(out_df[out_df['Leverage_Use'] == lev])\n",
    "            logging.info(f\"  {lev}: {count} ({count/len(out_df)*100:.2f}%)\")\n",
    "        logging.info(\"\\nLeverage Effect Distribution:\")\n",
    "        for eff in [\"Slight\", \"Strategic\", \"Systematic\", \"Amplified\"]:\n",
    "            count = len(out_df[out_df['Leverage_Effect'] == eff])\n",
    "            logging.info(f\"  {eff}: {count} ({count/len(out_df)*100:.2f}%)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error running main: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9640b15d-344b-4690-84c4-73c3b6497cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 16:05:21,591 - INFO - Database connection established successfully.\n",
      "2025-03-10 16:05:21,941 - INFO - Loaded 5586 rows from Funds_to_Screen table.\n",
      "2025-03-10 16:05:25,903 - INFO - Results saved to 'C:\\Users\\JulianHeron\\Software Projects\\Test files\\Risk_Overlays_20250310_160524.xlsx'\n",
      "2025-03-10 16:05:25,903 - INFO - === Detailed Classification Summary ===\n",
      "2025-03-10 16:05:25,904 - INFO - Total Funds Processed: 5586\n",
      "2025-03-10 16:05:25,905 - INFO - \n",
      "Leverage Use Distribution:\n",
      "2025-03-10 16:05:25,908 - INFO -   Low: 4888 (87.50%)\n",
      "2025-03-10 16:05:25,911 - INFO -   Medium: 625 (11.19%)\n",
      "2025-03-10 16:05:25,913 - INFO -   High: 73 (1.31%)\n",
      "2025-03-10 16:05:25,915 - INFO - \n",
      "Leverage Effect Distribution:\n",
      "2025-03-10 16:05:25,918 - INFO -   Slight: 101 (1.81%)\n",
      "2025-03-10 16:05:25,925 - INFO -   Strategic: 113 (2.02%)\n",
      "2025-03-10 16:05:25,930 - INFO -   Systematic: 5059 (90.57%)\n",
      "2025-03-10 16:05:25,935 - INFO -   Amplified: 313 (5.60%)\n"
     ]
    }
   ],
   "source": [
    "# new gross and net leverage from doge\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging to both console and file with UTF-8 encoding\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"))\n",
    "try:\n",
    "    console_handler.stream.reconfigure(encoding='utf-8')\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"classification.log\", encoding='utf-8'),\n",
    "        console_handler\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"use_detailed_logging\": True\n",
    "}\n",
    "\n",
    "# Database connection string\n",
    "connection_string = (\n",
    "    \"mssql+pyodbc://JULIANS_LAPTOP\\\\SQLEXPRESS/\"\n",
    "    \"CWA_Fund_Database?driver=ODBC+Driver+18+for+SQL+Server\"\n",
    "    \"&trusted_connection=yes&TrustServerCertificate=yes\"\n",
    ")\n",
    "\n",
    "# Create database connection\n",
    "def create_db_connection():\n",
    "    try:\n",
    "        engine = create_engine(connection_string)\n",
    "        logging.info(\"Database connection established successfully.\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error connecting to database: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Retrieve data from Funds_to_Screen table with category joins\n",
    "def load_fund_data():\n",
    "    engine = create_db_connection()\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            f.SymbolCUSIP, f.ProductName, f.fund_family, f.ycharts_url,\n",
    "            f.cash_long, f.cash_net, f.cash_short,\n",
    "            f.stock_long, f.stock_net, f.stock_short,\n",
    "            f.bond_long, f.bond_net, f.bond_short,\n",
    "            f.other_long, f.other_net, f.other_short,\n",
    "            f.preferred_long, f.preferred_net, f.preferred_short,\n",
    "            f.convertible_long, f.convertible_net, f.convertible_short,\n",
    "            yc.Category_Name AS YC_Category_Name\n",
    "        FROM Funds_to_Screen f\n",
    "        LEFT JOIN YC_Category_List yc ON f.YC_Category_ID = yc.ID\n",
    "        \"\"\"\n",
    "        df = pd.read_sql(query, engine)\n",
    "        logging.info(f\"Loaded {len(df)} rows from Funds_to_Screen table.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data from database: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        engine.dispose()\n",
    "\n",
    "# Utility function to safely handle None/NaN values\n",
    "def safe_lower(value):\n",
    "    return str(value).lower() if pd.notna(value) else \"\"\n",
    "\n",
    "# Classify leverage use and effect\n",
    "def classify_leverage(row, audit_log):\n",
    "    # Define exposure columns for each category\n",
    "    exposure_categories = {\n",
    "        'cash': ['cash_long', 'cash_short', 'cash_net'],\n",
    "        'stock': ['stock_long', 'stock_short', 'stock_net'],\n",
    "        'bond': ['bond_long', 'bond_short', 'bond_net'],\n",
    "        'other': ['other_long', 'other_short', 'other_net'],\n",
    "        'preferred': ['preferred_long', 'preferred_short', 'preferred_net'],\n",
    "        'convertible': ['convertible_long', 'convertible_short', 'convertible_net']\n",
    "    }\n",
    "\n",
    "    # Ensure all exposure columns are numeric and handle missing values\n",
    "    for category in exposure_categories:\n",
    "        for col in exposure_categories[category]:\n",
    "            val = pd.to_numeric(row.get(col, 0), errors='coerce')\n",
    "            row[col] = 0 if pd.isna(val) else val * 100  # Convert to percentage\n",
    "\n",
    "    # Calculate gross leverage for each category\n",
    "    gross_leverage = {}\n",
    "    for category in exposure_categories:\n",
    "        long_val = row[exposure_categories[category][0]]\n",
    "        short_val = row[exposure_categories[category][1]]\n",
    "        gross_leverage[category] = long_val + short_val\n",
    "\n",
    "    # Determine the dominant category (highest gross leverage)\n",
    "    dominant_category = max(gross_leverage, key=gross_leverage.get)\n",
    "    total_gross_leverage = gross_leverage[dominant_category]\n",
    "\n",
    "    # Classify Gross Leverage (Use)\n",
    "    if total_gross_leverage < 100:\n",
    "        leverage_use = \"Low\"\n",
    "    elif 100 <= total_gross_leverage <= 250:\n",
    "        leverage_use = \"Medium\"\n",
    "    else:\n",
    "        leverage_use = \"High\"\n",
    "\n",
    "    # Calculate Net Leverage (Effect) for the dominant category\n",
    "    net_leverage = row[exposure_categories[dominant_category][2]]\n",
    "\n",
    "    # Classify Net Leverage (Effect) with long-only adjustment\n",
    "    long_only_categories = [\"Taxable Fixed Income\", \"US Equity\", \"Global Bond-USD Hedged\"]\n",
    "    yc_category = safe_lower(row.get('YC_Category_Name', ''))\n",
    "    is_long_only = any(cat.lower() in yc_category for cat in long_only_categories) or dominant_category in ['bond', 'convertible']\n",
    "\n",
    "    if is_long_only and abs(net_leverage) < 10:\n",
    "        leverage_effect = \"Slight\"\n",
    "    else:\n",
    "        if abs(net_leverage) < 10:\n",
    "            leverage_effect = \"Slight\"\n",
    "        elif 10 <= abs(net_leverage) <= 50:\n",
    "            leverage_effect = \"Strategic\"\n",
    "        elif 50 < abs(net_leverage) <= 100:\n",
    "            leverage_effect = \"Systematic\"\n",
    "        else:\n",
    "            leverage_effect = \"Amplified\"\n",
    "\n",
    "    audit_log.append(\n",
    "        f\"Gross Leverage (Use): {round(total_gross_leverage, 4)}% (Dominant: {dominant_category}), \"\n",
    "        f\"Class: {leverage_use}, \"\n",
    "        f\"Net Leverage (Effect): {round(net_leverage, 4)}%, \"\n",
    "        f\"Class: {leverage_effect}\"\n",
    "    )\n",
    "\n",
    "    return leverage_use, leverage_effect\n",
    "\n",
    "# Process and classify each fund\n",
    "def process_fund(row):\n",
    "    audit_log = []\n",
    "    leverage_use, leverage_effect = classify_leverage(row, audit_log)\n",
    "    return {\n",
    "        'SymbolCUSIP': row.get('SymbolCUSIP', ''),\n",
    "        'ProductName': row.get('ProductName', ''),\n",
    "        'fund_family': row.get('fund_family', ''),\n",
    "        'Leverage_Use': leverage_use,\n",
    "        'Leverage_Effect': leverage_effect,\n",
    "        'ycharts_url': row.get('ycharts_url', ''),\n",
    "        'Audit_Log': \"; \".join(audit_log)\n",
    "    }\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load data\n",
    "    df = load_fund_data()\n",
    "\n",
    "    # Process each row\n",
    "    results = [process_fund(row) for _, row in df.iterrows()]\n",
    "\n",
    "    # Create DataFrame\n",
    "    out_df = pd.DataFrame(results)\n",
    "\n",
    "    # Define output path\n",
    "    base_path = r\"C:\\Users\\JulianHeron\\Software Projects\\Test files\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_filename = f\"Risk_Overlays_{timestamp}.xlsx\"\n",
    "    output_path = os.path.join(base_path, output_filename)\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    # Save to Excel\n",
    "    out_df.to_excel(output_path, index=False)\n",
    "    logging.info(f\"Results saved to '{output_path}'\")\n",
    "\n",
    "    # Detailed logging\n",
    "    if CONFIG.get(\"use_detailed_logging\", True):\n",
    "        logging.info(\"=== Detailed Classification Summary ===\")\n",
    "        logging.info(f\"Total Funds Processed: {len(out_df)}\")\n",
    "        logging.info(\"\\nLeverage Use Distribution:\")\n",
    "        for lev in [\"Low\", \"Medium\", \"High\"]:\n",
    "            count = len(out_df[out_df['Leverage_Use'] == lev])\n",
    "            logging.info(f\"  {lev}: {count} ({count/len(out_df)*100:.2f}%)\")\n",
    "        logging.info(\"\\nLeverage Effect Distribution:\")\n",
    "        for eff in [\"Slight\", \"Strategic\", \"Systematic\", \"Amplified\"]:\n",
    "            count = len(out_df[out_df['Leverage_Effect'] == eff])\n",
    "            logging.info(f\"  {eff}: {count} ({count/len(out_df)*100:.2f}%)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error running main: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "979b3582-0af6-45bb-9db8-20b85d6e2d18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 14:17:43,063 - INFO - Database connection established successfully.\n",
      "2025-03-12 14:17:43,280 - INFO - Loaded 5586 rows from Funds_to_Screen table.\n",
      "2025-03-12 14:17:46,804 - INFO - Results saved to 'C:\\Users\\JulianHeron\\Software Projects\\Test files\\Risk_Overlays_20250312_141745.xlsx'\n",
      "2025-03-12 14:17:46,806 - INFO - === Detailed Classification Summary ===\n",
      "2025-03-12 14:17:46,806 - INFO - Total Funds Processed: 5586\n",
      "2025-03-12 14:17:46,807 - INFO - \n",
      "Leverage Use Distribution:\n",
      "2025-03-12 14:17:46,809 - INFO -   None: 4250 (76.08%)\n",
      "2025-03-12 14:17:46,811 - INFO -   Low: 980 (17.54%)\n",
      "2025-03-12 14:17:46,813 - INFO -   Medium: 163 (2.92%)\n",
      "2025-03-12 14:17:46,814 - INFO -   High: 145 (2.60%)\n",
      "2025-03-12 14:17:46,815 - INFO -   Unclassified: 48 (0.86%)\n",
      "2025-03-12 14:17:46,816 - INFO - \n",
      "Leverage Effect Distribution:\n",
      "2025-03-12 14:17:46,818 - INFO -   Slight: 4517 (80.86%)\n",
      "2025-03-12 14:17:46,819 - INFO -   Strategic: 266 (4.76%)\n",
      "2025-03-12 14:17:46,821 - INFO -   Systematic: 744 (13.32%)\n",
      "2025-03-12 14:17:46,823 - INFO -   Amplified: 11 (0.20%)\n",
      "2025-03-12 14:17:46,824 - INFO -   Missing Data: 0 (0.00%)\n",
      "2025-03-12 14:17:46,825 - INFO -   Unclassified: 48 (0.86%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging to both console and file with UTF-8 encoding\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"))\n",
    "try:\n",
    "    console_handler.stream.reconfigure(encoding='utf-8')\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"classification.log\", encoding='utf-8'),\n",
    "        console_handler\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"use_detailed_logging\": True\n",
    "}\n",
    "\n",
    "# Database connection string\n",
    "connection_string = (\n",
    "    \"mssql+pyodbc://JULIANS_LAPTOP\\\\SQLEXPRESS/\"\n",
    "    \"CWA_Fund_Database?driver=ODBC+Driver+18+for+SQL+Server\"\n",
    "    \"&trusted_connection=yes&TrustServerCertificate=yes\"\n",
    ")\n",
    "\n",
    "# Create database connection\n",
    "def create_db_connection():\n",
    "    try:\n",
    "        engine = create_engine(connection_string)\n",
    "        logging.info(\"Database connection established successfully.\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error connecting to database: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Helper function to convert strings\n",
    "def str_to_bool(value):\n",
    "    if isinstance(value, str):  # Check if the value is a string\n",
    "        value = value.strip().lower()  # Remove whitespace and standardize case\n",
    "        if value in ['true', '1']:\n",
    "            return True\n",
    "        elif value in ['false', '0']:\n",
    "            return False\n",
    "    return bool(value)  # Fallback for non-string types (e.g., None or actual booleans)\n",
    "\n",
    "# Retrieve data from Funds_to_Screen table with boolean tags\n",
    "def load_fund_data():\n",
    "    engine = create_db_connection()\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            f.SymbolCUSIP, f.ProductName, f.fund_family, f.ycharts_url,\n",
    "            f.cash_long, f.cash_net, f.cash_short,\n",
    "            f.stock_long, f.stock_net, f.stock_short,\n",
    "            f.bond_long, f.bond_net, f.bond_short,\n",
    "            f.other_long, f.other_net, f.other_short,\n",
    "            f.preferred_long, f.preferred_net, f.preferred_short,\n",
    "            f.convertible_long, f.convertible_net, f.convertible_short,\n",
    "            f.leveraged_fund, f.inverse_fund, f.currency_hedged_fund\n",
    "        FROM Funds_to_Screen f\n",
    "        \"\"\"\n",
    "        df = pd.read_sql(query, engine)\n",
    "        logging.info(f\"Loaded {len(df)} rows from Funds_to_Screen table.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data from database: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        engine.dispose()\n",
    "\n",
    "# Classify leverage use and effect\n",
    "def classify_leverage(row, audit_log):\n",
    "    # Define exposure categories\n",
    "    exposure_categories = {\n",
    "        'cash': ['cash_long', 'cash_short', 'cash_net'],\n",
    "        'stock': ['stock_long', 'stock_short', 'stock_net'],\n",
    "        'bond': ['bond_long', 'bond_short', 'bond_net'],\n",
    "        'other': ['other_long', 'other_short', 'other_net'],\n",
    "        'preferred': ['preferred_long', 'preferred_short', 'preferred_net'],\n",
    "        'convertible': ['convertible_long', 'convertible_short', 'convertible_net']\n",
    "    }\n",
    "\n",
    "    # Convert exposure data to percentages and handle missing values\n",
    "    total_exposure = 0\n",
    "    for category in exposure_categories:\n",
    "        for col in exposure_categories[category]:\n",
    "            val = pd.to_numeric(row.get(col, 0), errors='coerce')\n",
    "            row[col] = val * 100 if not pd.isna(val) else 0  # Convert float to percentage (e.g., 0.024 → 2.4%)\n",
    "            if col in ['cash_long', 'stock_long', 'bond_long', 'other_long', 'preferred_long', 'convertible_long',\n",
    "                       'cash_short', 'stock_short', 'bond_short', 'other_short', 'preferred_short', 'convertible_short']:\n",
    "                total_exposure += row[col]\n",
    "\n",
    "    # Handle zero-data funds\n",
    "    if total_exposure == 0:\n",
    "        audit_log.append(\"Warning: No exposure data for this fund, defaulting to Unclassified/Unclassified\")\n",
    "        return \"Unclassified\", \"Unclassified\"\n",
    "\n",
    "    # Calculate total long, short, net, and other exposures\n",
    "    total_long = sum(row[exposure_categories[cat][0]] for cat in exposure_categories)\n",
    "    total_short = sum(row[exposure_categories[cat][1]] for cat in exposure_categories)\n",
    "    total_net = sum(row[exposure_categories[cat][2]] for cat in exposure_categories)\n",
    "    total_other = row['other_long'] + row['other_short']\n",
    "    total_gross_leverage = total_long + total_short\n",
    "    calculated_net_leverage = total_long - total_short\n",
    "\n",
    "    # Calculate exposure qualifiers\n",
    "    total_short_percent = total_short / total_gross_leverage * 100 if total_gross_leverage > 0 else 0\n",
    "    total_other_percent = total_other / total_gross_leverage * 100 if total_gross_leverage > 0 else 0\n",
    "    total_cash = row['cash_long']\n",
    "    cash_percent = total_cash / total_gross_leverage * 100 if total_gross_leverage > 0 else 0\n",
    "\n",
    "    # Debug logging\n",
    "    audit_log.append(f\"Debug: Total Long = {total_long}%, Total Short = {total_short}%, Total Other = {total_other}%, \"\n",
    "                     f\"Total Net (sum) = {total_net}%, Calculated Net = {calculated_net_leverage}%\")\n",
    "\n",
    "    # Classify Leverage Use\n",
    "    if total_gross_leverage < 101:\n",
    "        leverage_use = \"None\"\n",
    "    elif total_gross_leverage <= 150:\n",
    "        leverage_use = \"Low\"\n",
    "    elif total_gross_leverage <= 250:\n",
    "        leverage_use = \"Medium\"\n",
    "    else:\n",
    "        leverage_use = \"High\"\n",
    "\n",
    "    # Explicitly convert currency_hedged_fund to boolean\n",
    "    currency_hedged = row.get('currency_hedged_fund', False)\n",
    "    if isinstance(currency_hedged, str):\n",
    "        currency_hedged = currency_hedged.lower() == 'true'\n",
    "    else:\n",
    "        currency_hedged = bool(currency_hedged)\n",
    "\n",
    "    # Convert leveraged_fund and inverse_fund to boolean using str_to_bool\n",
    "    leveraged_fund = str_to_bool(row.get('leveraged_fund', False))\n",
    "    inverse_fund = str_to_bool(row.get('inverse_fund', False))\n",
    "\n",
    "    # Classify Leverage Effect with updated conditions\n",
    "    high_leverage = (abs(calculated_net_leverage) >= 150 or total_other_percent > 15 or total_gross_leverage > 150)\n",
    "    if (leveraged_fund or inverse_fund) and high_leverage:\n",
    "        leverage_effect = \"Amplified\"\n",
    "    elif high_leverage:\n",
    "        leverage_effect = \"Systematic\"\n",
    "    elif total_short_percent >= 30 and abs(calculated_net_leverage) < 50:\n",
    "        leverage_effect = \"Systematic\"\n",
    "    elif 5 <= total_short_percent < 30 or (5 <= total_other_percent <= 15) or currency_hedged:\n",
    "        leverage_effect = \"Systematic\"\n",
    "    elif total_short_percent < 5 and total_other_percent < 2 and abs(total_net - 100) < 10 and total_gross_leverage <= 105:\n",
    "        leverage_effect = \"Slight\"\n",
    "    else:\n",
    "        leverage_effect = \"Strategic\"\n",
    "\n",
    "    # Final logging\n",
    "    audit_log.append(\n",
    "        f\"Gross Leverage (Use): {round(total_gross_leverage, 4)}%, Class: {leverage_use}, \"\n",
    "        f\"Net Leverage (Effect): {round(total_net, 4)}%, Class: {leverage_effect}, \"\n",
    "        f\"Short: {round(total_short_percent, 2)}%, Other: {round(total_other_percent, 2)}%, Cash: {round(cash_percent, 2)}%\"\n",
    "    )\n",
    "\n",
    "    return leverage_use, leverage_effect\n",
    "\n",
    "# Process and classify each fund\n",
    "def process_fund(row):\n",
    "    audit_log = []\n",
    "    leverage_use, leverage_effect = classify_leverage(row, audit_log)\n",
    "    result = {\n",
    "        'SymbolCUSIP': row.get('SymbolCUSIP', ''),\n",
    "        'ProductName': row.get('ProductName', ''),\n",
    "        'fund_family': row.get('fund_family', ''),\n",
    "        'Leverage_Use': leverage_use,\n",
    "        'Leverage_Effect': leverage_effect,\n",
    "        'ycharts_url': row.get('ycharts_url', ''),\n",
    "        'Audit_Log': \"; \".join(audit_log),\n",
    "        # Add all exposure data\n",
    "        'cash_long': row.get('cash_long', 0),\n",
    "        'cash_short': row.get('cash_short', 0),\n",
    "        'cash_net': row.get('cash_net', 0),\n",
    "        'stock_long': row.get('stock_long', 0),\n",
    "        'stock_short': row.get('stock_short', 0),\n",
    "        'stock_net': row.get('stock_net', 0),\n",
    "        'bond_long': row.get('bond_long', 0),\n",
    "        'bond_short': row.get('bond_short', 0),\n",
    "        'bond_net': row.get('bond_net', 0),\n",
    "        'other_long': row.get('other_long', 0),\n",
    "        'other_short': row.get('other_short', 0),\n",
    "        'other_net': row.get('other_net', 0),\n",
    "        'preferred_long': row.get('preferred_long', 0),\n",
    "        'preferred_short': row.get('preferred_short', 0),\n",
    "        'preferred_net': row.get('preferred_net', 0),\n",
    "        'convertible_long': row.get('convertible_long', 0),\n",
    "        'convertible_short': row.get('convertible_short', 0),\n",
    "        'convertible_net': row.get('convertible_net', 0),\n",
    "        # Add boolean tags\n",
    "        'leveraged_fund': row.get('leveraged_fund', False),\n",
    "        'inverse_fund': row.get('inverse_fund', False),\n",
    "        'currency_hedged_fund': row.get('currency_hedged_fund', False)\n",
    "    }\n",
    "    return result\n",
    "# Main function\n",
    "def main():\n",
    "    # Load data\n",
    "    df = load_fund_data()\n",
    "\n",
    "    # Process each row\n",
    "    results = [process_fund(row) for _, row in df.iterrows()]\n",
    "\n",
    "    # Create DataFrame\n",
    "    out_df = pd.DataFrame(results)\n",
    "\n",
    "    # Define output path\n",
    "    base_path = r\"C:\\Users\\JulianHeron\\Software Projects\\Test files\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_filename = f\"Risk_Overlays_{timestamp}.xlsx\"\n",
    "    output_path = os.path.join(base_path, output_filename)\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    # Save to Excel\n",
    "    out_df.to_excel(output_path, index=False)\n",
    "    logging.info(f\"Results saved to '{output_path}'\")\n",
    "\n",
    "    # Detailed logging\n",
    "    if CONFIG.get(\"use_detailed_logging\", True):\n",
    "        logging.info(\"=== Detailed Classification Summary ===\")\n",
    "        logging.info(f\"Total Funds Processed: {len(out_df)}\")\n",
    "        logging.info(\"\\nLeverage Use Distribution:\")\n",
    "        for lev in [\"None\", \"Low\", \"Medium\", \"High\", \"Unclassified\"]:\n",
    "            count = len(out_df[out_df['Leverage_Use'] == lev])\n",
    "            logging.info(f\"  {lev}: {count} ({count/len(out_df)*100:.2f}%)\")\n",
    "        logging.info(\"\\nLeverage Effect Distribution:\")\n",
    "        for eff in [\"Slight\", \"Strategic\", \"Systematic\", \"Amplified\", \"Missing Data\", \"Unclassified\"]:\n",
    "            count = len(out_df[out_df['Leverage_Effect'] == eff])\n",
    "            logging.info(f\"  {eff}: {count} ({count/len(out_df)*100:.2f}%)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error running main: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04851b85-7338-43bf-9e1b-608dbe9c8439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 14:30:51,110 - INFO - Database connection established successfully.\n",
      "2025-03-12 14:30:51,606 - INFO - Loaded 5586 rows from Funds_to_Screen table.\n",
      "2025-03-12 14:30:58,296 - INFO - Results saved to 'C:\\Users\\JulianHeron\\Software Projects\\Test files\\Risk_Overlays_20250312_143054.xlsx'\n",
      "2025-03-12 14:30:58,298 - INFO - Database connection established successfully.\n",
      "2025-03-12 14:31:09,705 - INFO - Successfully updated 5586 records in Funds_to_Screen table\n",
      "2025-03-12 14:31:09,718 - INFO - === Detailed Classification Summary ===\n",
      "2025-03-12 14:31:09,727 - INFO - Total Funds Processed: 5586\n",
      "2025-03-12 14:31:09,730 - INFO - \n",
      "Leverage Use Distribution:\n",
      "2025-03-12 14:31:09,740 - INFO -   None: 4250 (76.08%)\n",
      "2025-03-12 14:31:09,753 - INFO -   Low: 980 (17.54%)\n",
      "2025-03-12 14:31:09,755 - INFO -   Medium: 163 (2.92%)\n",
      "2025-03-12 14:31:09,757 - INFO -   High: 145 (2.60%)\n",
      "2025-03-12 14:31:09,759 - INFO -   Unclassified: 48 (0.86%)\n",
      "2025-03-12 14:31:09,765 - INFO - \n",
      "Leverage Effect Distribution:\n",
      "2025-03-12 14:31:09,768 - INFO -   Slight: 4517 (80.86%)\n",
      "2025-03-12 14:31:09,771 - INFO -   Strategic: 266 (4.76%)\n",
      "2025-03-12 14:31:09,777 - INFO -   Systematic: 744 (13.32%)\n",
      "2025-03-12 14:31:09,782 - INFO -   Amplified: 11 (0.20%)\n",
      "2025-03-12 14:31:09,785 - INFO -   Missing Data: 0 (0.00%)\n",
      "2025-03-12 14:31:09,789 - INFO -   Unclassified: 48 (0.86%)\n"
     ]
    }
   ],
   "source": [
    "# Final Risk drivers code \n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text  # Add text import\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging to both console and file with UTF-8 encoding\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"))\n",
    "try:\n",
    "    console_handler.stream.reconfigure(encoding='utf-8')\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"classification.log\", encoding='utf-8'),\n",
    "        console_handler\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"use_detailed_logging\": True\n",
    "}\n",
    "\n",
    "# Database connection string\n",
    "connection_string = (\n",
    "    \"mssql+pyodbc://JULIANS_LAPTOP\\\\SQLEXPRESS/\"\n",
    "    \"CWA_Fund_Database?driver=ODBC+Driver+18+for+SQL+Server\"\n",
    "    \"&trusted_connection=yes&TrustServerCertificate=yes\"\n",
    ")\n",
    "\n",
    "# Create database connection\n",
    "def create_db_connection():\n",
    "    try:\n",
    "        engine = create_engine(connection_string)\n",
    "        logging.info(\"Database connection established successfully.\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error connecting to database: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Helper function to convert strings\n",
    "def str_to_bool(value):\n",
    "    if isinstance(value, str):  # Check if the value is a string\n",
    "        value = value.strip().lower()  # Remove whitespace and standardize case\n",
    "        if value in ['true', '1']:\n",
    "            return True\n",
    "        elif value in ['false', '0']:\n",
    "            return False\n",
    "    return bool(value)  # Fallback for non-string types (e.g., None or actual booleans)\n",
    "\n",
    "# Retrieve data from Funds_to_Screen table with boolean tags\n",
    "def load_fund_data():\n",
    "    engine = create_db_connection()\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            f.SymbolCUSIP, f.ProductName, f.fund_family, f.ycharts_url,\n",
    "            f.cash_long, f.cash_net, f.cash_short,\n",
    "            f.stock_long, f.stock_net, f.stock_short,\n",
    "            f.bond_long, f.bond_net, f.bond_short,\n",
    "            f.other_long, f.other_net, f.other_short,\n",
    "            f.preferred_long, f.preferred_net, f.preferred_short,\n",
    "            f.convertible_long, f.convertible_net, f.convertible_short,\n",
    "            f.leveraged_fund, f.inverse_fund, f.currency_hedged_fund\n",
    "        FROM Funds_to_Screen f\n",
    "        \"\"\"\n",
    "        df = pd.read_sql(query, engine)\n",
    "        logging.info(f\"Loaded {len(df)} rows from Funds_to_Screen table.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data from database: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        engine.dispose()\n",
    "\n",
    "# Classify leverage use and effect\n",
    "def classify_leverage(row, audit_log):\n",
    "    # Define exposure categories\n",
    "    exposure_categories = {\n",
    "        'cash': ['cash_long', 'cash_short', 'cash_net'],\n",
    "        'stock': ['stock_long', 'stock_short', 'stock_net'],\n",
    "        'bond': ['bond_long', 'bond_short', 'bond_net'],\n",
    "        'other': ['other_long', 'other_short', 'other_net'],\n",
    "        'preferred': ['preferred_long', 'preferred_short', 'preferred_net'],\n",
    "        'convertible': ['convertible_long', 'convertible_short', 'convertible_net']\n",
    "    }\n",
    "\n",
    "    # Convert exposure data to percentages and handle missing values\n",
    "    total_exposure = 0\n",
    "    for category in exposure_categories:\n",
    "        for col in exposure_categories[category]:\n",
    "            val = pd.to_numeric(row.get(col, 0), errors='coerce')\n",
    "            row[col] = val * 100 if not pd.isna(val) else 0  # Convert float to percentage (e.g., 0.024 → 2.4%)\n",
    "            if col in ['cash_long', 'stock_long', 'bond_long', 'other_long', 'preferred_long', 'convertible_long',\n",
    "                       'cash_short', 'stock_short', 'bond_short', 'other_short', 'preferred_short', 'convertible_short']:\n",
    "                total_exposure += row[col]\n",
    "\n",
    "    # Handle zero-data funds\n",
    "    if total_exposure == 0:\n",
    "        audit_log.append(\"Warning: No exposure data for this fund, defaulting to Unclassified/Unclassified\")\n",
    "        return \"Unclassified\", \"Unclassified\"\n",
    "\n",
    "    # Calculate total long, short, net, and other exposures\n",
    "    total_long = sum(row[exposure_categories[cat][0]] for cat in exposure_categories)\n",
    "    total_short = sum(row[exposure_categories[cat][1]] for cat in exposure_categories)\n",
    "    total_net = sum(row[exposure_categories[cat][2]] for cat in exposure_categories)\n",
    "    total_other = row['other_long'] + row['other_short']\n",
    "    total_gross_leverage = total_long + total_short\n",
    "    calculated_net_leverage = total_long - total_short\n",
    "\n",
    "    # Calculate exposure qualifiers\n",
    "    total_short_percent = total_short / total_gross_leverage * 100 if total_gross_leverage > 0 else 0\n",
    "    total_other_percent = total_other / total_gross_leverage * 100 if total_gross_leverage > 0 else 0\n",
    "    total_cash = row['cash_long']\n",
    "    cash_percent = total_cash / total_gross_leverage * 100 if total_gross_leverage > 0 else 0\n",
    "\n",
    "    # Debug logging\n",
    "    audit_log.append(f\"Debug: Total Long = {total_long}%, Total Short = {total_short}%, Total Other = {total_other}%, \"\n",
    "                     f\"Total Net (sum) = {total_net}%, Calculated Net = {calculated_net_leverage}%\")\n",
    "\n",
    "    # Classify Leverage Use\n",
    "    if total_gross_leverage < 101:\n",
    "        leverage_use = \"None\"\n",
    "    elif total_gross_leverage <= 150:\n",
    "        leverage_use = \"Low\"\n",
    "    elif total_gross_leverage <= 250:\n",
    "        leverage_use = \"Medium\"\n",
    "    else:\n",
    "        leverage_use = \"High\"\n",
    "\n",
    "    # Explicitly convert currency_hedged_fund to boolean\n",
    "    currency_hedged = row.get('currency_hedged_fund', False)\n",
    "    if isinstance(currency_hedged, str):\n",
    "        currency_hedged = currency_hedged.lower() == 'true'\n",
    "    else:\n",
    "        currency_hedged = bool(currency_hedged)\n",
    "\n",
    "    # Convert leveraged_fund and inverse_fund to boolean using str_to_bool\n",
    "    leveraged_fund = str_to_bool(row.get('leveraged_fund', False))\n",
    "    inverse_fund = str_to_bool(row.get('inverse_fund', False))\n",
    "\n",
    "    # Classify Leverage Effect with updated conditions\n",
    "    high_leverage = (abs(calculated_net_leverage) >= 150 or total_other_percent > 15 or total_gross_leverage > 150)\n",
    "    if (leveraged_fund or inverse_fund) and high_leverage:\n",
    "        leverage_effect = \"Amplified\"\n",
    "    elif high_leverage:\n",
    "        leverage_effect = \"Systematic\"\n",
    "    elif total_short_percent >= 30 and abs(calculated_net_leverage) < 50:\n",
    "        leverage_effect = \"Systematic\"\n",
    "    elif 5 <= total_short_percent < 30 or (5 <= total_other_percent <= 15) or currency_hedged:\n",
    "        leverage_effect = \"Systematic\"\n",
    "    elif total_short_percent < 5 and total_other_percent < 2 and abs(total_net - 100) < 10 and total_gross_leverage <= 105:\n",
    "        leverage_effect = \"Slight\"\n",
    "    else:\n",
    "        leverage_effect = \"Strategic\"\n",
    "\n",
    "    # Final logging\n",
    "    audit_log.append(\n",
    "        f\"Gross Leverage (Use): {round(total_gross_leverage, 4)}%, Class: {leverage_use}, \"\n",
    "        f\"Net Leverage (Effect): {round(total_net, 4)}%, Class: {leverage_effect}, \"\n",
    "        f\"Short: {round(total_short_percent, 2)}%, Other: {round(total_other_percent, 2)}%, Cash: {round(cash_percent, 2)}%\"\n",
    "    )\n",
    "\n",
    "    return leverage_use, leverage_effect\n",
    "\n",
    "# Process and classify each fund\n",
    "def process_fund(row):\n",
    "    audit_log = []\n",
    "    leverage_use, leverage_effect = classify_leverage(row, audit_log)\n",
    "    result = {\n",
    "        'SymbolCUSIP': row.get('SymbolCUSIP', ''),\n",
    "        'ProductName': row.get('ProductName', ''),\n",
    "        'fund_family': row.get('fund_family', ''),\n",
    "        'Leverage_Use': leverage_use,\n",
    "        'Leverage_Effect': leverage_effect,\n",
    "        'ycharts_url': row.get('ycharts_url', ''),\n",
    "        'Audit_Log': \"; \".join(audit_log),\n",
    "        # Add all exposure data\n",
    "        'cash_long': row.get('cash_long', 0),\n",
    "        'cash_short': row.get('cash_short', 0),\n",
    "        'cash_net': row.get('cash_net', 0),\n",
    "        'stock_long': row.get('stock_long', 0),\n",
    "        'stock_short': row.get('stock_short', 0),\n",
    "        'stock_net': row.get('stock_net', 0),\n",
    "        'bond_long': row.get('bond_long', 0),\n",
    "        'bond_short': row.get('bond_short', 0),\n",
    "        'bond_net': row.get('bond_net', 0),\n",
    "        'other_long': row.get('other_long', 0),\n",
    "        'other_short': row.get('other_short', 0),\n",
    "        'other_net': row.get('other_net', 0),\n",
    "        'preferred_long': row.get('preferred_long', 0),\n",
    "        'preferred_short': row.get('preferred_short', 0),\n",
    "        'preferred_net': row.get('preferred_net', 0),\n",
    "        'convertible_long': row.get('convertible_long', 0),\n",
    "        'convertible_short': row.get('convertible_short', 0),\n",
    "        'convertible_net': row.get('convertible_net', 0),\n",
    "        # Add boolean tags\n",
    "        'leveraged_fund': row.get('leveraged_fund', False),\n",
    "        'inverse_fund': row.get('inverse_fund', False),\n",
    "        'currency_hedged_fund': row.get('currency_hedged_fund', False)\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# Updated function to update existing records in Funds_to_Screen\n",
    "def update_funds_to_screen(df):\n",
    "    engine = create_db_connection()\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            for index, row in df.iterrows():\n",
    "                update_query = text(\"\"\"\n",
    "                    UPDATE Funds_to_Screen\n",
    "                    SET Leverage_Use = :leverage_use,\n",
    "                        Leverage_Effect = :leverage_effect\n",
    "                    WHERE SymbolCUSIP = :symbol_cusip\n",
    "                \"\"\")\n",
    "                connection.execute(\n",
    "                    update_query,\n",
    "                    {\n",
    "                        'leverage_use': row['Leverage_Use'],\n",
    "                        'leverage_effect': row['Leverage_Effect'],\n",
    "                        'symbol_cusip': row['SymbolCUSIP']\n",
    "                    }\n",
    "                )\n",
    "            connection.commit()\n",
    "        logging.info(f\"Successfully updated {len(df)} records in Funds_to_Screen table\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error updating database: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        engine.dispose()\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load data\n",
    "    df = load_fund_data()\n",
    "\n",
    "    # Process each row\n",
    "    results = [process_fund(row) for _, row in df.iterrows()]\n",
    "\n",
    "    # Create DataFrame\n",
    "    out_df = pd.DataFrame(results)\n",
    "\n",
    "    # Define output path\n",
    "    base_path = r\"C:\\Users\\JulianHeron\\Software Projects\\Test files\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_filename = f\"Risk_Overlays_{timestamp}.xlsx\"\n",
    "    output_path = os.path.join(base_path, output_filename)\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    # Save to Excel\n",
    "    out_df.to_excel(output_path, index=False)\n",
    "    logging.info(f\"Results saved to '{output_path}'\")\n",
    "\n",
    "    # Update database with classification results\n",
    "    update_funds_to_screen(out_df)\n",
    "\n",
    "    # Detailed logging\n",
    "    if CONFIG.get(\"use_detailed_logging\", True):\n",
    "        logging.info(\"=== Detailed Classification Summary ===\")\n",
    "        logging.info(f\"Total Funds Processed: {len(out_df)}\")\n",
    "        logging.info(\"\\nLeverage Use Distribution:\")\n",
    "        for lev in [\"None\", \"Low\", \"Medium\", \"High\", \"Unclassified\"]:\n",
    "            count = len(out_df[out_df['Leverage_Use'] == lev])\n",
    "            logging.info(f\"  {lev}: {count} ({count/len(out_df)*100:.2f}%)\")\n",
    "        logging.info(\"\\nLeverage Effect Distribution:\")\n",
    "        for eff in [\"Slight\", \"Strategic\", \"Systematic\", \"Amplified\", \"Missing Data\", \"Unclassified\"]:\n",
    "            count = len(out_df[out_df['Leverage_Effect'] == eff])\n",
    "            logging.info(f\"  {eff}: {count} ({count/len(out_df)*100:.2f}%)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error running main: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4662c229-6d01-4bb3-aa60-89585d2474e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c8315c-dd12-40fd-b0d3-beded6127092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Database Administration)",
   "language": "python",
   "name": "databaseadminenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
