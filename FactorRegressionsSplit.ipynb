{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7e9fd-268f-44ec-a034-6cae458e670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting up the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1712ef7e-dde6-4df3-acfb-5f4e78dbae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just equity and fixed income regressions below from grok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98b6bb50-5095-4283-bc0c-054fd8104816",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 16:42:42,243 - INFO - Starting main pipeline\n",
      "Processing Global:   0%|          | 0/10 [00:00<?, ?it/s]2025-04-21 16:42:44,136 - DEBUG - Starting process_fund\n",
      "2025-04-21 16:42:44,137 - DEBUG - Starting run_regressions\n",
      "2025-04-21 16:42:44,138 - DEBUG - Starting load_factors_for_regression\n",
      "2025-04-21 16:42:44,138 - ERROR - Error processing EMDM: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:44,139 - DEBUG - Starting process_fund\n",
      "2025-04-21 16:42:44,140 - DEBUG - Starting run_regressions\n",
      "2025-04-21 16:42:44,141 - DEBUG - Starting load_factors_for_regression\n",
      "2025-04-21 16:42:44,142 - ERROR - Error processing GSFP: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:44,142 - DEBUG - Starting process_fund\n",
      "2025-04-21 16:42:44,144 - DEBUG - Starting run_regressions\n",
      "2025-04-21 16:42:44,144 - DEBUG - Starting load_factors_for_regression\n",
      "2025-04-21 16:42:44,145 - ERROR - Error processing PCGG: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:44,146 - DEBUG - Starting process_fund\n",
      "2025-04-21 16:42:44,148 - DEBUG - Starting run_regressions\n",
      "2025-04-21 16:42:44,148 - DEBUG - Starting load_factors_for_regression\n",
      "2025-04-21 16:42:44,149 - ERROR - Error processing ROIS: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:44,150 - DEBUG - Starting process_fund\n",
      "2025-04-21 16:42:44,151 - DEBUG - Starting run_regressions\n",
      "2025-04-21 16:42:44,151 - DEBUG - Starting load_factors_for_regression\n",
      "2025-04-21 16:42:44,152 - ERROR - Error processing AMKIX: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:44,153 - DEBUG - Starting process_fund\n",
      "2025-04-21 16:42:44,154 - DEBUG - Starting run_regressions\n",
      "2025-04-21 16:42:44,154 - DEBUG - Starting load_factors_for_regression\n",
      "2025-04-21 16:42:44,155 - ERROR - Error processing AGVGX: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:44,156 - DEBUG - Starting process_fund\n",
      "2025-04-21 16:42:44,157 - DEBUG - Starting run_regressions\n",
      "2025-04-21 16:42:44,158 - DEBUG - Starting load_factors_for_regression\n",
      "2025-04-21 16:42:44,158 - ERROR - Error processing MAGCX: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:44,159 - DEBUG - Starting process_fund\n",
      "2025-04-21 16:42:44,159 - DEBUG - Starting run_regressions\n",
      "2025-04-21 16:42:44,161 - DEBUG - Starting load_factors_for_regression\n",
      "2025-04-21 16:42:44,161 - ERROR - Error processing CGIIX: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:44,162 - DEBUG - Starting process_fund\n",
      "2025-04-21 16:42:44,163 - DEBUG - Starting run_regressions\n",
      "2025-04-21 16:42:44,164 - DEBUG - Starting load_factors_for_regression\n",
      "2025-04-21 16:42:44,164 - ERROR - Error processing CLAIX: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:44,165 - DEBUG - Starting process_fund\n",
      "2025-04-21 16:42:44,166 - DEBUG - Starting run_regressions\n",
      "2025-04-21 16:42:44,167 - DEBUG - Starting load_factors_for_regression\n",
      "2025-04-21 16:42:44,167 - ERROR - Error processing ESGYX: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "Processing Global: 100%|██████████| 10/10 [00:00<00:00, 306.51it/s]\n",
      "2025-04-21 16:42:44,169 - INFO - Region Global generated 0 records with 10 errors, skips: {}\n",
      "Processing Intl:   0%|          | 0/10 [00:00<?, ?it/s]2025-04-21 16:42:45,523 - ERROR - Error processing BBCA: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:45,524 - ERROR - Error processing FPA: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:45,526 - ERROR - Error processing ASEA: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:45,527 - ERROR - Error processing EWN: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:45,528 - ERROR - Error processing FGM: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:45,529 - ERROR - Error processing IEUS: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:45,530 - ERROR - Error processing CXSE: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:45,531 - ERROR - Error processing FLHK: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:45,533 - ERROR - Error processing EMHY: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:45,534 - ERROR - Error processing JMKIX: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "Processing Intl: 100%|██████████| 10/10 [00:00<00:00, 768.82it/s]\n",
      "2025-04-21 16:42:45,535 - INFO - Region Intl generated 0 records with 10 errors, skips: {}\n",
      "Processing USA:   0%|          | 0/10 [00:00<?, ?it/s]2025-04-21 16:42:47,002 - ERROR - Error processing CXGCX: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:47,003 - ERROR - Error processing FYX: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:47,004 - ERROR - Error processing UYLD: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:47,005 - ERROR - Error processing AFNIX: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:47,006 - ERROR - Error processing EIRRX: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:47,007 - ERROR - Error processing FKAIX: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:47,008 - ERROR - Error processing FISPX: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:47,010 - ERROR - Error processing TCILX: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:47,011 - ERROR - Error processing TRAMX: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "2025-04-21 16:42:47,012 - ERROR - Error processing MXIIX: load_factors_for_regression() got an unexpected keyword argument 'log_level'\n",
      "Processing USA: 100%|██████████| 10/10 [00:00<00:00, 777.34it/s]\n",
      "2025-04-21 16:42:47,013 - INFO - Region USA generated 0 records with 10 errors, skips: {}\n",
      "2025-04-21 16:42:47,015 - INFO - Pipeline summary: {'total_funds': 10, 'regions': {'Global': {'funds_processed': 10, 'records': 0, 'errors': 10}, 'Intl': {'funds_processed': 10, 'records': 0, 'errors': 10}, 'USA': {'funds_processed': 10, 'records': 0, 'errors': 10}}, 'errors': 30, 'skip_reasons': defaultdict(<class 'int'>, {})}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "from datetime import timedelta, datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from sqlalchemy import create_engine, text\n",
    "from tqdm import tqdm\n",
    "import statsmodels.api as sm\n",
    "import time\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "from functools import wraps\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"database\": {\n",
    "        \"server\": \"JULIANS_LAPTOP\\\\SQLEXPRESS\",\n",
    "        \"database\": \"CWA_Fund_Database\",\n",
    "        \"driver\": \"ODBC Driver 18 for SQL Server\"\n",
    "    },\n",
    "    \"return_metric\": \"1 Month Return\",\n",
    "    \"rolling_periods\": [12, 24, 36, 48, 60],\n",
    "    \"dry_run\": True,\n",
    "    \"sample_dry_run\": True,\n",
    "    \"sample_size\": 10,\n",
    "    \"chunk_size\": 5000,\n",
    "    \"batch_insert_size\": 5000,\n",
    "    \"max_workers_cpu\": 16,\n",
    "    \"batch_size\": 100,\n",
    "    \"log_level\": \"warning\"  # Options: \"debug\", \"info\", \"warning\"\n",
    "}\n",
    "\n",
    "CONNECTION_STRING = (\n",
    "    f\"mssql+pyodbc://{CONFIG['database']['server']}/{CONFIG['database']['database']}\"\n",
    "    f\"?driver={CONFIG['database']['driver']}&trusted_connection=yes&TrustServerCertificate=yes\"\n",
    ")\n",
    "engine = create_engine(CONNECTION_STRING, pool_size=10, max_overflow=5)\n",
    "\n",
    "RETURN_METRIC = CONFIG[\"return_metric\"]\n",
    "ROLLING_PERIODS = CONFIG[\"rolling_periods\"]\n",
    "DRY_RUN = CONFIG[\"dry_run\"]\n",
    "SAMPLE_DRY_RUN = CONFIG[\"sample_dry_run\"]\n",
    "SAMPLE_SIZE = CONFIG[\"sample_size\"]\n",
    "CHUNK_SIZE = CONFIG[\"chunk_size\"]\n",
    "BATCH_INSERT_SIZE = CONFIG[\"batch_insert_size\"]\n",
    "MAX_WORKERS_CPU = CONFIG[\"max_workers_cpu\"]\n",
    "BATCH_SIZE = CONFIG[\"batch_size\"]\n",
    "\n",
    "# Logging setup\n",
    "SUMMARY_LOG = \"factor_attribution_summary.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"factor_attribution.log\"),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ],\n",
    "    force=True\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def log_summary(message):\n",
    "    with open(SUMMARY_LOG, 'a') as f:\n",
    "        f.write(f\"{datetime.now()}: {message}\\n\")\n",
    "\n",
    "# Mappings\n",
    "CATEGORY_TO_REGION = {\n",
    "    \"US Equity Large Cap Blend\": (\"USA\", \"US Equity Large Cap Blend\"),\n",
    "    \"US Equity Large Cap Growth\": (\"USA\", \"US Equity Large Cap Growth\"),\n",
    "    \"US Equity Large Cap Value\": (\"USA\", \"US Equity Large Cap Value\"),\n",
    "    \"US Equity Mid Cap\": (\"USA\", \"US Equity Mid Cap\"),\n",
    "    \"US Equity Small Cap\": (\"USA\", \"US Equity Small Cap\"),\n",
    "    \"Global Equity Large Cap\": (\"Global\", \"Global Equity Large Cap\"),\n",
    "    \"Global Equity Mid/Small Cap\": (\"Global\", \"Global Equity Mid/Small Cap\"),\n",
    "    \"Global Emerging Markets Equity\": (\"Global\", \"Global Emerging Markets Equity\"),\n",
    "    \"Europe Equity Large Cap\": (\"Intl\", \"Europe Equity Large Cap\"),\n",
    "    \"Asia Equity\": (\"Intl\", \"Asia Equity\"),\n",
    "    \"Japan Equity\": (\"Intl\", \"Japan Equity\"),\n",
    "    \"Emerging Markets Fixed Income\": (\"Intl\", \"Emerging Markets Fixed Income\"),\n",
    "    \"US Fixed Income\": (\"USA\", \"US Fixed Income\"),\n",
    "    \"US Municipal Fixed Income\": (\"USA\", \"US Municipal Fixed Income\"),\n",
    "    \"Global Fixed Income\": (\"Global\", \"Global Fixed Income\"),\n",
    "    \"Flexible Allocation\": (\"Global\", \"Flexible Allocation\"),\n",
    "    \"Aggressive Allocation\": (\"Global\", \"Aggressive Allocation\"),\n",
    "    \"Moderate Allocation\": (\"Global\", \"Moderate Allocation\"),\n",
    "    \"Cautious Allocation\": (\"Global\", \"Cautious Allocation\"),\n",
    "    \"Options Trading\": (\"USA\", \"Options Trading\"),\n",
    "    \"Multialternative\": (\"Global\", \"Multialternative\"),\n",
    "    \"Market Neutral\": (\"Global\", \"Market Neutral\"),\n",
    "    \"Long/Short Equity\": (\"Global\", \"Long/Short Equity\"),\n",
    "    \"Alternative Miscellaneous\": (\"Global\", \"Alternative Miscellaneous\"),\n",
    "    \"Energy Sector Equity\": (\"USA\", \"Energy Sector Equity\"),\n",
    "    \"Equity Miscellaneous\": (\"USA\", \"Equity Miscellaneous\"),\n",
    "    \"Financials Sector Equity\": (\"USA\", \"Financials Sector Equity\"),\n",
    "    \"Healthcare Sector Equity\": (\"USA\", \"Healthcare Sector Equity\"),\n",
    "    \"Consumer Goods & Services Sector Equity\": (\"USA\", \"Consumer Goods & Services Sector Equity\"),\n",
    "    \"Communications Sector Equity\": (\"USA\", \"Communications Sector Equity\"),\n",
    "    \"Industrials Sector Equity\": (\"USA\", \"Industrials Sector Equity\"),\n",
    "    \"Other Sector Equity\": (\"USA\", \"Other Sector Equity\"),\n",
    "    \"Real Estate Sector Equity\": (\"USA\", \"Real Estate Sector Equity\"),\n",
    "    \"Precious Metals Sector Equity\": (\"USA\", \"Precious Metals Sector Equity\"),\n",
    "    \"Technology Sector Equity\": (\"USA\", \"Technology Sector Equity\"),\n",
    "    \"Utilities Sector Equity\": (\"USA\", \"Utilities Sector Equity\"),\n",
    "    \"Natural Resources Sector Equity\": (\"USA\", \"Natural Resources Sector Equity\"),\n",
    "    \"Infrastructure Sector Equity\": (\"USA\", \"Infrastructure Sector Equity\"),\n",
    "    \"Trading Tools\": (\"USA\", \"Trading Tools\"),\n",
    "    \"Asia ex-Japan Equity\": (\"Intl\", \"Asia ex-Japan Equity\"),\n",
    "    \"Australia & New Zealand Equity\": (\"Intl\", \"Australia & New Zealand Equity\"),\n",
    "    \"Canadian Equity Large Cap\": (\"Intl\", \"Canadian Equity Large Cap\"),\n",
    "    \"Europe Equity Mid/Small Cap\": (\"Intl\", \"Europe Equity Mid/Small Cap\"),\n",
    "    \"Greater China Equity\": (\"Intl\", \"Greater China Equity\"),\n",
    "    \"India Equity\": (\"Intl\", \"India Equity\"),\n",
    "    \"Mexico Equity\": (\"Intl\", \"Mexico Equity\"),\n",
    "    \"Korea Equity\": (\"Intl\", \"Korea Equity\"),\n",
    "    \"Latin America Equity\": (\"Intl\", \"Latin America Equity\"),\n",
    "    \"UK Equity Large Cap\": (\"Intl\", \"UK Equity Large Cap\"),\n",
    "    \"Thailand Equity\": (\"Intl\", \"Thailand Equity\"),\n",
    "    \"Convertibles\": (\"USA\", \"Convertibles\"),\n",
    "    \"Fixed Income Miscellaneous\": (\"USA\", \"Fixed Income Miscellaneous\"),\n",
    "    \"Allocation Miscellaneous\": (\"Global\", \"Allocation Miscellaneous\")\n",
    "}\n",
    "\n",
    "CATEGORY_TO_REGRESSIONS = {\n",
    "    \"Energy Sector Equity\": \"Equity_USA\",\n",
    "    \"Equity Miscellaneous\": \"Equity_USA\",\n",
    "    \"Financials Sector Equity\": \"Equity_USA\",\n",
    "    \"Healthcare Sector Equity\": \"Equity_USA\",\n",
    "    \"Consumer Goods & Services Sector Equity\": \"Equity_USA\",\n",
    "    \"Communications Sector Equity\": \"Equity_USA\",\n",
    "    \"Industrials Sector Equity\": \"Equity_USA\",\n",
    "    \"Other Sector Equity\": \"Equity_USA\",\n",
    "    \"Real Estate Sector Equity\": \"Equity_USA\",\n",
    "    \"Precious Metals Sector Equity\": \"Equity_USA\",\n",
    "    \"Technology Sector Equity\": \"Equity_USA\",\n",
    "    \"Utilities Sector Equity\": \"Equity_USA\",\n",
    "    \"US Equity Large Cap Blend\": \"Equity_USA\",\n",
    "    \"US Equity Large Cap Growth\": \"Equity_USA\",\n",
    "    \"US Equity Large Cap Value\": \"Equity_USA\",\n",
    "    \"US Equity Mid Cap\": \"Equity_USA\",\n",
    "    \"US Equity Small Cap\": \"Equity_USA\",\n",
    "    \"Options Trading\": \"Equity_USA\",\n",
    "    \"Natural Resources Sector Equity\": \"Equity_USA\",\n",
    "    \"Infrastructure Sector Equity\": \"Equity_USA\",\n",
    "    \"Asia ex-Japan Equity\": \"Equity_Intl\",\n",
    "    \"Australia & New Zealand Equity\": \"Equity_Intl\",\n",
    "    \"Canadian Equity Large Cap\": \"Equity_Intl\",\n",
    "    \"Europe Equity Large Cap\": \"Equity_Intl\",\n",
    "    \"Europe Equity Mid/Small Cap\": \"Equity_Intl\",\n",
    "    \"Greater China Equity\": \"Equity_Intl\",\n",
    "    \"India Equity\": \"Equity_Intl\",\n",
    "    \"Mexico Equity\": \"Equity_Intl\",\n",
    "    \"Japan Equity\": \"Equity_Intl\",\n",
    "    \"Korea Equity\": \"Equity_Intl\",\n",
    "    \"Latin America Equity\": \"Equity_Intl\",\n",
    "    \"UK Equity Large Cap\": \"Equity_Intl\",\n",
    "    \"Thailand Equity\": \"Equity_Intl\",\n",
    "    \"Global Emerging Markets Equity\": \"Equity_Global\",\n",
    "    \"Global Equity Large Cap\": \"Equity_Global\",\n",
    "    \"Global Equity Mid/Small Cap\": \"Equity_Global\",\n",
    "    \"Global Fixed Income\": \"Fixed_Income\",\n",
    "    \"Convertibles\": \"Fixed_Income\",\n",
    "    \"Emerging Markets Fixed Income\": \"Fixed_Income\",\n",
    "    \"Fixed Income Miscellaneous\": \"Fixed_Income\",\n",
    "    \"US Fixed Income\": \"Fixed_Income\",\n",
    "    \"US Municipal Fixed Income\": \"Fixed_Income\",\n",
    "    \"Aggressive Allocation\": \"Allocation\",\n",
    "    \"Allocation Miscellaneous\": \"Allocation\",\n",
    "    \"Cautious Allocation\": \"Allocation\",\n",
    "    \"Flexible Allocation\": \"Allocation\",\n",
    "    \"Moderate Allocation\": \"Allocation\",\n",
    "    \"Alternative Miscellaneous\": \"Alternative\",\n",
    "    \"Long/Short Equity\": \"Alternative\",\n",
    "    \"Market Neutral\": \"Alternative\",\n",
    "    \"Multialternative\": \"Alternative\"\n",
    "}\n",
    "\n",
    "REGRESSION_SETS = {\n",
    "    \"Equity_USA\": [\n",
    "        (\"Equity_USA_1\", ['MKT', 'HML_Devil', 'QMJ', 'SMB', 'UMD', 'BAB'], \"USA\"),\n",
    "        (\"Equity_USA_5\", ['MKT', 'BAB', 'TSM-FI', 'TSM-FX'], \"USA\"),\n",
    "        (\"Equity_USA_6\", ['MKT', 'SMB', 'BAB'], \"USA\"),\n",
    "        (\"Equity_USA_7\", ['MKT', 'HML_Devil', 'QMJ', 'UMD', 'SMB', 'BAB', 'TSM-FI', 'TSM-FX'], \"USA\")\n",
    "    ],\n",
    "    \"Equity_Intl\": [\n",
    "        (\"Equity_Intl_1\", ['MKT', 'HML_Devil', 'QMJ', 'SMB', 'UMD', 'TSM-EQ', 'BAB'], \"Intl\")\n",
    "    ],\n",
    "    \"Equity_Global\": [\n",
    "        (\"Equity_Global_1\", ['MKT', 'HML_Devil', 'QMJ', 'SMB', 'UMD', 'TSM-EQ', 'BAB'], \"Global\")\n",
    "    ],\n",
    "    \"Fixed_Income\": [\n",
    "        (\"FI_1\", ['TERM_Int', 'TERM_Long', 'CREDIT', 'CREDIT_HY', 'TSM-FI', 'TSM-FX'], \"Global\")\n",
    "    ],\n",
    "    \"Allocation\": [\n",
    "        (\"Allocation_1\", ['MKT', 'HML_Devil', 'QMJ', 'SMB', 'UMD', 'BAB', 'TSM-EQ', 'TSM-FI'], \"Global\")\n",
    "    ],\n",
    "    \"Alternative\": [\n",
    "        (\"Alternative_1\", ['MKT', 'HML_Devil', 'QMJ', 'SMB', 'UMD', 'BAB', 'TSM-EQ', 'TSM-FI', 'TSM-FX'], \"Global\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Global factor cache\n",
    "FACTOR_CACHE = {}\n",
    "\n",
    "# Helper Functions\n",
    "def category_to_region(category):\n",
    "    return CATEGORY_TO_REGION.get(category, (\"USA\", \"Unknown\"))\n",
    "\n",
    "@contextmanager\n",
    "def database_transaction():\n",
    "    with engine.connect() as connection:\n",
    "        transaction = connection.begin()\n",
    "        try:\n",
    "            yield connection\n",
    "            transaction.commit()\n",
    "        except Exception as e:\n",
    "            transaction.rollback()\n",
    "            logger.error(f\"Transaction failed: {e}\")\n",
    "            raise\n",
    "\n",
    "def timer(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        log_level = kwargs.get('log_level', CONFIG[\"log_level\"])\n",
    "        if log_level == \"debug\":\n",
    "            logger.debug(f\"Starting {func.__name__}\")\n",
    "        result = func(*args, **kwargs)\n",
    "        if log_level in [\"debug\", \"info\"]:\n",
    "            logger.info(f\"{func.__name__} took {time.time() - start_time:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def standardize_dates(df, freq='ME'):\n",
    "    \"\"\"Standardize DataFrame index to month-end dates.\"\"\"\n",
    "    df.index = pd.to_datetime(df.index) + pd.offsets.MonthEnd(0)\n",
    "    df.index = df.index.drop_duplicates()\n",
    "    return df.asfreq(freq)\n",
    "\n",
    "def align_regression_dates(returns, factor_data, start_date, end_date, expected_months, log_level=\"warning\"):\n",
    "    \"\"\"Align fund returns and factor data for a regression window.\"\"\"\n",
    "    window_dates = returns.index.intersection(factor_data.index)\n",
    "    window_dates = window_dates[(window_dates >= start_date) & (window_dates <= end_date)]\n",
    "    \n",
    "    if len(window_dates) < expected_months:\n",
    "        if log_level == \"debug\":\n",
    "            logger.debug(f\"Insufficient data for window {start_date} to {end_date}: {len(window_dates)}/{expected_months} months\")\n",
    "        return None, None, f\"Insufficient data: {len(window_dates)} months\"\n",
    "    \n",
    "    window_returns = returns.reindex(window_dates)\n",
    "    window_factors = factor_data.reindex(window_dates)\n",
    "    \n",
    "    if window_returns.isna().any() or window_factors.isna().any().any():\n",
    "        if log_level == \"debug\":\n",
    "            logger.debug(f\"Missing data in window {start_date} to {end_date}\")\n",
    "        return None, None, \"Missing data in window\"\n",
    "    \n",
    "    if log_level == \"debug\":\n",
    "        logger.debug(f\"Aligned window {start_date} to {end_date}: {len(window_dates)} months\")\n",
    "    \n",
    "    return window_returns, window_factors, None\n",
    "\n",
    "# Data Loading\n",
    "@timer\n",
    "def load_factors(engine, factors, region, log_level=\"warning\"):\n",
    "    \"\"\"Load equity factors from factor_returns table.\"\"\"\n",
    "    REGION_MAP = {\"International\": \"Intl\"}\n",
    "    region = REGION_MAP.get(region, region)\n",
    "    \n",
    "    cache_key = (tuple(factors), region)\n",
    "    if cache_key in FACTOR_CACHE:\n",
    "        if log_level == \"debug\":\n",
    "            logger.debug(f\"Using cached factors for {cache_key}\")\n",
    "        return FACTOR_CACHE[cache_key]\n",
    "    \n",
    "    factor_str = \",\".join([f\"'{f}'\" for f in factors])\n",
    "    query = text(\"\"\"\n",
    "        SELECT factor, date, value\n",
    "        FROM factor_returns\n",
    "        WHERE factor IN ({}) AND UPPER(region) = UPPER(:region)\n",
    "    \"\"\".format(factor_str))\n",
    "    \n",
    "    with database_transaction() as conn:\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS_CPU) as executor:\n",
    "            future = executor.submit(pd.read_sql_query, query, conn, params={\"region\": region}, parse_dates=[\"date\"])\n",
    "            df = future.result()\n",
    "    \n",
    "    if df.empty:\n",
    "        if log_level in [\"debug\", \"info\"]:\n",
    "            logger.warning(f\"No data loaded from factor_returns for factors={factors}, region={region}\")\n",
    "        if region != \"USA\":\n",
    "            if log_level == \"debug\":\n",
    "                logger.info(f\"Falling back to USA for factors={factors}\")\n",
    "            query = text(\"\"\"\n",
    "                SELECT factor, date, value\n",
    "                FROM factor_returns\n",
    "                WHERE factor IN ({}) AND region = 'USA'\n",
    "            \"\"\".format(factor_str))\n",
    "            with ThreadPoolExecutor(max_workers=MAX_WORKERS_CPU) as executor:\n",
    "                future = executor.submit(pd.read_sql_query, query, conn, params={})\n",
    "                df = future.result()\n",
    "    \n",
    "    if df.empty:\n",
    "        logger.error(f\"No data loaded for factors={factors}, region={region} or USA\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if log_level == \"debug\":\n",
    "        logger.debug(f\"Loaded {len(df)} rows for factors={factors}, region={region}\")\n",
    "    \n",
    "    df = df.pivot_table(index=\"date\", columns=\"factor\", values=\"value\")\n",
    "    df = standardize_dates(df)\n",
    "    df = df[df.index >= '2015-01-01']  # Start date for factor data\n",
    "    \n",
    "    if not df.apply(lambda x: pd.api.types.is_numeric_dtype(x)).all():\n",
    "        logger.error(f\"Non-numeric data in factors: {df.columns[~df.apply(lambda x: pd.api.types.is_numeric_dtype(x))].tolist()}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if log_level == \"debug\":\n",
    "        logger.debug(f\"Factor data range: {df.index.min()} to {df.index.max()}, {len(df)} months\")\n",
    "    \n",
    "    FACTOR_CACHE[cache_key] = df\n",
    "    return df\n",
    "\n",
    "@timer\n",
    "def load_fixed_income_factors(factor_list, log_level=\"warning\"):\n",
    "    \"\"\"Load fixed income factors from Fixed_Income_Factor_Returns table.\"\"\"\n",
    "    factor_str = \",\".join([f\"'{f}'\" for f in factor_list])\n",
    "    query = text(\"\"\"\n",
    "        SELECT Date, Factor_Name, ReturnValue\n",
    "        FROM Fixed_Income_Factor_Returns\n",
    "        WHERE Factor_Name IN ({})\n",
    "    \"\"\".format(factor_str))\n",
    "    \n",
    "    with database_transaction() as conn:\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS_CPU) as executor:\n",
    "            future = executor.submit(pd.read_sql_query, query, conn, parse_dates=[\"Date\"])\n",
    "            df = future.result()\n",
    "    \n",
    "    if df.empty:\n",
    "        logger.warning(f\"No fixed income factors for {factor_list}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = df.pivot(index=\"Date\", columns=\"Factor_Name\", values=\"ReturnValue\")\n",
    "    df = standardize_dates(df)\n",
    "    df = df[df.index >= '2015-01-01']  # Start date for factor data\n",
    "    \n",
    "    if not df.apply(lambda x: pd.api.types.is_numeric_dtype(x)).all():\n",
    "        logger.error(f\"Non-numeric data in fixed income factors: {df.columns[~df.apply(lambda x: pd.api.types.is_numeric_dtype(x))].tolist()}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if log_level == \"debug\":\n",
    "        logger.debug(f\"Fixed income factor data range: {df.index.min()} to {df.index.max()}, {len(df)} months\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "@timer\n",
    "def load_fund_metadata(log_level=\"warning\"):\n",
    "    \"\"\"Load fund metadata from database.\"\"\"\n",
    "    query = text(\"\"\"\n",
    "        SELECT \n",
    "            f.SymbolCUSIP, \n",
    "            f.Region, \n",
    "            f.YC_Global_Category_ID, \n",
    "            c.Global_Category_Name,\n",
    "            f.YC_Category_ID,\n",
    "            y.Category_Name,\n",
    "            f.CWA_Broad_Category_ID,\n",
    "            b.CWA_Broad_Category_Name\n",
    "        FROM Funds_to_Screen f\n",
    "        JOIN YC_Global_Category_List c ON f.YC_Global_Category_ID = c.ID\n",
    "        JOIN YC_Category_List y ON f.YC_Category_ID = y.ID\n",
    "        LEFT JOIN CWA_Broad_Category_List b ON f.CWA_Broad_Category_ID = b.ID\n",
    "    \"\"\")\n",
    "    \n",
    "    with database_transaction() as conn:\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS_CPU) as executor:\n",
    "            future = executor.submit(pd.read_sql_query, query, conn)\n",
    "            df = future.result()\n",
    "    \n",
    "    if log_level in [\"debug\", \"info\"]:\n",
    "        logger.info(f\"Loaded metadata for {len(df)} funds\")\n",
    "    \n",
    "    df[[\"Region\", \"FactorProfile\"]] = df[\"Global_Category_Name\"].map(category_to_region).apply(pd.Series)\n",
    "    if df[\"CWA_Broad_Category_Name\"].isnull().all():\n",
    "        logger.warning(\"CWA_Broad_Category_Name missing; some regressions may be skipped\")\n",
    "    return df.dropna(subset=[\"Region\", \"FactorProfile\"])\n",
    "\n",
    "@timer\n",
    "def load_fund_returns(fund_ids=None, log_level=\"warning\"):\n",
    "    \"\"\"Load fund returns from database.\"\"\"\n",
    "    query = text(\"\"\"\n",
    "        SELECT SymbolCUSIP, Date, ReturnValue\n",
    "        FROM Fund_Returns_Timeseries\n",
    "        WHERE Metric = :metric AND ReturnValue IS NOT NULL AND Date IS NOT NULL\n",
    "    \"\"\")\n",
    "    params = {\"metric\": RETURN_METRIC}\n",
    "    \n",
    "    if fund_ids:\n",
    "        fund_ids = [str(fid) for fid in fund_ids if isinstance(fid, str) and fid.strip()]\n",
    "        if not fund_ids:\n",
    "            logger.warning(\"No valid SymbolCUSIP provided\")\n",
    "            return pd.DataFrame()\n",
    "        query = text(str(query) + f\" AND SymbolCUSIP IN ({','.join([f':fid{i}' for i in range(len(fund_ids))])})\")\n",
    "        params.update({f\"fid{i}\": fid for i, fid in enumerate(fund_ids)})\n",
    "    \n",
    "    chunks = []\n",
    "    with database_transaction() as conn:\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS_CPU) as executor:\n",
    "            for chunk in executor.submit(pd.read_sql_query, query, conn, params=params, parse_dates=[\"Date\"], chunksize=CHUNK_SIZE).result():\n",
    "                if log_level == \"debug\":\n",
    "                    logger.debug(f\"Loaded chunk of {len(chunk)} rows\")\n",
    "                chunks.append(chunk)\n",
    "    \n",
    "    df = pd.concat(chunks) if chunks else pd.DataFrame()\n",
    "    if df.empty:\n",
    "        logger.warning(f\"No returns data loaded for SymbolCUSIP: {fund_ids}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if log_level in [\"debug\", \"info\"]:\n",
    "        logger.info(f\"Loaded returns for {len(df['SymbolCUSIP'].unique())} funds\")\n",
    "    \n",
    "    df = df.pivot(index=\"Date\", columns=\"SymbolCUSIP\", values=\"ReturnValue\")\n",
    "    df = standardize_dates(df)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        total_months = len(df)\n",
    "        missing_months = df[col].isna().sum()\n",
    "        if log_level in [\"debug\", \"info\"]:\n",
    "            logger.info(f\"{col}: {missing_months} of {total_months} months missing\")\n",
    "    \n",
    "    if log_level == \"debug\":\n",
    "        logger.debug(f\"Fund returns range: {df.index.min()} to {df.index.max()}, {len(df)} months\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Regression Processing\n",
    "@timer\n",
    "def run_rolling_regression(symbol, returns, factor_data_dict, regression_name, window_months, log_level=\"warning\"):\n",
    "    \"\"\"Run rolling OLS regressions stepping 1 month.\"\"\"\n",
    "    records = []\n",
    "    skip_reasons = defaultdict(int)\n",
    "    \n",
    "    returns = returns.sort_index().dropna()\n",
    "    if returns.std() < 1e-6:\n",
    "        skip_reasons[\"low_variance\"] += 1\n",
    "        if log_level == \"debug\":\n",
    "            logger.debug(f\"Skipping {symbol} ({regression_name}, window={window_months}m): low variance\")\n",
    "        return records, skip_reasons\n",
    "    \n",
    "    # Get available dates for each factor dataset\n",
    "    eq_data = factor_data_dict.get(\"equity\")\n",
    "    fi_data = factor_data_dict.get(\"fixed_income\")\n",
    "    \n",
    "    available_dates = returns.index\n",
    "    if eq_data is not None:\n",
    "        available_dates = available_dates.intersection(eq_data.index)\n",
    "    if fi_data is not None:\n",
    "        available_dates = available_dates.intersection(fi_data.index)\n",
    "    \n",
    "    if len(available_dates) < window_months:\n",
    "        skip_reasons[\"insufficient_data\"] += 1\n",
    "        if log_level == \"debug\":\n",
    "            logger.debug(f\"Skipping {symbol} ({regression_name}, window={window_months}m): {len(available_dates)} months available\")\n",
    "        return records, skip_reasons\n",
    "    \n",
    "    for end_date in available_dates[window_months-1::1]:  # Step 1 month\n",
    "        start_date = end_date - pd.offsets.MonthEnd(window_months)\n",
    "        if start_date < available_dates[0]:\n",
    "            continue\n",
    "        \n",
    "        # Align returns and factor data for the window\n",
    "        window_returns, _, error = align_regression_dates(\n",
    "            returns, returns, start_date, end_date, window_months, log_level\n",
    "        )\n",
    "        if error:\n",
    "            skip_reasons[error] += 1\n",
    "            continue\n",
    "        \n",
    "        factor_parts = []\n",
    "        if eq_data is not None:\n",
    "            window_eq, eq_factors, error = align_regression_dates(\n",
    "                returns, eq_data, start_date, end_date, window_months, log_level\n",
    "            )\n",
    "            if error:\n",
    "                skip_reasons[error] += 1\n",
    "                continue\n",
    "            factor_parts.append(eq_factors)\n",
    "        \n",
    "        if fi_data is not None:\n",
    "            window_fi, fi_factors, error = align_regression_dates(\n",
    "                returns, fi_data, start_date, end_date, window_months, log_level\n",
    "            )\n",
    "            if error:\n",
    "                skip_reasons[error] += 1\n",
    "                continue\n",
    "            factor_parts.append(fi_factors)\n",
    "        \n",
    "        if not factor_parts:\n",
    "            skip_reasons[\"no_factor_data\"] += 1\n",
    "            if log_level == \"debug\":\n",
    "                logger.debug(f\"No factor data for {symbol} ({regression_name}, window={window_months}m)\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            window_factors = pd.concat(factor_parts, axis=1)\n",
    "            if window_factors.empty or window_factors.shape[1] == 0:\n",
    "                skip_reasons[\"empty_factor_data\"] += 1\n",
    "                if log_level == \"debug\":\n",
    "                    logger.debug(f\"Empty factor data after concat for {symbol} ({regression_name}, window={window_months}m)\")\n",
    "                continue\n",
    "            \n",
    "            if log_level == \"debug\":\n",
    "                logger.debug(f\"Window factors shape for {symbol} ({regression_name}, window={window_months}m): {window_factors.shape}\")\n",
    "            \n",
    "            X = sm.add_constant(window_factors)\n",
    "            model = sm.OLS(window_returns, X, missing='drop').fit()\n",
    "            dropped_pct = (window_months - model.nobs) / window_months\n",
    "            if dropped_pct > 0.2 and log_level == \"debug\":\n",
    "                logger.debug(f\"High data loss in regression for {symbol} ({regression_name}, end_date={end_date}): {dropped_pct:.2%}\")\n",
    "            \n",
    "            coefficients = model.params.to_dict()\n",
    "            tvalues = model.tvalues.to_dict()\n",
    "            pvalues = model.pvalues.to_dict()\n",
    "            r_squared = model.rsquared\n",
    "            \n",
    "            record = {\n",
    "                \"SymbolCUSIP\": symbol,\n",
    "                \"RegressionName\": regression_name,\n",
    "                \"Window\": f\"{window_months}m\",\n",
    "                \"EndDate\": end_date,\n",
    "                \"R_Squared\": r_squared\n",
    "            }\n",
    "            for factor in coefficients:\n",
    "                record[f\"{factor}_beta\"] = coefficients[factor]\n",
    "                record[f\"{factor}_tvalue\"] = tvalues.get(factor, None)\n",
    "                record[f\"{factor}_pvalue\"] = pvalues.get(factor, None)\n",
    "            records.append(record)\n",
    "        except Exception as e:\n",
    "            skip_reasons[\"regression_error\"] += 1\n",
    "            if log_level in [\"debug\", \"info\"]:\n",
    "                logger.warning(f\"Regression failed for {symbol} ({regression_name}, window={window_months}m, end_date={end_date}): {str(e)}\")\n",
    "    \n",
    "    if not records and log_level in [\"debug\", \"info\"]:\n",
    "        logger.info(f\"No regressions completed for {symbol} ({regression_name}, window={window_months}m): {dict(skip_reasons)}\")\n",
    "    \n",
    "    return records, skip_reasons\n",
    "\n",
    "# Fund Processing\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@timer\n",
    "def load_factors_for_regression(fund_data, factors, region):\n",
    "    try:\n",
    "        # Load factor data\n",
    "        factor_data = load_factors(factors, region)\n",
    "        fund_returns = fund_data['returns']\n",
    "        \n",
    "        # Check lengths and date ranges\n",
    "        fund_len = len(fund_returns)\n",
    "        factor_len = len(factor_data)\n",
    "        fund_start, fund_end = fund_returns.index.min(), fund_returns.index.max()\n",
    "        factor_start, factor_end = factor_data.index.min(), factor_data.index.max()\n",
    "        \n",
    "        if fund_len != factor_len:\n",
    "            logger.error(\n",
    "                f\"Length mismatch for fund {fund_data['ticker']}: \"\n",
    "                f\"Fund returns ({fund_len}, {fund_start} to {fund_end}) \"\n",
    "                f\"vs Factor data ({factor_len}, {factor_start} to {factor_end})\"\n",
    "            )\n",
    "            raise ValueError(\"Length mismatch between fund returns and factor data\")\n",
    "        \n",
    "        # Align data on common dates\n",
    "        aligned_fund, aligned_factors = fund_returns.align(factor_data, join='inner')\n",
    "        if len(aligned_fund) == 0:\n",
    "            logger.error(f\"No common dates for fund {fund_data['ticker']} between fund and factor data\")\n",
    "            raise ValueError(\"No overlapping dates after alignment\")\n",
    "        \n",
    "        logger.info(\n",
    "            f\"Aligned data for {fund_data['ticker']}: \"\n",
    "            f\"{len(aligned_fund)} rows from {aligned_fund.index.min()} to {aligned_fund.index.max()}\"\n",
    "        )\n",
    "        return aligned_fund, aligned_factors\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in load_factors_for_regression for {fund_data['ticker']}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "@timer\n",
    "def run_regressions(symbol, returns, regression_category, regression_sets, log_level=\"warning\"):\n",
    "    \"\"\"Run regressions for a fund.\"\"\"\n",
    "    records = []\n",
    "    skip_reasons = defaultdict(int)\n",
    "    \n",
    "    for reg_name, factors, region in regression_sets[regression_category]:\n",
    "        factor_data = load_factors_for_regression(factors, region, reg_name, symbol, log_level=log_level)\n",
    "        if factor_data is None:\n",
    "            skip_reasons[f\"no_factor_data_{reg_name}\"] += 1\n",
    "            continue\n",
    "        \n",
    "        for window in ROLLING_PERIODS:\n",
    "            reg_records, reg_skip_reasons = run_rolling_regression(\n",
    "                symbol, returns[symbol], factor_data, reg_name, window, log_level=log_level\n",
    "            )\n",
    "            records.extend(reg_records)\n",
    "            for reason, count in reg_skip_reasons.items():\n",
    "                skip_reasons[f\"{reason}_{reg_name}_{window}m\"] += count\n",
    "    \n",
    "    return records, skip_reasons\n",
    "\n",
    "@timer\n",
    "def process_fund(fund_data, log_level=\"warning\"):\n",
    "    \"\"\"Process a single fund.\"\"\"\n",
    "    symbol = fund_data[\"SymbolCUSIP\"]\n",
    "    category = fund_data[\"Global_Category_Name\"]\n",
    "    returns = pd.Series(fund_data[\"returns\"]).dropna()\n",
    "    \n",
    "    if returns.empty:\n",
    "        if log_level in [\"debug\", \"info\"]:\n",
    "            logger.warning(f\"No valid returns for {symbol}\")\n",
    "        return [], {\"no_returns\": 1}\n",
    "    \n",
    "    regression_category = CATEGORY_TO_REGRESSIONS.get(category, \"Allocation\")\n",
    "    if regression_category == \"Allocation\" and category not in CATEGORY_TO_REGRESSIONS:\n",
    "        if log_level in [\"debug\", \"info\"]:\n",
    "            logger.warning(f\"Unmapped category {category} for {symbol}; using Allocation\")\n",
    "    \n",
    "    records, skip_reasons = run_regressions(\n",
    "        symbol, fund_data[\"returns_df\"], regression_category, REGRESSION_SETS, log_level=log_level\n",
    "    )\n",
    "    if log_level in [\"debug\", \"info\"]:\n",
    "        logger.info(f\"Generated {len(records)} regression records for {symbol}, skips: {dict(skip_reasons)}\")\n",
    "    return records, skip_reasons\n",
    "\n",
    "@timer\n",
    "def process_region(region, fund_data_list, log_level=\"warning\"):\n",
    "    \"\"\"Process funds in a region.\"\"\"\n",
    "    records = []\n",
    "    errors = 0\n",
    "    skip_reasons = defaultdict(int)\n",
    "    \n",
    "    # Enable debug logging for Global region to diagnose length mismatch\n",
    "    region_log_level = \"debug\" if region == \"Global\" else log_level\n",
    "    \n",
    "    if log_level in [\"debug\", \"info\"]:\n",
    "        logger.info(f\"Processing {len(fund_data_list)} funds in {region}\")\n",
    "    \n",
    "    if SAMPLE_DRY_RUN:\n",
    "        for fund_data in tqdm(fund_data_list, total=len(fund_data_list), desc=f\"Processing {region}\", file=sys.stdout):\n",
    "            try:\n",
    "                fund_records, fund_skips = process_fund(fund_data, log_level=region_log_level)\n",
    "                records.extend(fund_records)\n",
    "                for reason, count in fund_skips.items():\n",
    "                    skip_reasons[reason] += count\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {fund_data['SymbolCUSIP']}: {str(e)}\")\n",
    "                errors += 1\n",
    "    else:\n",
    "        for i in range(0, len(fund_data_list), BATCH_SIZE):\n",
    "            batch = fund_data_list[i:i + BATCH_SIZE]\n",
    "            with ProcessPoolExecutor(max_workers=MAX_WORKERS_CPU) as executor:\n",
    "                future_to_fund = {\n",
    "                    executor.submit(process_fund, fund_data, log_level=region_log_level): fund_data[\"SymbolCUSIP\"]\n",
    "                    for fund_data in batch\n",
    "                }\n",
    "                for future in tqdm(future_to_fund, total=len(batch), desc=f\"Processing {region} batch\", file=sys.stdout):\n",
    "                    try:\n",
    "                        fund_records, fund_skips = future.result()\n",
    "                        records.extend(fund_records)\n",
    "                        for reason, count in fund_skips.items():\n",
    "                            skip_reasons[reason] += count\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error processing {future_to_fund[future]}: {str(e)}\")\n",
    "                        errors += 1\n",
    "    \n",
    "    logger.info(f\"Region {region} generated {len(records)} records with {errors} errors, skips: {dict(skip_reasons)}\")\n",
    "    log_summary(f\"Region {region}: {len(fund_data_list)} funds, {len(records)} records, {errors} errors\")\n",
    "    \n",
    "    if not DRY_RUN and records:\n",
    "        insert_batch(records, log_level=log_level)\n",
    "    \n",
    "    return records, errors, skip_reasons\n",
    "\n",
    "# Database Output\n",
    "@timer\n",
    "def insert_batch(records, log_level=\"warning\"):\n",
    "    \"\"\"Insert regression records to database.\"\"\"\n",
    "    if DRY_RUN:\n",
    "        if log_level in [\"debug\", \"info\"]:\n",
    "            logger.info(f\"Dry run: Would insert {len(records)} records\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        df = pd.DataFrame(records)\n",
    "        batch_size = BATCH_INSERT_SIZE\n",
    "        for i in range(0, len(df), batch_size):\n",
    "            batch_df = df.iloc[i:i + batch_size]\n",
    "            with database_transaction() as connection:\n",
    "                batch_df.to_sql(\"AQRR_Factor_Attribution\", connection, if_exists=\"append\", index=False, method=\"multi\")\n",
    "            if log_level in [\"debug\", \"info\"]:\n",
    "                logger.info(f\"Inserted {len(batch_df)} records to database\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inserting batch: {e}\")\n",
    "        raise\n",
    "\n",
    "# Main Pipeline\n",
    "@timer\n",
    "def main(log_level=\"warning\"):\n",
    "    \"\"\"Run the factor attribution pipeline.\"\"\"\n",
    "    logger.info(\"Starting main pipeline\")\n",
    "    log_summary(\"Pipeline started\")\n",
    "    \n",
    "    try:\n",
    "        fund_meta = load_fund_metadata(log_level=log_level)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load metadata: {e}\")\n",
    "        log_summary(f\"Error: Failed to load metadata: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "    \n",
    "    regions = sorted(set(fund_meta[\"Region\"]) - {'Unknown'})\n",
    "    if log_level in [\"debug\", \"info\"]:\n",
    "        logger.info(f\"Total funds: {len(fund_meta)}, Regions: {regions}\")\n",
    "    log_summary(f\"Total funds: {len(fund_meta)}, Regions: {regions}\")\n",
    "    \n",
    "    fund_ids = fund_meta[\"SymbolCUSIP\"].tolist()\n",
    "    if SAMPLE_DRY_RUN:\n",
    "        fund_ids = random.sample(fund_ids, min(SAMPLE_SIZE, len(fund_ids)))\n",
    "        if log_level in [\"debug\", \"info\"]:\n",
    "            logger.info(f\"Sampled {len(fund_ids)} funds\")\n",
    "        log_summary(f\"Sampled {len(fund_ids)} funds\")\n",
    "    \n",
    "    summary = {\"total_funds\": len(fund_ids), \"regions\": {}, \"errors\": 0, \"skip_reasons\": defaultdict(int)}\n",
    "    for region in regions:\n",
    "        region_fund_ids = fund_meta[fund_meta[\"Region\"] == region][\"SymbolCUSIP\"].tolist()\n",
    "        if not region_fund_ids:\n",
    "            logger.warning(f\"No SymbolCUSIP found for region {region}\")\n",
    "            summary[\"regions\"][region] = {\"funds_processed\": 0, \"records\": 0, \"errors\": 0}\n",
    "            continue\n",
    "        if SAMPLE_DRY_RUN:\n",
    "            region_fund_ids = random.sample(region_fund_ids, min(SAMPLE_SIZE, len(region_fund_ids)))\n",
    "            if log_level in [\"debug\", \"info\"]:\n",
    "                logger.info(f\"Sampled {len(region_fund_ids)} SymbolCUSIP for {region}\")\n",
    "        \n",
    "        try:\n",
    "            returns = load_fund_returns(region_fund_ids, log_level=log_level)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load returns for {region}: {e}\")\n",
    "            log_summary(f\"Error: Failed to load returns for {region}: {e}\")\n",
    "            summary[\"regions\"][region] = {\"funds_processed\": 0, \"records\": 0, \"errors\": 1}\n",
    "            summary[\"errors\"] += 1\n",
    "            continue\n",
    "        \n",
    "        region_funds = [\n",
    "            {\n",
    "                \"SymbolCUSIP\": row[\"SymbolCUSIP\"],\n",
    "                \"Global_Category_Name\": row[\"Global_Category_Name\"],\n",
    "                \"CWA_Broad_Category_Name\": row.get(\"CWA_Broad_Category_Name\", None),\n",
    "                \"returns\": returns[row[\"SymbolCUSIP\"]].to_dict() if row[\"SymbolCUSIP\"] in returns.columns else {},\n",
    "                \"returns_df\": returns\n",
    "            }\n",
    "            for _, row in fund_meta.iterrows() if row[\"SymbolCUSIP\"] in returns.columns\n",
    "        ]\n",
    "        if not region_funds:\n",
    "            logger.warning(f\"No valid returns data for {region}\")\n",
    "            summary[\"regions\"][region] = {\"funds_processed\": 0, \"records\": 0, \"errors\": 0}\n",
    "            continue\n",
    "        \n",
    "        records, errors, region_skip_reasons = process_region(region, region_funds, log_level=log_level)\n",
    "        summary[\"regions\"][region] = {\"funds_processed\": len(region_funds), \"records\": len(records), \"errors\": errors}\n",
    "        summary[\"errors\"] += errors\n",
    "        for reason, count in region_skip_reasons.items():\n",
    "            summary[\"skip_reasons\"][reason] += count\n",
    "    \n",
    "    logger.info(f\"Pipeline summary: {summary}\")\n",
    "    log_summary(f\"Pipeline completed: {summary}\")\n",
    "    return summary\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main(log_level=CONFIG[\"log_level\"])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Main execution failed: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0dda94-b1d8-4592-8685-9df83f9b6e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118fce80-ceb7-4c46-a426-f378c5a2f249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00edfbf7-6993-4590-aa0e-66486dd365cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56876784-e0f7-4f7f-9daf-ce43fc8b32a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad074b8-0e9d-46a8-8f50-82ef939ec5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1559d808-954c-466a-b9c4-49226852feca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811bfea8-18ad-46bd-b139-626bc9f10daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f23a853-a52a-4fa5-b49c-b91a7c76f0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de584e3-52f7-46dc-b961-c74e58637099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9885be-b473-4f25-b2d9-ef4046af2a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bf18fd-88ec-45b9-b0b0-9688f3cd7b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c6891b-576f-4f1c-986e-f7be8cf4a849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c96e18-5cfb-4830-9928-512bb06031ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5346b2-6601-4e67-9258-5a217a43822f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473daa89-a1ff-4c8b-a820-3a9715bb6424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab7d207-b66b-43e0-9a15-7b92afed9fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0623af1d-4deb-489a-a08e-3c1375a45dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e845d2-f194-478b-924d-a51988af3f68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927e899b-32a1-4ce9-a575-27ab7e0f1421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d0ec52-6945-4d92-b40c-5c30c251231c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7578da3-03a3-40c8-bb0b-ca53f4277d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f030383-f4ff-4653-b5c3-36f65c24c9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a303001-86ae-446c-87af-5991d1f9e35d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedda002-e3bb-4aae-a6ae-d51d3039694b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078fc6d7-d559-4fcf-ba7a-c2cdf72e894b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45d6d0f-9525-46ba-94c3-e8484d433b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f0c7a3-85e8-4c94-8b44-326ca495d7d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b69ca-0925-44f1-a056-01ec96f60bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669060f6-59e8-4acb-a5bd-522295a6d217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41bb1a-55a4-4e21-92b1-15ec99f980f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c76069-4dc1-47ae-963c-6191794fe972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2846ab1a-9644-4c9e-be0a-aa8abf17e574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Database Administration)",
   "language": "python",
   "name": "databaseadminenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
